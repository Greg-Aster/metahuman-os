{
  "$schema": "https://metahuman.dev/schemas/models.json",
  "version": "1.0.0",
  "description": "Model registry for multi-model orchestration. Defines roles and their associated models.",
  "globalSettings": {
    "includePersonaSummary": true,
    "useAdapter": false,
    "activeAdapter": null
  },
  "defaults": {
    "orchestrator": "default.orchestrator",
    "persona": "default.persona",
    "curator": "default.curator",
    "coder": "default.coder",
    "planner": "default.planner",
    "summarizer": "default.summarizer",
    "fallback": "default.fallback"
  },
  "models": {
    "default.orchestrator": {
      "provider": "ollama",
      "model": "qwen3:14b",
      "adapters": [],
      "roles": [
        "orchestrator",
        "router",
        "planner"
      ],
      "description": "Capable orchestrator using same model as persona for better intent routing",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.1,
        "topP": 0.9,
        "repeatPenalty": 1.1
      },
      "metadata": {
        "priority": "high",
        "alwaysLoaded": true,
        "estimatedLatency": "fast",
        "sizeGB": 9.3,
        "purpose": "Executive function - intelligent intent routing and tool selection"
      }
    },
    "default.persona": {
      "provider": "ollama",
      "model": "qwen3:14b",
      "adapters": [],
      "roles": [
        "persona",
        "conversation",
        "introspection"
      ],
      "description": "Default persona model for conversational responses",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.7,
        "topP": 0.9,
        "repeatPenalty": 1.2
      },
      "metadata": {
        "priority": "high",
        "alwaysLoaded": true,
        "estimatedLatency": "fast",
        "sizeGB": 9.3
      }
    },
    "persona.with-lora": {
      "provider": "ollama",
      "model": "greg-local-2025-11-02-002011-c333e1",
      "adapters": [
        "/home/greggles/metahuman/out/adapters/2025-11-02/2025-11-02-002011-c333e1/adapter.gguf"
      ],
      "baseModel": "Qwen/Qwen3-14B",
      "roles": [
        "persona",
        "conversation"
      ],
      "description": "Persona model with LoRA adapter for voice tuning",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.7,
        "topP": 0.9,
        "repeatPenalty": 1.2
      },
      "metadata": {
        "priority": "high",
        "alwaysLoaded": false,
        "estimatedLatency": "medium",
        "adapterLoadTime": 1000,
        "trainedOn": "2025-11-02",
        "evalScore": 0.989879518072289
      }
    },
    "default.curator": {
      "provider": "ollama",
      "model": "qwen3:14b",
      "adapters": [],
      "roles": [
        "curator",
        "summarizer",
        "memory-prep"
      ],
      "description": "Mid-size curator model for memory curation and training data preparation",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.3,
        "topP": 0.9,
        "repeatPenalty": 1.1
      },
      "metadata": {
        "priority": "medium",
        "alwaysLoaded": false,
        "estimatedLatency": "fast",
        "sizeGB": 9.3,
        "purpose": "Memory curation - clean data extraction and training prep"
      }
    },
    "default.coder": {
      "provider": "ollama",
      "model": "qwen3-coder:30b",
      "adapters": [],
      "roles": [
        "coder",
        "code-generation",
        "code-review"
      ],
      "description": "Specialized model for code generation, review, and refactoring",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.2,
        "topP": 0.95,
        "repeatPenalty": 1.1
      },
      "metadata": {
        "priority": "medium",
        "alwaysLoaded": false,
        "estimatedLatency": "fast",
        "sizeGB": 9.3,
        "purpose": "Code specialist - generation, review, debugging, refactoring",
        "specialization": "code"
      }
    },
    "default.planner": {
      "provider": "ollama",
      "model": "qwen3-coder:30b",
      "adapters": [],
      "roles": [
        "planner",
        "strategist",
        "task-breakdown"
      ],
      "description": "Strategic planning and task decomposition specialist",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.4,
        "topP": 0.9,
        "repeatPenalty": 1.2
      },
      "metadata": {
        "priority": "medium",
        "alwaysLoaded": false,
        "estimatedLatency": "fast",
        "sizeGB": 9.3,
        "purpose": "Planning specialist - strategic thinking, task breakdown, roadmap creation",
        "specialization": "planning"
      }
    },
    "default.summarizer": {
      "provider": "ollama",
      "model": "qwen3:14b",
      "adapters": [],
      "roles": [
        "summarizer",
        "digest",
        "condensation"
      ],
      "description": "Document and conversation summarization specialist",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.3,
        "topP": 0.9,
        "repeatPenalty": 1.1
      },
      "metadata": {
        "priority": "medium",
        "alwaysLoaded": false,
        "estimatedLatency": "fast",
        "sizeGB": 9.3,
        "purpose": "Summarization specialist - condensing documents, extracting key points",
        "specialization": "summarization"
      }
    },
    "default.fallback": {
      "provider": "ollama",
      "model": "qwen3:14b",
      "adapters": [],
      "roles": [
        "fallback",
        "general"
      ],
      "description": "Fallback model when primary models unavailable",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.7,
        "topP": 0.9
      },
      "metadata": {
        "priority": "low",
        "alwaysLoaded": true,
        "estimatedLatency": "fast",
        "sizeGB": 9.3
      }
    },
    "big-brother.claude": {
      "provider": "claude-code",
      "model": "claude-sonnet-4",
      "adapters": [],
      "roles": [
        "orchestrator",
        "planner",
        "coder"
      ],
      "description": "Claude Code escalation for complex tasks - use when local models are stuck",
      "options": {
        "contextWindow": 200000,
        "temperature": 0.7,
        "timeout": 300000
      },
      "metadata": {
        "priority": "low",
        "alwaysLoaded": false,
        "estimatedLatency": "slow",
        "source": "big-brother",
        "purpose": "Escalation path for operator when local models fail repeatedly",
        "autoEscalate": true,
        "escalationThreshold": 3
      }
    },
    "cloud.qwen3-coder-30b": {
      "provider": "runpod_serverless",
      "model": "Qwen3-Coder-30B-A3B-Instruct-AWQ",
      "adapters": [],
      "roles": [
        "coder",
        "orchestrator",
        "persona",
        "planner",
        "curator",
        "psychotherapist"
      ],
      "description": "Cloud-hosted Qwen3 Coder 30B via RunPod Serverless - high-performance coding and reasoning",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.7,
        "topP": 0.95,
        "repeatPenalty": 1.1
      },
      "metadata": {
        "priority": "high",
        "alwaysLoaded": false,
        "estimatedLatency": "medium",
        "source": "runpod-serverless",
        "endpointTier": "default",
        "coldStartWarning": "First request may take 30-60s if GPU is cold",
        "purpose": "Cloud GPU inference for high-capability tasks"
      }
    },
    "cloud.qwen3-14b": {
      "provider": "runpod_serverless",
      "model": "Qwen3-14B-Instruct",
      "adapters": [],
      "roles": [
        "orchestrator",
        "persona",
        "summarizer",
        "fallback"
      ],
      "description": "Cloud-hosted Qwen3 14B via RunPod Serverless - balanced performance model",
      "options": {
        "contextWindow": 8192,
        "temperature": 0.7,
        "topP": 0.95,
        "repeatPenalty": 1.1
      },
      "metadata": {
        "priority": "medium",
        "alwaysLoaded": false,
        "estimatedLatency": "fast",
        "source": "runpod-serverless",
        "endpointTier": "14b",
        "coldStartWarning": "First request may take 20-30s if GPU is cold",
        "purpose": "Cloud GPU inference for general tasks"
      }
    }
  },
  "roleHierarchy": {
    "orchestrator": [
      "default.orchestrator",
      "default.fallback"
    ],
    "persona": [
      "default.persona",
      "persona.with-lora",
      "default.fallback"
    ],
    "curator": [
      "default.curator",
      "default.fallback"
    ],
    "coder": [
      "default.coder",
      "default.fallback"
    ],
    "planner": [
      "default.planner",
      "default.fallback"
    ],
    "summarizer": [
      "default.summarizer",
      "default.curator",
      "default.fallback"
    ],
    "fallback": [
      "default.fallback"
    ]
  },
  "cognitiveModeMappings": {
    "dual": {
      "orchestrator": "default.orchestrator",
      "persona": "default.persona",
      "coder": "default.coder",
      "planner": "default.planner",
      "curator": "default.curator",
      "summarizer": "default.summarizer",
      "description": "Full cognitive mirror with operator routing"
    },
    "agent": {
      "orchestrator": "default.orchestrator",
      "persona": "default.persona",
      "coder": "default.coder",
      "planner": "default.planner",
      "curator": "default.curator",
      "summarizer": "default.summarizer",
      "description": "Lightweight assistant mode"
    },
    "emulation": {
      "orchestrator": "vllm.active",
      "persona": "vllm.active",
      "description": "Read-only personality snapshot using active backend"
    }
  },
  "providers": {
    "ollama": {
      "baseUrl": "http://localhost:11434",
      "timeout": 120000,
      "retries": 2
    },
    "openai": {
      "baseUrl": "https://api.openai.com/v1",
      "timeout": 60000,
      "retries": 3
    },
    "runpod_serverless": {
      "baseUrl": "https://api.runpod.ai/v2",
      "timeout": 120000,
      "retries": 2
    },
    "claude-code": {
      "baseUrl": "local",
      "timeout": 300000,
      "retries": 1
    }
  }
}
