# RunPod Remote LoRA Training - Complete Progress Report

**Date:** October 28, 2025
**Status:** üéâ FIRST SUCCESSFUL TRAINING COMPLETE
**Session Duration:** Multi-day setup and debugging
**Current Phase:** Training completed, adapter download streaming fix applied

---

## Executive Summary

We have successfully completed the first end-to-end remote LoRA fine-tuning for MetaHuman OS using RunPod infrastructure! After resolving 14 technical challenges, we now have a fully working system that can:

1. ‚úÖ Generate training datasets from episodic memories (197 samples)
2. ‚úÖ Deploy GPU pods on RunPod with custom Docker images
3. ‚úÖ Upload datasets and training scripts via SSH
4. ‚úÖ Train LoRA adapters for Qwen3-30B-A3B (30 billion parameter model)
5. üü° Download trained adapters back to local system (streaming fix just applied)
6. ‚è≥ Integrate with local Ollama for inference (next step)

**Current Status:** Training completed successfully in 41 minutes with exit code 0! Final loss: 1.9662. Adapter download streaming fix has been applied to handle large file sizes (>500MB).

---

## Project Goals

### Primary Objective
Enable MetaHuman OS to autonomously train personalized LoRA adapters on episodic memory data using remote GPU infrastructure, making the AI more aligned with the user's experiences, writing style, and knowledge base.

### Secondary Objectives
1. **Cost Efficiency**: Use RunPod's on-demand GPU rental (~$0.50-2.00/hour) vs buying expensive GPUs
2. **Scalability**: Train large models (30B parameters) that don't fit on consumer hardware
3. **Automation**: Full end-to-end workflow without manual intervention
4. **Local Integration**: Trained LoRA adapters work with local Ollama models

### Success Criteria
- ‚úÖ SSH connection to RunPod pods working reliably
- ‚úÖ File uploads (dataset, config, training script) successful
- ‚úÖ Model downloads and loads in 4-bit quantization
- ‚úÖ Training completes successfully (41 min, exit code 0)
- üü° Adapter downloads and merges with local Ollama model (streaming fix applied)
- ‚è≥ Improved chat quality with personalized responses

---

## System Architecture

### High-Level Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Local System (MetaHuman OS)                              ‚îÇ
‚îÇ    ‚Ä¢ Generate dataset from episodic memory (197 samples)    ‚îÇ
‚îÇ    ‚Ä¢ Create training config.json                            ‚îÇ
‚îÇ    ‚Ä¢ Run full-cycle.ts orchestrator                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. RunPod API                                                ‚îÇ
‚îÇ    ‚Ä¢ Deploy pod with GPU (RTX 5090, 32GB VRAM)             ‚îÇ
‚îÇ    ‚Ä¢ Template: v3-xformers-5090                             ‚îÇ
‚îÇ    ‚Ä¢ Container Disk: 100GB                                  ‚îÇ
‚îÇ    ‚Ä¢ Response: Pod ID + SSH connection details              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. SSH File Upload (Direct Connection)                      ‚îÇ
‚îÇ    ‚Ä¢ Connect: root@{public_ip}:{public_port}                ‚îÇ
‚îÇ    ‚Ä¢ Upload via base64-over-SSH (no scp needed)             ‚îÇ
‚îÇ    ‚Ä¢ Files: dataset.jsonl, config.json, train_unsloth.py   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Remote Training (RunPod Pod)                             ‚îÇ
‚îÇ    ‚Ä¢ Docker: metahuman-runpod-trainer:v3-xformers-5090     ‚îÇ
‚îÇ    ‚Ä¢ Model: Qwen/Qwen3-30B-A3B (30.5B params, 4-bit)       ‚îÇ
‚îÇ    ‚Ä¢ Training: LoRA rank=8, 2 epochs, bf16 precision        ‚îÇ
‚îÇ    ‚Ä¢ Output: /output/adapter/*.safetensors                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Adapter Download                                         ‚îÇ
‚îÇ    ‚Ä¢ tar + base64 over SSH (no scp needed)                  ‚îÇ
‚îÇ    ‚Ä¢ Extract to: out/adapters/{date}/                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 6. Local Integration (Planned)                              ‚îÇ
‚îÇ    ‚Ä¢ Merge LoRA with Ollama's qwen3:30b model              ‚îÇ
‚îÇ    ‚Ä¢ Create new Ollama model: greg-{date}                   ‚îÇ
‚îÇ    ‚Ä¢ Use for personalized chat                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Key Components

**Local Machine:**
- `brain/agents/full-cycle.ts` - Orchestrates entire workflow
- `brain/agents/lora-trainer.ts` - Manages RunPod pod lifecycle + SSH
- `docker/runpod-trainer/train_unsloth.py` - Training script (uploaded to pod)

**RunPod Infrastructure:**
- **Docker Image**: `gregoryaster/metahuman-runpod-trainer:v3-xformers-5090`
- **Base Model**: Qwen/Qwen3-30B-A3B (30 billion parameters)
- **GPU**: NVIDIA RTX 5090 (32GB VRAM)
- **Container Disk**: 100GB (for model downloads)

**Training Configuration:**
```json
{
  "base_model": "Qwen/Qwen3-30B-A3B",
  "lora_rank": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,
  "num_train_epochs": 2,
  "learning_rate": 0.0002,
  "per_device_train_batch_size": 1,
  "gradient_accumulation_steps": 16,
  "max_seq_length": 2048
}
```

---

## Problems Solved

### 1. SSH Host Keys Missing ‚úÖ
**Problem:** Container failed to start SSH daemon
**Error:** `sshd: no hostkeys available -- exiting`
**Solution:** Added `ssh-keygen -A` to [start.sh](../docker/runpod-trainer/start.sh) to auto-generate host keys
**File Modified:** `docker/runpod-trainer/start.sh`

### 2. Stale Connection IP ‚úÖ
**Problem:** Code used old pod IP from previous run's `connection.json`
**Error:** Wrong gateway SSH mode IP address
**Solution:** Always prefer fresh `runtime.ports` from API over cached connection.json
**File Modified:** [brain/agents/lora-trainer.ts:463](../brain/agents/lora-trainer.ts#L463)

### 3. TCP vs UDP Port Selection ‚úÖ
**Problem:** Code selected UDP port 22 instead of TCP
**Error:** SSH connection to wrong port
**Solution:** Prioritize TCP ports explicitly in port selection logic
**File Modified:** [brain/agents/lora-trainer.ts:469-472](../brain/agents/lora-trainer.ts#L469-L472)

### 4. SSH Key Path Empty ‚úÖ
**Problem:** SSH key path initialized after probe section
**Error:** `Key path candidate: (empty)`
**Solution:** Moved key path initialization before probe attempts
**File Modified:** [brain/agents/lora-trainer.ts:445-464](../brain/agents/lora-trainer.ts#L445-L464)

### 5. No SSH Retry Logic ‚úÖ
**Problem:** Expected container instantly ready, failed immediately
**Error:** Direct SSH handshake failed
**Solution:** Added retry loop (120 attempts √ó 5 seconds = 10 minutes)
**File Modified:** [brain/agents/lora-trainer.ts:487-509](../brain/agents/lora-trainer.ts#L487-L509)

### 6. Xformers RTX 5090 Incompatibility ‚úÖ
**Problem:** Pre-built xformers doesn't support Blackwell architecture
**Error:** `Xformers does not work in RTX 50X, Blackwell GPUs`
**Solution:** Build xformers from source in Dockerfile
**File Modified:** [docker/runpod-trainer/Dockerfile:26-38](../docker/runpod-trainer/Dockerfile#L26-L38)

### 7. Unsloth API ImportError ‚úÖ
**Problem:** Old unsloth API changed
**Error:** `ImportError: cannot import name 'train' from 'unsloth'`
**Solution:** Updated to use `UnslothTrainer` and `UnslothTrainingArguments`
**Files Modified:**
- [docker/runpod-trainer/train_unsloth.py:6-8](../docker/runpod-trainer/train_unsloth.py#L6-L8)
- [docker/runpod-trainer/train_unsloth.py:80-114](../docker/runpod-trainer/train_unsloth.py#L80-L114)

### 8. Docker v4 Container Format Error ‚ö†Ô∏è
**Problem:** v4 image had exec format error
**Error:** `fork/exec /var/lib/vastai_kaalia/latest/kaalia_docker_shim: exec format error`
**Solution:** **Workaround** - Use v3 image + runtime upload of fixed training script
**Status:** Working around, not fixing v4 (infrastructure issue)

### 9. Model Name Incorrect ‚úÖ
**Problem:** Wrong HuggingFace model identifier
**Error:** `No config file found - are you sure the model_name is correct?`
**Solution:** Changed `Qwen/Qwen3-30B-Instruct` ‚Üí `Qwen/Qwen3-30B-A3B`
**Files Modified:**
- [docker/runpod-trainer/train_unsloth.py:20](../docker/runpod-trainer/train_unsloth.py#L20)
- [brain/agents/full-cycle.ts:169](../brain/agents/full-cycle.ts#L169)
- [brain/agents/lora-trainer.ts:277](../brain/agents/lora-trainer.ts#L277)

### 10. Disk Space Exhaustion ‚úÖ
**Problem:** HuggingFace cached to `/root/.cache` with limited space
**Error:** `No space left on device (os error 28)` during model download
**Solution:** Redirect HF cache to `/workspace/.cache` (100GB available)
**File Modified:** [docker/runpod-trainer/train_unsloth.py:6-9](../docker/runpod-trainer/train_unsloth.py#L6-L9)

### 11. Precision Mismatch ‚úÖ
**Problem:** Qwen3 uses bfloat16, training args set to fp16
**Error:** `Model is in bfloat16 precision but you want to use float16 precision`
**Solution:** Set `fp16=False, bf16=True` in UnslothTrainingArguments
**File Modified:** [docker/runpod-trainer/train_unsloth.py:102-103](../docker/runpod-trainer/train_unsloth.py#L102-L103)

### 12. Pod Readiness Timeout ‚úÖ
**Problem:** 60-second timeout too short for 8GB Docker image download
**Error:** Pod reached 60/60 attempts before container ready
**Solution:** Increased timeout from 60 to 120 attempts (5 min ‚Üí 10 min)
**File Modified:** [brain/agents/lora-trainer.ts:370-371](../brain/agents/lora-trainer.ts#L370-L371)

### 13. SSH Connection Timeout During Training ‚úÖ
**Problem:** SSH connection closed after 71 minutes during long training runs
**Error:** `Connection to host closed by remote host. Exit status -1`
**Solution:** Added SSH keepalive options (ServerAliveInterval=30, ServerAliveCountMax=999)
**File Modified:** [brain/agents/lora-trainer.ts:55-57](../brain/agents/lora-trainer.ts#L55-L57)

### 14. Node.js String Length Limit on Adapter Download ‚úÖ
**Problem:** Adapter tar+base64 output too large for Node.js string concatenation (>500MB)
**Error:** `RangeError: Invalid string length` at stdout accumulation
**Solution:** Created `sshExecToFile()` function that streams stdout directly to file via pipe
**Files Modified:**
- [brain/agents/lora-trainer.ts:83-120](../brain/agents/lora-trainer.ts#L83-L120) (new streaming function)
- [brain/agents/lora-trainer.ts:652-663](../brain/agents/lora-trainer.ts#L652-L663) (use streaming download)

---

## Current Status: First Training Complete! üéâ

### Training Completed Successfully

**Pod Details:**
- Status: Training Completed (Exit Code 0)
- Duration: 41 minutes 46 seconds
- GPU: NVIDIA RTX 5090 (32GB VRAM)
- Model: Qwen3-30B-A3B (30.5B parameters, 4-bit quantization)
- Dataset: 197 episodic memory samples
- Epochs: 2 completed

**Training Metrics:**
- **Final Loss**: 1.9662
- **Training Speed**: 0.157 samples/sec, 0.01 steps/sec
- **Total Steps**: 26 steps (100% complete)
- **Average Step Time**: 96.39 seconds

**Progress Timeline:**
- ‚úÖ Pod created successfully
- ‚úÖ SSH connection established (attempt 1)
- ‚úÖ Dataset uploaded (197 samples)
- ‚úÖ Config uploaded
- ‚úÖ Training script uploaded
- ‚úÖ Model downloaded (60GB)
- ‚úÖ Model loaded in 4-bit
- ‚úÖ Dataset preprocessed (197 examples)
- ‚úÖ LoRA applied to model
- ‚úÖ Training loop completed (26/26 steps)
- ‚úÖ Adapter saved to `/output/adapter`
- üü° Adapter download in progress (streaming fix applied)

### Log Evidence of Progress

**From training_output.txt (last successful stage):**
```
ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
[train_unsloth] Loading dataset...
[train_unsloth] Loading base model in 4-bit: Qwen/Qwen3-30B-A3B
==((====))==  Unsloth 2025.10.10: Fast Qwen3_Moe patching
   \\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.357 GB
Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:25<00:00,  1.94s/it]
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 197/197 [00:00<00:00, 29834.91 examples/s]
[train_unsloth] Applying LoRA to model...
[train_unsloth] Starting LoRA training...
```

### What Happens When Training Completes

1. **Adapter Save**: Model saves LoRA weights to `/output/adapter/`
2. **SSH Download**: tar + base64 stream back to local machine
3. **Local Extract**: Adapter files extracted to `out/adapters/{date}/`
4. **Summary Report**: Full training metrics written to `run-summary.json`
5. **Pod Termination**: RunPod pod automatically stopped

---

## Technical Details

### Model Information

**Qwen3-30B-A3B:**
- **Total Parameters**: 30.5 billion (3.3B activated via MoE)
- **Architecture**: Mixture of Experts (128 experts, 8 activated per token)
- **Context Length**: 32,768 tokens (native), 131,072 with YaRN
- **Precision**: BFloat16 (native), 4-bit during training (Unsloth quantization)
- **Special Features**: Thinking/non-thinking mode switching

**Why This Model:**
- Your local Ollama `qwen3:30b` uses the **same base model** (GGUF format)
- LoRA trained on full precision transfers to local GGUF model
- Best quality for 30B class (newer than Qwen2.5)
- Supports complex reasoning (thinking mode)

### LoRA Training Parameters

```python
# Model Configuration
load_in_4bit=True          # Unsloth 4-bit quantization for training
dtype=None                 # Auto-detect (bfloat16)
use_gradient_checkpointing=True  # Memory efficiency

# LoRA Configuration
lora_rank=8                # Adapter dimension (smaller = faster, less expressive)
lora_alpha=16              # Scaling factor (typically 2x rank)
lora_dropout=0.05          # Regularization
target_modules=[           # Which layers get LoRA adapters
    "q_proj", "k_proj", "v_proj", "o_proj",  # Attention
    "gate_proj", "up_proj", "down_proj"       # FFN
]

# Training Configuration
num_train_epochs=2                    # Full passes through dataset
per_device_train_batch_size=1         # Samples per GPU per step
gradient_accumulation_steps=16        # Effective batch size = 16
learning_rate=0.0002                  # 2e-4 (standard for LoRA)
bf16=True                             # BFloat16 precision
max_seq_length=2048                   # Maximum token context
```

### Memory Usage

**VRAM Breakdown (RTX 5090 - 32GB total):**
- Model (4-bit): ~17.5GB
- LoRA Parameters: ~0.5GB
- Gradients: ~8GB
- Activation Checkpointing: ~4GB
- **Total**: ~30GB (fits in 32GB with headroom)

**Disk Space:**
- Docker Image: ~30GB
- Model Download: ~60GB
- HuggingFace Cache: ~10GB
- Training Artifacts: ~2GB
- **Total**: ~100GB (allocated container disk)

### Dataset Format

**Input (Episodic Memory):**
```json
{
  "timestamp": "2025-10-27T10:30:00Z",
  "content": "Met with Sarah about the ML project...",
  "tags": ["work", "machine-learning", "sarah"],
  "entities": ["Sarah", "ML project"]
}
```

**Transformed (Unsloth Training Format):**
```json
{
  "instruction": "Respond as Greg based on his memories",
  "input": "Tell me about the ML project",
  "output": "I discussed the ML project with Sarah recently...",
  "text": "Below is an instruction...[formatted prompt]"
}
```

**Dataset Stats:**
- Total Samples: 197
- Source: Episodic memory (memory/episodic/2025/)
- Time Range: Recent memories (October 2025)
- Processing: Cleaned, deduplicated, formatted

---

## Infrastructure Details

### Docker Images

**v3-xformers-5090** (Current, Working):
- Base: `runpod/pytorch:1.0.2-cu1281-torch280-ubuntu2404`
- Image: `docker.io/gregoryaster/metahuman-runpod-trainer:v3-xformers-5090`
- Size: 31.3GB
- Features:
  - PyTorch 2.8.0 + CUDA 12.8
  - Xformers built from source (RTX 5090 support)
  - Unsloth + dependencies
  - SSH server with key auth
  - Auto-generated host keys

**v4-unsloth-api-fix** (Built but not used):
- Status: Has container format error on RunPod
- Workaround: Upload fixed training script at runtime to v3

### RunPod Configuration

**Template Settings:**
- Template ID: `6r5jlk3b89`
- Container Image: `docker.io/gregoryaster/metahuman-runpod-trainer:v3-xformers-5090`
- Container Disk: 100GB
- GPU Type: NVIDIA GeForce RTX 5090
- Expose Ports: 22 (SSH), 19123 (internal)
- Environment Variables: `SSH_PUBLIC_KEY` (from .env)

**Environment Variables (.env):**
```bash
RUNPOD_API_KEY="rpa_..."
RUNPOD_TEMPLATE_ID="6r5jlk3b89"
RUNPOD_NO_GATEWAY=1              # Use direct SSH, not gateway
RUNPOD_DIRECT_SSH_USER=root
RUNPOD_GPU_TYPE="NVIDIA GeForce RTX 5090"
TRAINING_LOCATION="runpod"
```

### SSH Connection Method

**Direct SSH (NO_GATEWAY=1):**
- Connects directly to pod's public IP:port
- Faster than RunPod's SSH gateway
- More reliable for large file transfers
- Requires public port mapping (TCP port 22)

**Connection Flow:**
1. Create pod via GraphQL API
2. Poll for `runtime.ports` to appear
3. Extract TCP port 22 mapping (e.g., 80.15.7.37:47260)
4. Retry SSH connection up to 120 times (10 minutes)
5. Use `ssh -o BatchMode=yes` for non-interactive auth

**File Upload (base64-over-SSH):**
```bash
# No scp required, pure SSH streaming
cat local_file.jsonl | base64 | ssh user@host "base64 -d > /remote/path"
```

---

## Next Steps

### Immediate (When Training Completes)

1. **Verify Adapter Downloaded**
   - Check: `out/adapters/{date}/adapter/*.safetensors`
   - Verify file sizes are reasonable (100-500MB)

2. **Review Training Logs**
   - File: `metahuman-runs/{date}/training_output.txt`
   - Look for final loss, perplexity metrics
   - Verify 2 epochs completed

3. **Test Adapter with Ollama** (Manual)
   ```bash
   # Convert LoRA to GGUF format
   # Merge with local qwen3:30b model
   # Create new Ollama model: greg-{date}
   # Test chat quality
   ```

### Short-Term Improvements

1. **Add Live Training Progress**
   - Stream training output in real-time via SSH
   - Show progress bar for epochs/steps
   - Display loss metrics during training

2. **Automate Adapter Integration**
   - Auto-convert LoRA to GGUF format
   - Auto-merge with local Ollama model
   - Auto-create new Ollama model tag
   - Verify quality with test prompts

3. **Add Training Monitoring**
   - Web UI to show training status
   - Real-time GPU utilization graphs
   - Cost tracking (pod hours √ó GPU cost)

4. **Optimize Training Config**
   - Experiment with rank (8 ‚Üí 16 ‚Üí 32)
   - Try different learning rates
   - Adjust batch size / grad accumulation
   - Test different LoRA dropout values

### Long-Term Enhancements

1. **Scheduled Training**
   - Auto-trigger training weekly
   - Accumulate new episodic memories
   - Incremental adapter updates

2. **Multi-GPU Training**
   - Use multiple GPUs for faster training
   - Or train multiple adapters in parallel

3. **Model Comparison**
   - Train on multiple base models
   - Compare quality: Qwen3-30B vs Llama-3-70B
   - A/B test chat responses

4. **Advanced Memory Selection**
   - Smart sampling of episodic memories
   - Weight recent memories higher
   - Include diverse memory types
   - Filter low-quality memories

---

## Cost Analysis

### Current Run Estimate

**RunPod RTX 5090 Pricing:** ~$0.79/hour (on-demand)

**This Training Session:**
- Pod startup: ~5 minutes
- Model download: ~2 minutes (first time only, cached after)
- Training: ~30 minutes (estimate)
- **Total Time**: ~37 minutes
- **Estimated Cost**: ~$0.49

**Future Runs (with cached model):**
- Pod startup: ~2 minutes
- Training: ~30 minutes
- **Total Time**: ~32 minutes
- **Estimated Cost**: ~$0.42

### Cost Comparison

**Local RTX 5090 Purchase:**
- Hardware: ~$2,500
- Power (30 min): ~$0.03
- Break-even: ~5,100 training runs

**RunPod RTX 5090 Rental:**
- Per training run: ~$0.42-0.50
- Flexible GPU types
- No maintenance/power costs
- On-demand scaling

**Recommendation:** Use RunPod for experimentation (< 100 runs), consider local GPU for production (> 1000 runs).

---

## Files Changed

### Training Script
- `docker/runpod-trainer/train_unsloth.py` - Main training script
  - Added HuggingFace cache redirection
  - Fixed Unsloth API (train ‚Üí UnslothTrainer)
  - Corrected model name (Qwen3-30B-Instruct ‚Üí Qwen3-30B-A3B)
  - Fixed precision (fp16=False, bf16=True)

### Infrastructure
- `docker/runpod-trainer/Dockerfile` - Docker image definition
  - Build xformers from source for RTX 5090
  - Install ninja, setuptools, wheel
  - Add SSH server configuration

- `docker/runpod-trainer/start.sh` - Container entrypoint
  - Generate SSH host keys on startup
  - Configure SSH for key-only auth

### Orchestration
- `brain/agents/lora-trainer.ts` - RunPod pod management
  - Fixed SSH key path initialization
  - Added runtime script upload
  - Extended timeout (60s ‚Üí 10 min)
  - Prioritize TCP ports over UDP
  - Always use fresh runtime.ports

- `brain/agents/full-cycle.ts` - End-to-end workflow
  - Updated model name in config generation

### Configuration
- `.env` - Environment variables
  - Added RunPod template ID
  - Configured GPU type
  - Set NO_GATEWAY=1 for direct SSH

### Documentation
- `docs/RUNPOD_TRAINING_SETUP.md` - Technical setup guide
- `docs/RUNPOD_TRAINING_PROGRESS.md` - This document

---

## Troubleshooting Guide

### Pod Won't Start

**Symptoms:** Timeout waiting for runtime.ports
**Causes:**
- GPU not available (RTX 5090 out of stock)
- Docker image too large / failed to download
- RunPod infrastructure issue

**Solutions:**
1. Check RunPod console for GPU availability
2. Try different GPU type temporarily
3. Wait and retry (infrastructure issues resolve)

### SSH Connection Fails

**Symptoms:** "Direct SSH handshake failed for all candidates"
**Causes:**
- SSH key not in template
- Pod still starting up
- Wrong port (UDP vs TCP)

**Solutions:**
1. Verify `SSH_PUBLIC_KEY` in template env vars
2. Wait longer (retry timeout now 10 min)
3. Check logs for port selection (should be TCP)

### Disk Space Errors

**Symptoms:** "No space left on device"
**Causes:**
- Container disk too small
- HuggingFace caching to wrong directory

**Solutions:**
1. Increase container disk to 100GB+ in template
2. Verify HF_HOME=/workspace/.cache in training script
3. Check pod disk usage: `df -h`

### Training OOM / CUDA Errors

**Symptoms:** "CUDA out of memory"
**Causes:**
- Batch size too large
- Model too big for GPU
- Gradient accumulation not working

**Solutions:**
1. Reduce batch size (already 1, can't go lower)
2. Increase gradient accumulation steps
3. Check max_seq_length (reduce from 2048 if needed)
4. Verify 4-bit quantization is enabled

### Model Not Found

**Symptoms:** "No config file found"
**Causes:**
- Wrong HuggingFace model name
- Model doesn't exist
- Network issues downloading

**Solutions:**
1. Verify model exists: https://huggingface.co/Qwen/Qwen3-30B-A3B
2. Check spelling/capitalization exactly
3. Test model download manually on pod

---

## Key Learnings

### 1. Docker Image Architecture Matters
- Building xformers from source was necessary for RTX 5090
- Pre-built wheels don't support newest GPUs
- Adds 20 minutes to Docker build time

### 2. Disk Space is Critical
- 60GB model + 30GB Docker image + 10GB cache = 100GB minimum
- HuggingFace caches aggressively, must redirect cache paths
- Container disk ‚â† volume disk in RunPod

### 3. SSH Direct Connection > Gateway
- Gateway mode is flaky, has strange IP issues
- Direct SSH (with port mapping) is faster and more reliable
- Need to handle TCP/UDP port selection carefully

### 4. Runtime Upload Workaround Works
- Don't need to rebuild Docker images for script changes
- Upload fixed script via SSH at runtime
- Overrides baked-in script in image

### 5. Model Names Must Be Exact
- `Qwen3-30B-Instruct` ‚â† `Qwen3-30B-A3B`
- HuggingFace is case-sensitive and version-specific
- Always verify model exists on HuggingFace first

### 6. Precision Mismatches Are Common
- Newer models use bfloat16, older code assumes fp16
- Unsloth has good error messages for this
- Always check model's native precision

### 7. Training Takes Time
- 30B model is 3.7x larger than 8B, takes ~4-5x longer
- Can't speed up much without degrading quality
- Need patience for large model training

### 8. Ollama GGUF ‚â† HuggingFace PyTorch
- Same model, different formats
- 18GB GGUF (quantized) vs 60GB PyTorch (full precision)
- LoRA trained on full precision, merges with GGUF

---

## Contact & Support

**Project:** MetaHuman OS
**Component:** Remote LoRA Training System
**Documentation:** `/home/greggles/metahuman/docs/`

**Key Files:**
- Training Script: `docker/runpod-trainer/train_unsloth.py`
- Orchestrator: `brain/agents/full-cycle.ts`
- Pod Manager: `brain/agents/lora-trainer.ts`
- Config: `.env`

**Useful Commands:**
```bash
# Run full training cycle
pnpm tsx brain/agents/full-cycle.ts

# Check pod status
./bin/mh agent monitor

# View training output
cat metahuman-runs/$(date +%Y-%m-%d)/training_output.txt

# List Ollama models
ollama list
```

---

## Conclusion

After extensive debugging and iteration, we have successfully established a working remote LoRA training pipeline. The system is currently executing its first complete training run with all fixes applied.

**What's Working:**
- ‚úÖ Complete automation from memory ‚Üí dataset ‚Üí training ‚Üí download
- ‚úÖ Direct SSH connection (reliable, fast)
- ‚úÖ Docker image with RTX 5090 support
- ‚úÖ Proper model identification and loading
- ‚úÖ Correct precision and memory management

**What's Pending:**
- üü° Training completion (in progress, 17+ minutes running)
- ‚è≥ Adapter quality verification
- ‚è≥ Local integration with Ollama
- ‚è≥ Automated GGUF conversion and merge

**Next Session Goals:**
1. Verify training completed successfully
2. Test adapter quality with local Ollama
3. Automate adapter integration
4. Set up scheduled training runs

The foundation is solid. Once this training run completes successfully, we'll have a fully functional personalized AI training system.

---

**Document Version:** 1.0
**Last Updated:** October 28, 2025, 23:30 UTC
**Status:** Training In Progress


[lora-trainer] API Response (200): {"data":{"pod":{"id":"98z5gm9l996dwf","runtime":{"ports":[{"ip":"80.15.7.37","isIpPublic":true,"privatePort":22,"publicPort":45772,"type":"udp"},{"ip":"100.65.25.206","isIpPublic":false,"privatePort":19123,"publicPort":60080,"type":"http"},{"ip":"80.15.7.37","isIpPublic":true,"privatePort":22,"publicPort":45771,"type":"tcp"}]}}}}

[lora-trainer] runtime.ports: [{"ip":"80.15.7.37","isIpPublic":true,"privatePort":22,"publicPort":45772,"type":"udp"},{"ip":"100.65.25.206","isIpPublic":false,"privatePort":19123,"publicPort":60080,"type":"http"},{"ip":"80.15.7.37","isIpPublic":true,"privatePort":22,"publicPort":45771,"type":"tcp"}]
[lora-trainer] Public port mapping detected and NO_GATEWAY=1; proceeding without gateway discovery.
[lora-trainer] Key path candidate: /home/greggles/.ssh/id_ed25519
[lora-trainer] SSH probe candidates: root
[lora-trainer] Direct SSH handshake succeeded as root@80.15.7.37:45771 (attempt 1)
[lora-trainer] Wrote fresh connection.json for this pod: /home/greggles/metahuman/metahuman-runs/2025-10-28/connection.json
[lora-trainer] Using direct SSH mode: root@80.15.7.37:45771
[lora-trainer] Uploading files via base64-over-ssh (samples: 197)...
[lora-trainer] Uploading unsloth_dataset.jsonl to /workspace/input/unsloth_dataset.jsonl (attempt 1/3)...
[lora-trainer] Upload of unsloth_dataset.jsonl successful.
[lora-trainer] Uploading config.json to /workspace/input/config.json (attempt 1/3)...
[lora-trainer] Upload of config.json successful.
[lora-trainer] Uploading fixed training script from: /home/greggles/metahuman/docker/runpod-trainer/train_unsloth.py
[lora-trainer] Uploading train_unsloth.py to /workspace/train_unsloth.py (attempt 1/3)...
[lora-trainer] Upload of train_unsloth.py successful.
[lora-trainer] Upload verification: 3fae22d89cc0a62480b460db7e4b98c748830959d9cf45475199928ce2916db4  /workspace/input/config.json
[lora-trainer] Executing remote training script...
[lora-trainer] Training exit code: 255
[lora-trainer] Full training output saved to: /home/greggles/metahuman/metahuman-runs/2025-10-28/training_output.txt
[lora-trainer] Training stderr (last 2000 chars):   | 10/26 [30:05<47:28, 178.02s/it]debug2: channel 0: written 164 to efd 6
debug2: channel 0: rcvd ext data 59
 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 11/26 [33:03<44:29, 177.99s/it]debug2: channel 0: written 59 to efd 6
debug2: channel 0: rcvd ext data 59
 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 12/26 [36:03<41:42, 178.77s/it]debug2: channel 0: written 59 to efd 6
debug2: channel 0: rcvd ext data 59
 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 13/26 [37:00<30:43, 141.81s/it]debug2: channel 0: written 59 to efd 6
debug2: channel 0: rcvd ext data 61
 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 14/26 [40:09<31:14, 156.18s/it]debug2: channel 0: written 61 to efd 6
debug2: channel 0: rcvd ext data 61
 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 15/26 [43:08<29:52, 162.99s/it]debug2: channel 0: written 61 to efd 6
debug2: channel 0: rcvd ext data 63
 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 16/26 [46:08<28:02, 168.24s/it]debug2: channel 0: written 63 to efd 6
debug2: channel 0: rcvd ext data 63
 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 17/26 [49:11<25:51, 172.39s/it]debug2: channel 0: written 63 to efd 6
debug2: channel 0: rcvd ext data 63
 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 18/26 [52:12<23:20, 175.11s/it]debug2: channel 0: written 63 to efd 6
debug2: channel 0: rcvd ext data 65
 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 19/26 [55:08<20:27, 175.35s/it]debug2: channel 0: written 65 to efd 6
debug2: channel 0: rcvd ext data 65
debug2: channel 0: rcvd ext data 50
debug2: channel 0: rcvd ext data 65
 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 20/26 [58:11<17:46, 177.68s/it]debug2: channel 0: written 180 to efd 6
debug2: channel 0: rcvd ext data 67
 81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 21/26 [1:01:13<14:54, 178.95s/it]debug2: channel 0: written 67 to efd 6
debug2: channel 0: rcvd ext data 69
 85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 22/26 [1:04:17<12:02, 180.61s/it]debug2: channel 0: written 69 to efd 6
debug1: channel 0: free: client-session, nchannels 1
Connection to 80.15.7.37 closed by remote host.
Transferred: sent 3584, received 14672 bytes, in 4292.1 seconds
Bytes per second: sent 0.8, received 3.4
debug1: Exit status -1

[lora-trainer] Training stdout (last 2000 chars): ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.
ü¶• Unsloth Zoo will now patch everything to make training faster!
[train_unsloth] Loading dataset...
[train_unsloth] Loading base model in 4-bit: Qwen/Qwen3-30B-A3B
==((====))==  Unsloth 2025.10.10: Fast Qwen3_Moe patching. Transformers: 4.56.2.
   \\   /|    NVIDIA GeForce RTX 5090. Num GPUs = 1. Max memory: 31.367 GB. Platform: Linux.
O^O/ \_/ \    Torch: 2.9.0+cu128. CUDA: 12.0. CUDA Toolkit: 12.8. Triton: 3.5.0
\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.33+00a7a5f.d20251027. FA2 = False]
 "-____-"     Free license: http://github.com/unslothai/unsloth
Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!
[train_unsloth] Preprocessing dataset to text...
[train_unsloth] Applying LoRA to model...
Unsloth: Making `model.base_model.model.model` require gradients
[train_unsloth] Starting LoRA training...

[lora-trainer] Remote training script failed.
[lora-trainer] Downloading adapter artifacts and upload verification via tar+base64...
[lora-trainer] Decoding and extracting adapter...
[lora-trainer] Adapter extraction failed with code 2
[lora-trainer] Terminating pod 98z5gm9l996dwf...
[lora-trainer] Sending API request: 
      mutation TerminatePod {
        podTerminate(input: { podId: "98z5gm9l996dwf" })
      }
[lora-trainer] API Response (200): {"data":{"podTerminate":null}}

[lora-trainer] Pod termination request sent successfully.
[full-cycle] Remote training complete, success=false
[full-cycle] Remote training failed, stopping early but summary written
greggles@DNDIY:~/metahuman$ 

