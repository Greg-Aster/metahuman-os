<!DOCTYPE html><html lang="en" class="scroll-smooth"> <head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1"><meta name="description" content="MetaHuman Dashboard"><title>User Guide - MetaHuman OS</title><link rel="stylesheet" href="/./assets/guide.St8gsWha.css">
<link rel="stylesheet" href="/./assets/guide.Go2oEByj.css">
<link rel="stylesheet" href="/./assets/user-guide.C36kz_PU.css"><script type="module" src="/./assets/hoisted.BpPwpwrJ.js"></script></head> <body class="container-page flex flex-col"> <header class="border-b border-gray-200/60 dark:border-white/10 backdrop-blur sticky top-0"> <div class="page-max flex items-center justify-between py-4"> <a href="/" class="font-semibold text-lg">üß† MetaHuman OS</a> <nav class="flex items-center gap-3"> <button class="btn" aria-label="Toggle theme" onclick="
  (function(){
    const c = document.documentElement.classList;
    const dark = c.toggle('dark');
    try { localStorage.setItem('metahuman-theme', dark ? 'dark' : 'light'); } catch {}
  })();
"> <svg width="18" height="18" viewBox="0 0 24 24" fill="none" xmlns="http://www.w3.org/2000/svg"> <path class="dark:hidden" d="M12 4V2M12 22v-2M5 5 3.586 3.586M20.414 20.414 19 19M4 12H2m20 0h-2M5 19l-1.414 1.414M20.414 3.586 19 5M12 6a6 6 0 100 12 6 6 0 000-12z" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></path> <path class="hidden dark:block" d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z" fill="currentColor"></path> </svg> <span class="hidden sm:inline">Theme</span> </button>  </nav> </div> </header> <main class="page-max flex-grow min-h-0">  <div class="user-guide-container" data-astro-cid-xjasbecl> <!-- Chapter Index Sidebar --> <aside class="chapter-sidebar" data-astro-cid-xjasbecl> <div class="sidebar-header" data-astro-cid-xjasbecl> <h2 data-astro-cid-xjasbecl>User Guide</h2> <p class="subtitle" data-astro-cid-xjasbecl>MetaHuman OS Manual</p> </div> <nav class="chapter-nav" data-astro-cid-xjasbecl> <div class="category-section" data-category="getting-started" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="getting-started" data-first-category data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>üöÄ Getting Started</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="getting-started" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="core-concepts" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Core Concepts</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="installation-setup" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Installation Setup</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="overview" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Overview</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="quick-start" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Quick Start</span> </button> </li> </ul> </div><div class="category-section" data-category="using-metahuman" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="using-metahuman" data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>üí¨ Using MetaHuman</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="using-metahuman" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="chat-interface" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Chat Interface</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="dashboard-monitoring" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Dashboard Monitoring</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="memory-system" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Memory System</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="task-management" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Task Management</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="voice-features" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Voice Features</span> </button> </li> </ul> </div><div class="category-section" data-category="training-personalization" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="training-personalization" data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>üß† Training &amp; Personalization</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="training-personalization" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="ai-training" data-first data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Ai Training</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="cognitive-modes" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Cognitive Modes</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="persona-editor" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Persona Editor</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="persona-generator" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Persona Generator</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="voice-training" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Voice Training</span> </button> </li> </ul> </div><div class="category-section" data-category="advanced-features" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="advanced-features" data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>üéì Advanced Features</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="advanced-features" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="autonomous-agents" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Autonomous Agents</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="headless-mode" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Headless Mode</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="multi-user-profiles" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Multi User Profiles</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="node-editor" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Node Editor</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="skills-system" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Skills System</span> </button> </li> </ul> </div><div class="category-section" data-category="configuration-admin" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="configuration-admin" data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>‚öôÔ∏è Configuration &amp; Admin</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="configuration-admin" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="authentication" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Authentication</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="configuration-files" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Configuration Files</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="deployment" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Deployment</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="security-trust" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Security Trust</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="special-states" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Special States</span> </button> </li> </ul> </div><div class="category-section" data-category="reference" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="reference" data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>üìñ Reference</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="reference" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="cli-reference" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Cli Reference</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="faq" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Faq</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="known-issues" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Known Issues</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="troubleshooting" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Troubleshooting</span> </button> </li> </ul> </div><div class="category-section" data-category="appendix" data-astro-cid-xjasbecl> <button class="category-header" data-category-id="appendix" data-astro-cid-xjasbecl> <span class="category-name" data-astro-cid-xjasbecl>üìã Appendix</span> <span class="category-toggle" data-astro-cid-xjasbecl>‚ñº</span> </button> <ul class="chapter-list" data-category-chapters="appendix" data-astro-cid-xjasbecl> <li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="easter-eggs" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Easter Eggs</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="ethical-use" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Ethical Use</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="roadmap" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Roadmap</span> </button> </li><li class="chapter-item" data-astro-cid-xjasbecl> <button class="chapter-button" data-chapter="terms-of-service" data-astro-cid-xjasbecl> <span class="chapter-number" data-astro-cid-xjasbecl></span> <span class="chapter-title" data-astro-cid-xjasbecl>Terms Of Service</span> </button> </li> </ul> </div> </nav> </aside> <!-- Chapter Content --> <main class="chapter-content" data-astro-cid-xjasbecl> <article id="ai-training" class="chapter" data-visible="true" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>AI Training</h1>
<p>Train custom AI models to make MetaHuman think, speak, and reason like you. MetaHuman OS uses LoRA (Low-Rank Adaptation) adapters to personalize large language models with your personality, memories, and communication patterns.</p>
<h2>Overview</h2>
<p>AI training creates personalized language models from your:</p>
<ul>
<li><strong>Episodic memories</strong> - Conversations and observations</li>
<li><strong>Persona data</strong> - Values, goals, communication style</li>
<li><strong>Therapy sessions</strong> - Persona Generator interviews</li>
<li><strong>Cognitive mode data</strong> - Differentiated by mode (Dual/Agent/Emulation)</li>
</ul>
<p><strong>Training Methods:</strong></p>
<ul>
<li><strong>Local LoRA</strong> - Train on your own GPU (NVIDIA, 24GB+ VRAM)</li>
<li><strong>Remote LoRA</strong> - Train on RunPod cloud GPU (pay-per-use)</li>
<li><strong>Fine-Tuning</strong> - Full model fine-tuning (advanced, resource-intensive)</li>
</ul>
<p><strong>Dual-Adapter System:</strong></p>
<ul>
<li><strong>Historical adapter</strong> - Consolidated long-term memory (all past training)</li>
<li><strong>Recent adapter</strong> - Last 30 days of fresh data</li>
<li><strong>Automatic merging</strong> - Both adapters loaded simultaneously for balanced personality</li>
</ul>
<h2>When to Train</h2>
<h3>First Training</h3>
<ul>
<li>After using MetaHuman for 2-4 weeks</li>
<li>Once you have 100+ memories (conversations, observations)</li>
<li>After completing Persona Generator interview</li>
</ul>
<h3>Regular Training</h3>
<ul>
<li><strong>Monthly</strong> - Recommended for active users (enabled by default in <code>etc/training.json</code>)</li>
<li><strong>Quarterly</strong> - Minimum for less active users</li>
<li><strong>After major changes</strong> - New goals, values, or life events</li>
</ul>
<h3>Training Triggers</h3>
<ul>
<li>Accumulated 500+ new memories since last training</li>
<li>Persona significantly updated</li>
<li>Communication style has evolved</li>
<li>Want to capture new patterns or preferences</li>
</ul>
<h2>Accessing AI Training</h2>
<h3>Via Web UI (Training Wizard)</h3>
<ol>
<li>Navigate to <strong>AI Training</strong> in the left sidebar</li>
<li>Click <strong>&quot;Launch Training Wizard&quot;</strong></li>
<li>Follow 5-step guided process</li>
</ol>
<h3>Via CLI</h3>
<pre><code class="language-bash"># Local LoRA training
./bin/mh agent run full-cycle-local --username &lt;your-username&gt;

# Remote LoRA training (RunPod)
./bin/mh agent run full-cycle --username &lt;your-username&gt;
</code></pre>
<h2>Training Wizard (Web UI)</h2>
<p>The Training Wizard guides you through the entire process:</p>
<h3>Step 1: Choose Training Method</h3>
<p><strong>Local LoRA</strong></p>
<ul>
<li><strong>Requirements</strong>: NVIDIA GPU with 24GB+ VRAM</li>
<li><strong>Cost</strong>: Free (uses your hardware)</li>
<li><strong>Time</strong>: 30-60 minutes</li>
<li><strong>Best for</strong>: Users with capable GPUs, privacy-conscious</li>
<li><strong>Models supported</strong>: Qwen3-14B, Llama-3.1-20B</li>
</ul>
<p><strong>Remote LoRA (RunPod)</strong></p>
<ul>
<li><strong>Requirements</strong>: RunPod API key</li>
<li><strong>Cost</strong>: ~$0.50-$2.00 per training run</li>
<li><strong>Time</strong>: 20-40 minutes</li>
<li><strong>Best for</strong>: Users without GPU, one-time training</li>
<li><strong>GPU types</strong>: H100, A100, RTX 4090</li>
</ul>
<p><strong>Fine-Tuning</strong></p>
<ul>
<li><strong>Requirements</strong>: High-end GPU (40GB+ VRAM) or RunPod</li>
<li><strong>Cost</strong>: $5-15 per run (RunPod)</li>
<li><strong>Time</strong>: 2-4 hours</li>
<li><strong>Best for</strong>: Advanced users wanting maximum quality</li>
<li><strong>Models supported</strong>: Qwen3-30B-Instruct</li>
</ul>
<h3>Step 2: Configure RunPod (if Remote)</h3>
<p><strong>First-time Setup:</strong></p>
<ol>
<li>Sign up at <a href="https://runpod.io">runpod.io</a></li>
<li>Generate API key in RunPod dashboard</li>
<li>Enter API key in wizard</li>
<li>Select GPU type (H100 recommended)</li>
</ol>
<p><strong>Wizard Auto-Configuration:</strong></p>
<ul>
<li>Saves RunPod credentials to <code>profiles/&lt;username&gt;/etc/runpod.json</code></li>
<li>Validates API key before proceeding</li>
<li>Estimates cost based on GPU selection</li>
</ul>
<p><strong>Template:</strong></p>
<ul>
<li>Uses pre-configured <code>metahuman-runpod-trainer</code> template</li>
<li>Includes all dependencies (Unsloth, Python packages)</li>
<li>Auto-destroys pod after training completes</li>
</ul>
<h3>Step 3: Review Dataset</h3>
<p>The wizard analyzes your data and shows:</p>
<p><strong>Dataset Statistics:</strong></p>
<pre><code>Total Memories: 1,247
‚îú‚îÄ Episodic Memories: 892
‚îú‚îÄ Therapy Sessions: 3
‚îú‚îÄ Chat Conversations: 340
‚îî‚îÄ Recent Memories (30 days): 287

Cognitive Mode Distribution:
‚îú‚îÄ Dual: 456 (51%)
‚îú‚îÄ Agent: 189 (21%)
‚îî‚îÄ Emulation: 247 (28%)

Estimated Training Samples: 2,847
Oldest Memory: 2024-03-15
Newest Memory: 2025-11-25
</code></pre>
<p><strong>Quality Indicators:</strong></p>
<ul>
<li>‚úÖ Sufficient memories (100+ minimum)</li>
<li>‚úÖ Diverse content (conversations + observations + interviews)</li>
<li>‚úÖ Recent activity (last 30 days)</li>
<li>‚ö†Ô∏è Warnings if dataset is small or stale</li>
</ul>
<h3>Step 4: Training Configuration</h3>
<p><strong>Basic Settings:</strong></p>
<ul>
<li><strong>Base Model</strong>: Which model to fine-tune<ul>
<li><code>unsloth/Qwen3-14B</code> (LoRA default, balanced)</li>
<li><code>unsloth/Qwen3-30B-Instruct</code> (Fine-tune, higher quality)</li>
</ul>
</li>
<li><strong>Training Epochs</strong>: 3 (LoRA) or 2 (Fine-tune)</li>
<li><strong>Max Samples</strong>: 3000 (LoRA) or 5000 (Fine-tune)</li>
</ul>
<p><strong>Monthly Training Mode</strong> (enabled by default):</p>
<ul>
<li><strong>Days Recent</strong>: 30 - Use last 30 days for recent adapter</li>
<li><strong>Old Samples</strong>: 3000 - Max samples from history for historical adapter</li>
<li><strong>Creates dual-adapter</strong>: Both historical + recent trained separately</li>
</ul>
<p><strong>Advanced Settings:</strong></p>
<ul>
<li><strong>LoRA Rank</strong>: 8 (default), higher = more capacity</li>
<li><strong>Learning Rate</strong>: 0.0002 (2e-4 for LoRA), 0.00002 (2e-5 for fine-tuning)</li>
<li><strong>Batch Size</strong>: 1 (LoRA), 4 (fine-tuning)</li>
<li><strong>Gradient Accumulation Steps</strong>: 16 (LoRA), 8 (fine-tuning)</li>
<li><strong>Max Sequence Length</strong>: 2048 (~1500 words)</li>
</ul>
<p><strong>Optional Features:</strong></p>
<ul>
<li><strong>Enable S3 Upload</strong>: Upload model to S3 after training (if configured)</li>
<li><strong>Enable Preprocessing</strong>: Curate dataset before training (recommended)</li>
</ul>
<p><strong>Configuration File</strong>: All settings saved to <code>etc/training.json</code></p>
<h3>Step 5: Launch Training</h3>
<p><strong>Pre-flight Checks:</strong></p>
<ol>
<li>Dataset validated</li>
<li>GPU availability confirmed (local) or RunPod ready (remote)</li>
<li>Configuration reviewed</li>
<li>Disk space verified (need ~10GB free)</li>
</ol>
<p><strong>Click &quot;Start Training&quot;</strong></p>
<p><strong>Training Process Begins:</strong></p>
<pre><code>üöÄ Training Launched
‚îú‚îÄ Run ID: 2025-11-25-143022
‚îú‚îÄ Method: local-lora (dual-adapter)
‚îú‚îÄ PID: 123456
‚îî‚îÄ Logs streaming...
</code></pre>
<h2>Training Process (Behind the Scenes)</h2>
<h3>1. Dataset Building</h3>
<p><strong>Curator Agent</strong> (<code>curator.ts</code>):</p>
<ul>
<li>Reads episodic memories from <code>profiles/&lt;username&gt;/memory/episodic/</code></li>
<li>Filters by date range (30 days for recent, all for historical)</li>
<li>Extracts conversation pairs</li>
<li>Assigns confidence scores based on:<ul>
<li>Message clarity</li>
<li>Persona relevance</li>
<li>Conversation coherence</li>
<li>Cognitive mode appropriateness</li>
</ul>
</li>
</ul>
<p><strong>Schema Application</strong> (<code>schema-manager.ts</code>):</p>
<ul>
<li>Converts memories to training format</li>
<li>Applies persona context (identity, values, goals)</li>
<li>Formats as instruction-response pairs</li>
<li>Tags with cognitive mode metadata</li>
</ul>
<p><strong>Output:</strong></p>
<pre><code>profiles/&lt;username&gt;/out/adapters/2025-11-25/
‚îú‚îÄ‚îÄ recent/
‚îÇ   ‚îî‚îÄ‚îÄ instructions.jsonl       # Recent 30 days
‚îú‚îÄ‚îÄ historical/
‚îÇ   ‚îî‚îÄ‚îÄ instructions.jsonl       # All past data
‚îî‚îÄ‚îÄ metadata.json
</code></pre>
<p><strong>Sample Format:</strong></p>
<pre><code class="language-json">{
  &quot;instruction&quot;: &quot;What are your thoughts on AI safety?&quot;,
  &quot;response&quot;: &quot;I believe AI safety is crucial. As someone who values transparency and ethical development, I think we need robust testing, clear guidelines, and ongoing oversight.&quot;,
  &quot;metadata&quot;: {
    &quot;cognitiveMode&quot;: &quot;dual&quot;,
    &quot;confidence&quot;: 0.89,
    &quot;timestamp&quot;: &quot;2025-11-20T14:30:00Z&quot;
  }
}
</code></pre>
<h3>2. Optional Auto-Approval</h3>
<p><strong>Auto-Approver Agent</strong> (<code>auto-approver.ts</code>):</p>
<ul>
<li>Reviews dataset quality automatically</li>
<li>Checks thresholds:<ul>
<li>Minimum pairs: 50</li>
<li>Minimum high-confidence: 70%</li>
<li>Minimum reflection percentage: 10%</li>
<li>Maximum low-confidence: 15%</li>
</ul>
</li>
<li><strong>Dry run mode</strong>: Preview without approving</li>
<li><strong>Real mode</strong>: Auto-approves if quality passes</li>
</ul>
<p><strong>Configuration</strong>: <code>profiles/&lt;username&gt;/etc/auto-approval.json</code></p>
<p><strong>Approval File</strong>: <code>out/adapters/2025-11-25/approved.json</code></p>
<h3>3. LoRA Training</h3>
<p><strong>Adapter Builder</strong> (<code>adapter-builder.ts</code>):</p>
<ul>
<li>Uses Unsloth for efficient LoRA training</li>
<li>Trains on GPU (local or RunPod)</li>
<li>Creates separate adapters for historical and recent data</li>
<li>Saves checkpoints every 50 epochs</li>
</ul>
<p><strong>Training Script</strong>: <code>external/train_unsloth.py</code></p>
<p><strong>Progress Monitoring:</strong></p>
<pre><code>[Epoch 1/3] Loss: 0.8234 | LR: 2e-4
[Epoch 2/3] Loss: 0.3421 | LR: 2e-4
[Epoch 3/3] Loss: 0.1567 | LR: 2e-4
Training complete! Adapter saved.
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>out/adapters/2025-11-25/
‚îú‚îÄ‚îÄ recent/
‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors
‚îÇ   ‚îî‚îÄ‚îÄ adapter_config.json
‚îî‚îÄ‚îÄ historical/
    ‚îú‚îÄ‚îÄ adapter_model.safetensors
    ‚îî‚îÄ‚îÄ adapter_config.json
</code></pre>
<h3>4. Adapter Merging</h3>
<p><strong>Adapter Merger</strong> (<code>adapter-merger.ts</code>):</p>
<ul>
<li>Merges historical and recent adapters</li>
<li>Creates consolidated base model</li>
<li>Preserves both adapters for dual-loading</li>
</ul>
<p><strong>Historical Merge</strong> (one-time):</p>
<ul>
<li>Merges all past adapters into <code>history-merged.gguf</code></li>
<li>Only runs if historical adapter changed</li>
<li>Saves merged GGUF to <code>out/adapters/_history/</code></li>
</ul>
<p><strong>Dual-Adapter Setup</strong>:</p>
<ul>
<li>Historical: <code>_history/history-merged.gguf</code></li>
<li>Recent: <code>2025-11-25/adapter.gguf</code></li>
<li>Both loaded simultaneously by Ollama</li>
</ul>
<h3>5. GGUF Conversion</h3>
<p><strong>GGUF Converter</strong> (<code>gguf-converter.ts</code>):</p>
<ul>
<li>Converts safetensors adapters to GGUF format</li>
<li>Creates Ollama Modelfile</li>
<li>Quantizes to Q4_K_M (balanced quality/size)</li>
</ul>
<p><strong>Modelfile Example:</strong></p>
<pre><code>FROM unsloth/Qwen3-14B
ADAPTER ./adapter.gguf
PARAMETER temperature 0.7
PARAMETER top_p 0.9
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>out/adapters/2025-11-25/
‚îú‚îÄ‚îÄ adapter.gguf                 # Recent adapter
‚îú‚îÄ‚îÄ Modelfile                    # Ollama config
‚îî‚îÄ‚îÄ model_info.json              # Metadata
</code></pre>
<h3>6. Model Activation</h3>
<p><strong>Set Active Adapter</strong> (<code>adapters.ts</code>):</p>
<ul>
<li>Registers new adapter as active</li>
<li>Updates <code>etc/active-adapter.json</code>:</li>
</ul>
<pre><code class="language-json">{
  &quot;modelName&quot;: &quot;greggles-dual-2025-11-25&quot;,
  &quot;activatedAt&quot;: &quot;2025-11-25T14:30:00Z&quot;,
  &quot;isDualAdapter&quot;: true,
  &quot;adapters&quot;: {
    &quot;historical&quot;: &quot;out/adapters/_history/history-merged.gguf&quot;,
    &quot;recent&quot;: &quot;out/adapters/2025-11-25/adapter.gguf&quot;
  },
  &quot;baseModel&quot;: &quot;unsloth/Qwen3-14B&quot;,
  &quot;trainingMethod&quot;: &quot;local-lora&quot;,
  &quot;runLabel&quot;: &quot;monthly-2025-11-25&quot;
}
</code></pre>
<p><strong>Ollama Boot Message</strong>:</p>
<pre><code>[llm] Using adapter: greggles-dual-2025-11-25
  ‚îú‚îÄ Historical: _history/history-merged.gguf
  ‚îî‚îÄ Recent: 2025-11-25/adapter.gguf
</code></pre>
<h3>7. Cleanup</h3>
<p><strong>Training Cleanup</strong> (<code>training-cleanup.ts</code>):</p>
<ul>
<li>Removes temporary files</li>
<li>Compresses logs</li>
<li>Deletes intermediate artifacts</li>
<li>Keeps only final adapters and metadata</li>
</ul>
<h2>Monitoring Training</h2>
<h3>Live Log Streaming</h3>
<p><strong>Console Logs</strong> (technical):</p>
<pre><code>[curator] Processing 1,247 memories...
[curator] Filtered to 2,847 training pairs
[curator] Average confidence: 0.82
[adapter-builder] Starting LoRA training...
[adapter-builder] Epoch 1/3: Loss 0.8234
[adapter-builder] Epoch 2/3: Loss 0.3421
[adapter-builder] Epoch 3/3: Loss 0.1567
[gguf-converter] Converting to GGUF format...
[full-cycle-local] ‚úì Training complete!
</code></pre>
<p><strong>Event Stream</strong> (user-friendly):</p>
<pre><code>üìä Dataset prepared: 2,847 samples
üß† Historical adapter: Training...
‚è±Ô∏è ETA: 25 minutes
‚úÖ Historical adapter: Complete
üîÑ Recent adapter: Training...
‚è±Ô∏è ETA: 15 minutes
‚úÖ Recent adapter: Complete
üîó Merging adapters...
‚úÖ Model activated: greggles-dual-2025-11-25
</code></pre>
<h3>Training UI</h3>
<p><strong>Progress Display:</strong></p>
<ul>
<li>Real-time progress bar (0-100%)</li>
<li>Current step indicator</li>
<li>Estimated time remaining</li>
<li>Live log tail (last 50 lines)</li>
</ul>
<p><strong>Cancellation:</strong></p>
<ul>
<li>Click <strong>&quot;Cancel Training&quot;</strong> button</li>
<li>Gracefully stops processes</li>
<li>Cleans up partial files</li>
<li>Preserves previous working adapter</li>
</ul>
<h2>Training Storage Structure</h2>
<p>All training data is stored per-user:</p>
<pre><code>profiles/&lt;username&gt;/out/adapters/
‚îú‚îÄ‚îÄ 2025-11-25/                  # Latest training run
‚îÇ   ‚îú‚îÄ‚îÄ recent/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ instructions.jsonl   # Recent dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îú‚îÄ‚îÄ historical/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ instructions.jsonl   # Historical dataset
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ adapter_model.safetensors
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ adapter_config.json
‚îÇ   ‚îú‚îÄ‚îÄ adapter.gguf             # Recent GGUF
‚îÇ   ‚îú‚îÄ‚îÄ Modelfile
‚îÇ   ‚îú‚îÄ‚îÄ metadata.json
‚îÇ   ‚îú‚îÄ‚îÄ approved.json
‚îÇ   ‚îî‚îÄ‚îÄ eval.json
‚îú‚îÄ‚îÄ 2025-10-25/                  # Previous run
‚îú‚îÄ‚îÄ 2025-09-25/                  # Older run
‚îî‚îÄ‚îÄ _history/
    ‚îî‚îÄ‚îÄ history-merged.gguf      # Consolidated historical
</code></pre>
<h2>Configuration Files</h2>
<h3>Main Training Config</h3>
<p><strong>Location</strong>: <code>etc/training.json</code></p>
<pre><code class="language-json">{
  &quot;base_model&quot;: &quot;unsloth/Qwen3-14B&quot;,
  &quot;num_train_epochs&quot;: 3,
  &quot;max_samples&quot;: 3000,
  &quot;monthly_training&quot;: true,
  &quot;days_recent&quot;: 30,
  &quot;old_samples&quot;: 3000,
  &quot;lora_rank&quot;: 8,
  &quot;learning_rate&quot;: 0.0002,
  &quot;per_device_train_batch_size&quot;: 1,
  &quot;gradient_accumulation_steps&quot;: 16,
  &quot;max_seq_length&quot;: 2048
}
</code></pre>
<p><strong>Environment Override</strong>:</p>
<pre><code class="language-bash"># Use different base model
export METAHUMAN_BASE_MODEL=&quot;unsloth/Qwen3-30B-Instruct&quot;
</code></pre>
<h3>Auto-Approval Config</h3>
<p><strong>Location</strong>: <code>profiles/&lt;username&gt;/etc/auto-approval.json</code></p>
<pre><code class="language-json">{
  &quot;enabled&quot;: true,
  &quot;dryRun&quot;: false,
  &quot;thresholds&quot;: {
    &quot;minPairs&quot;: 50,
    &quot;minHighConfidence&quot;: 0.7,
    &quot;minReflectionPct&quot;: 0.1,
    &quot;maxLowConfidence&quot;: 0.15
  },
  &quot;alertEmail&quot;: null
}
</code></pre>
<h3>RunPod Config</h3>
<p><strong>Location</strong>: <code>profiles/&lt;username&gt;/etc/runpod.json</code></p>
<pre><code class="language-json">{
  &quot;apiKey&quot;: &quot;RUNPOD_API_KEY_HERE&quot;,
  &quot;templateId&quot;: &quot;metahuman-runpod-trainer&quot;,
  &quot;gpuType&quot;: &quot;NVIDIA H100 PCIe&quot;,
  &quot;costEstimate&quot;: &quot;$1.50&quot;
}
</code></pre>
<h2>Dual-Adapter System Explained</h2>
<h3>Why Two Adapters?</h3>
<p><strong>Historical Adapter:</strong></p>
<ul>
<li>Captures entire personality history</li>
<li>Stable, well-established patterns</li>
<li>All memories ever collected</li>
<li>Merged incrementally (only updates when historical data changes)</li>
</ul>
<p><strong>Recent Adapter:</strong></p>
<ul>
<li>Fresh, current data (last 30 days)</li>
<li>Captures recent shifts and new patterns</li>
<li>Trained every month</li>
<li>Replaces previous recent adapter</li>
</ul>
<p><strong>Combined Loading:</strong></p>
<ul>
<li>Ollama loads both adapters simultaneously</li>
<li>Historical provides foundation</li>
<li>Recent adds current context</li>
<li>Balanced personality: stable + adaptive</li>
</ul>
<h3>Adapter Lifecycle</h3>
<p><strong>First Training:</strong></p>
<ol>
<li>Collect 100+ memories</li>
<li>Train initial adapter</li>
<li>Both historical and recent are the same</li>
</ol>
<p><strong>Second Training (30 days later):</strong></p>
<ol>
<li>Historical: Use previous adapter as base, merge with new data</li>
<li>Recent: Train fresh on last 30 days</li>
<li>Now using dual-adapter system</li>
</ol>
<p><strong>Third Training (60 days later):</strong></p>
<ol>
<li>Historical: Merge previous historical + previous recent</li>
<li>Recent: Train fresh on last 30 days</li>
<li>Consolidated history grows, recent stays fresh</li>
</ol>
<h2>Best Practices</h2>
<h3>Data Collection</h3>
<ol>
<li><strong>Quantity</strong>: Aim for 100+ memories before first training</li>
<li><strong>Quality</strong>: Use persona generator for rich interview data</li>
<li><strong>Diversity</strong>: Mix conversations, observations, reflections</li>
<li><strong>Cognitive modes</strong>: Use Dual mode for training data (richer context)</li>
<li><strong>Regular updates</strong>: Add memories daily, train monthly</li>
</ol>
<h3>Training Frequency</h3>
<ul>
<li><strong>Active users</strong>: Monthly (automated if <code>monthly_training: true</code>)</li>
<li><strong>Moderate users</strong>: Quarterly</li>
<li><strong>Light users</strong>: When you have 500+ new memories</li>
<li><strong>After major changes</strong>: Immediately after personality updates</li>
</ul>
<h3>GPU Requirements</h3>
<p><strong>Local Training:</strong></p>
<ul>
<li>LoRA (14B model): 24GB VRAM</li>
<li>LoRA (20B model): 32GB VRAM</li>
<li>Fine-Tuning (30B model): 40GB+ VRAM</li>
</ul>
<p><strong>RunPod Recommendations:</strong></p>
<ul>
<li>H100: Best performance, ~$2/hour</li>
<li>A100: Good balance, ~$1.50/hour</li>
<li>RTX 4090: Budget option, ~$0.50/hour</li>
</ul>
<h3>Quality Optimization</h3>
<ol>
<li><strong>Clean data</strong>: Review memories, delete low-quality entries</li>
<li><strong>Persona accuracy</strong>: Keep persona up to date</li>
<li><strong>Confidence thresholds</strong>: Adjust in auto-approval config</li>
<li><strong>Training epochs</strong>: 2-3 for LoRA, 1-2 for fine-tuning</li>
<li><strong>Evaluation</strong>: Test adapter after training, verify responses</li>
</ol>
<h2>Troubleshooting</h2>
<h3>Training Fails to Start</h3>
<ul>
<li>Check GPU availability: <code>nvidia-smi</code> (local)</li>
<li>Verify RunPod API key (remote)</li>
<li>Check disk space (need 10GB+ free)</li>
<li>Review training logs for specific error</li>
</ul>
<h3>Poor Model Quality</h3>
<ul>
<li>Need more training data (100+ memories minimum)</li>
<li>Increase training epochs (3-5 for LoRA)</li>
<li>Adjust learning rate (try 0.0001 or 0.0003)</li>
<li>Review dataset quality (low confidence samples)</li>
<li>Ensure persona is accurate</li>
</ul>
<h3>Out of Memory (OOM)</h3>
<ul>
<li>Reduce batch size to 1</li>
<li>Reduce gradient accumulation steps</li>
<li>Use smaller base model (7B instead of 14B)</li>
<li>Use RunPod with larger GPU</li>
</ul>
<h3>Training Stuck/Frozen</h3>
<ul>
<li>Check GPU temperature (may be throttling)</li>
<li>Kill stuck processes: <code>./bin/mh agent ps</code> then kill PID</li>
<li>Check training logs for last activity</li>
<li>Restart with fresh run</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Configure <a href="cognitive-modes.md">Cognitive Modes</a> to use your trained adapter</li>
<li>Use trained model in <a href="../using-metahuman/chat-interface.md">Chat Interface</a></li>
<li>Monitor adapter quality in <a href="../using-metahuman/dashboard-monitoring.md">Dashboard</a></li>
<li>Combine with <a href="voice-training.md">Voice Training</a> for complete personalization</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl>  <button class="nav-button next-button" data-target="authentication" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Authentication</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="authentication" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h2>Authentication, Profiles &amp; Guest Access</h2>
<p>MetaHuman OS now ships with a fully integrated authentication layer. Every request resolves inside a user context that carries role, profile paths, and session metadata. This section explains how to register owners, invite guests, manage visibility, and migrate from older single-user deployments.</p>
<hr>
<h3>1. Roles &amp; Session Defaults</h3>
<table>
<thead>
<tr>
<th>Role</th>
<th>How it is created</th>
<th>Session Length</th>
<th>Capabilities</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Owner</strong></td>
<td>First account created via UI (or script)</td>
<td>24 hours</td>
<td>Full read/write access to their profile; can adjust settings, trust levels, and profile visibility.</td>
</tr>
<tr>
<td><strong>Guest</strong></td>
<td>Owner-created account (future UI) or existing credentials</td>
<td>1 hour</td>
<td>Always forced into emulation mode. Read-only access to the selected public profile.</td>
</tr>
<tr>
<td><strong>Anonymous</strong></td>
<td><strong>Continue as Guest</strong> (no credentials)</td>
<td>30 minutes</td>
<td>Must choose from public profiles; cannot write or access private data. API calls touching user data return <code>401/403</code>.</td>
</tr>
</tbody></table>
<p>Session cookies (<code>mh_session</code>) are HTTPOnly. Closing the browser does not log out; use the profile menu ‚Üí <strong>Logout</strong> to end a session immediately.</p>
<hr>
<h3>2. Creating the First Owner</h3>
<ol>
<li>Start the web UI:<pre><code class="language-bash">cd apps/site &amp;&amp; pnpm dev
# visit http://localhost:4321
</code></pre>
</li>
<li>The Authentication Gate appears after the splash screen. Choose <strong>Create Account</strong>.</li>
<li>Fill in the username, password, and optional display name. The first account automatically receives the <code>owner</code> role.</li>
<li>After registration you are redirected to the dashboard and a profile folder is created at <code>profiles/&lt;owner&gt;/</code>.</li>
</ol>
<h4>Legacy Installations</h4>
<p>Upgrading from the single-user layout? Run the migration script before launching the UI:</p>
<pre><code class="language-bash">pnpm tsx scripts/migrate-to-profiles.ts --username &lt;owner&gt;
</code></pre>
<p>This moves <code>memory/</code>, <code>persona/</code>, <code>logs/</code>, <code>etc/</code>, and voice-training data into <code>profiles/&lt;owner&gt;/‚Ä¶</code> while keeping shared voices under <code>out/voices/</code>.</p>
<hr>
<h3>3. Logging In &amp; Switching Accounts</h3>
<ol>
<li>Open the header menu (persona name) and click <strong>Login</strong>.</li>
<li>Enter your username and password.</li>
<li>Owners see their visibility badge and trust controls in the left sidebar once authenticated.</li>
<li>To switch users, log out first. Re-authenticate with different credentials or pick <strong>Continue as Guest</strong>.</li>
</ol>
<hr>
<h3>4. Guest Sessions &amp; Public Profiles</h3>
<p>Choosing <strong>Continue as Guest</strong> starts a short-lived anonymous session:</p>
<ol>
<li>The guest is prompted to pick from <strong>public</strong> personas. Private profiles are hidden.</li>
<li>Once a profile is selected, the guest enters the dashboard in read-only emulation mode. Memory writes, task updates, and configuration changes are blocked by the security policy.</li>
<li>Guests can explore chat, memories, and voice outputs for that persona, but audio recordings and training samples remain private to the owner.</li>
</ol>
<p>To make a persona available to guests, owners must mark it as <strong>Public</strong> (see below). Switching back to <strong>Private</strong> invalidates existing guest selections automatically‚Äîguests are asked to choose another profile.</p>
<hr>
<h3>5. Profile Visibility</h3>
<p>Owners control whether their persona appears in the guest selector.</p>
<ol>
<li>Navigate to <strong>System ‚Üí Settings ‚Üí Profile Visibility</strong>.</li>
<li>Choose:<ul>
<li><code>Private</code> ‚Äì Hidden from guest selector (default).</li>
<li><code>Public</code> ‚Äì Visible to anonymous/guest sessions.</li>
</ul>
</li>
<li>Visibility status is shown next to the profile badge in the sidebar. Changing the flag edits <code>profiles/&lt;owner&gt;/persona/core.json</code> metadata and the UI reflects it immediately.</li>
</ol>
<blockquote>
<p><strong>Tip:</strong> Voice training data remains private even when a profile is public. Only the chat and memory streams become browseable.</p>
</blockquote>
<hr>
<h3>6. Configuration &amp; Data Locations</h3>
<ul>
<li><strong>Authentication Database:</strong> <code>persona/users.json</code> (owner + guest credentials and metadata).</li>
<li><strong>User Profiles:</strong> <code>profiles/&lt;username&gt;/‚Ä¶</code> (memories, persona, logs, etc.).</li>
<li><strong>Voice Settings:</strong> <code>profiles/&lt;username&gt;/etc/voice.json</code>.</li>
<li><strong>Shared Piper voices:</strong> <code>out/voices/</code>.</li>
<li><strong>Audit Logs:</strong> <code>logs/audit/YYYY-MM-DD.ndjson</code> (capture acting user IDs).</li>
</ul>
<hr>
<h3>7. CLI &amp; Script Helpers</h3>
<p>The CLI automatically operates under the first owner profile unless <code>--user &lt;username&gt;</code> is supplied. For administrative tasks that still require scripting (e.g., resetting passwords) you can interact with the user API directly:</p>
<pre><code class="language-ts">import { createUser, deleteUser, listUsers } from &#39;@metahuman/core/users&#39;;
</code></pre>
<blockquote>
<p><strong>Warning:</strong> Do <strong>not</strong> edit <code>persona/users.json</code> by hand. Use the API helpers or the forthcoming owner UI to modify user records safely.</p>
</blockquote>
<hr>
<h3>8. Development Authentication Helper</h3>
<p>For developers working on the codebase, repeatedly logging in can be tedious. Use the dev-session helper script:</p>
<pre><code class="language-bash"># Create a long-lived session (30 days)
pnpm tsx scripts/dev-session.ts --username=greggles

# Output will show:
# ‚úÖ Dev session created successfully!
# Session ID: 38d26955b5588a341b78bfee344f637341758298af02f37a72e49630682fd6b4
# üç™ Copy this cookie value into your browser...
</code></pre>
<p><strong>To use the session:</strong></p>
<ol>
<li>Open DevTools (F12)</li>
<li>Go to Application ‚Üí Cookies ‚Üí <a href="http://localhost:4321">http://localhost:4321</a></li>
<li>Add cookie: <code>mh_session</code> = <code>&lt;session-id-from-script&gt;</code></li>
<li>Reload the page</li>
</ol>
<p>This eliminates authentication friction during development. The session persists for 30 days and is saved to <code>.env.development</code> for reference.</p>
<p><strong>Security Note:</strong> Only use this on your local development machine. Never use dev sessions in production or share session IDs.</p>
<hr>
<h3>9. Troubleshooting</h3>
<table>
<thead>
<tr>
<th>Issue</th>
<th>Fix</th>
</tr>
</thead>
<tbody><tr>
<td><strong>&quot;Authentication required&quot; when hitting APIs</strong></td>
<td>Ensure you are logged in. Anonymous sessions cannot access profile endpoints. Use the dev-session helper for local development.</td>
</tr>
<tr>
<td><strong>&quot;Access denied: Anonymous users cannot access user data paths&quot;</strong></td>
<td>This is the old error message. The new streamlined system returns clean 401 responses instead. Update your API endpoints to use <code>tryResolveProfilePath()</code> helper. See <a href="../AUTHENTICATION_STREAMLINED.md">AUTHENTICATION_STREAMLINED.md</a>.</td>
</tr>
<tr>
<td><strong>Guest can&#39;t see a persona</strong></td>
<td>Confirm the owner marked the profile as <code>Public</code>. Visibility changes take effect immediately.</td>
</tr>
<tr>
<td><strong>Session expires unexpectedly</strong></td>
<td>Check system clock, review <code>logs/run/sessions.json</code>, and confirm <code>pnpm dev</code> output for validation errors.</td>
</tr>
<tr>
<td><strong>Forgot owner password</strong></td>
<td>Stop the server, run a short script using <code>deleteUser(userId)</code> then re-run <code>createUser()</code> with new credentials, or temporarily remove the entry from <code>persona/users.json</code> and restart.</td>
</tr>
<tr>
<td><strong>Legacy data still in root directories</strong></td>
<td>Re-run <code>pnpm tsx scripts/migrate-to-profiles.ts --username &lt;owner&gt;</code> and verify symlinks/old folders were moved.</td>
</tr>
</tbody></table>
<hr>
<h3>10. API Endpoint Authentication Patterns</h3>
<p>For developers building API endpoints, MetaHuman OS provides streamlined path resolution helpers that eliminate &quot;anonymous user&quot; errors:</p>
<h4>Path Resolution Helpers</h4>
<p>Instead of directly accessing <code>paths.*</code> (which throws for anonymous users), use:</p>
<pre><code class="language-typescript">import { tryResolveProfilePath, requireProfilePath, systemPaths } from &#39;@metahuman/core&#39;;

// For public reads (return defaults for anonymous users)
const result = tryResolveProfilePath(&#39;personaCore&#39;);
if (!result.ok) {
  return new Response(JSON.stringify({ default: &#39;data&#39; }), { status: 200 });
}
const data = fs.readFileSync(result.path, &#39;utf-8&#39;);

// For protected operations (return 401 for anonymous users)
const result = tryResolveProfilePath(&#39;episodic&#39;);
if (!result.ok) {
  return new Response(
    JSON.stringify({ error: &#39;Authentication required&#39; }),
    { status: 401 }
  );
}
fs.writeFileSync(path.join(result.path, &#39;event.json&#39;), data);

// For system operations (not user-specific)
const agentPath = path.join(systemPaths.brain, &#39;agents&#39;, `${name}.ts`);
</code></pre>
<h4>API Endpoint Categories</h4>
<ol>
<li><p><strong>Public Reads</strong>: Degrade gracefully for anonymous users</p>
<ul>
<li>Examples: <code>/api/boot</code>, <code>/api/persona-core</code> (GET)</li>
<li>Return sensible defaults instead of errors</li>
</ul>
</li>
<li><p><strong>Protected Operations</strong>: Require authentication</p>
<ul>
<li>Examples: <code>/api/capture</code>, <code>/api/memories</code>, <code>/api/tasks</code></li>
<li>Return 401 with clear error message</li>
</ul>
</li>
<li><p><strong>System Operations</strong>: Use <code>systemPaths</code> directly</p>
<ul>
<li>Examples: <code>/api/agent</code>, <code>/api/models</code>, <code>/api/auth/*</code></li>
<li>Never touch user-specific paths</li>
</ul>
</li>
</ol>
<p>See <a href="../AUTHENTICATION_STREAMLINED.md">AUTHENTICATION_STREAMLINED.md</a> for complete implementation guide.</p>
<hr>
<h3>11. Security Checklist</h3>
<ul>
<li>Keep only the intended personas public. Use private visibility when sharing the instance on a network.</li>
<li>Regularly inspect <code>logs/audit/*.ndjson</code> to confirm guest sessions remain read-only.</li>
<li>Use <code>HIGH_SECURITY=true</code> in <code>.env</code> to force emulation mode across all roles during demonstrations.</li>
<li>Back up both <code>profiles/</code> and <code>persona/users.json</code> together‚Äîthey form the complete data set.</li>
</ul>
<hr>
<h3>12. Next Steps</h3>
<ul>
<li>Learn about <a href="19-multi-user-profiles.md">Multi-User Profiles &amp; Guest Mode</a> for detailed information on persona facets, profile switching, and the special &quot;Mutant Super Intelligence&quot; feature.</li>
<li>Review <a href="10-security-trust.md">Security &amp; Trust</a> for trust levels and directory boundaries.</li>
<li>Update profile-specific configuration in <a href="14-configuration-files.md"><code>profiles/&lt;username&gt;/etc/</code></a>.</li>
<li>If you need to expose the UI remotely, follow the <a href="17-cloudflare-tunnel-setup.md">Cloudflare Tunnel guide</a> after hardening credentials.</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="ai-training" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Ai Training</span> </span> </button> <button class="nav-button next-button" data-target="autonomous-agents" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Autonomous Agents</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="autonomous-agents" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Autonomous Agents</h1>
<p>MetaHuman OS includes a comprehensive suite of <strong>40 autonomous agents</strong> that run as background processes to enrich memories, generate reflections, train models, and maintain the system. These agents are coordinated by the <strong>Scheduler Service</strong> and can be triggered by intervals, time-of-day, events, or user activity.</p>
<blockquote>
<p><strong>Source of Truth</strong>: This documentation is generated from actual agent implementations in <a href="../../brain/agents/"><code>brain/agents/</code></a> as of 2025-11-25.</p>
</blockquote>
<hr>
<h2>Agent Categories</h2>
<p>Agents are organized by function:</p>
<ol>
<li><a href="#memory-processing-agents">Memory Processing</a> ‚Äî Enrich and organize memories</li>
<li><a href="#curiosity--reflection-agents">Curiosity &amp; Reflection</a> ‚Äî Generate questions and internal thoughts</li>
<li><a href="#sleep--night-processing-agents">Sleep &amp; Night Processing</a> ‚Äî Nightly pipelines and dream generation</li>
<li><a href="#dataset-building-agents">Dataset Building</a> ‚Äî Prepare training data from memories</li>
<li><a href="#training-pipeline-agents">Training Pipeline</a> ‚Äî LoRA and fine-tuning orchestration</li>
<li><a href="#training-support-agents">Training Support</a> ‚Äî Utilities for model training</li>
<li><a href="#persona--analysis-agents">Persona &amp; Analysis</a> ‚Äî Personality evolution and understanding</li>
<li><a href="#audio-processing-agents">Audio Processing</a> ‚Äî Transcription and audio memory conversion</li>
<li><a href="#system-services">System Services</a> ‚Äî Core infrastructure and scheduling</li>
</ol>
<hr>
<h2>Memory Processing Agents</h2>
<h3>1. Organizer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/organizer.ts</code></p>
<p><strong>Purpose</strong>: Automatically processes and enriches memories with LLM-extracted tags and entities.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Scans episodic memories for unprocessed entries (across all users)</li>
<li>Uses LLM to extract:<ul>
<li>Tags (lowercase keywords)</li>
<li>Entities (people, places, organizations)</li>
</ul>
</li>
<li>Updates <code>metadata.processed = true</code> when complete</li>
<li><strong>MULTI-USER</strong>: Processes all logged-in users sequentially with isolated contexts</li>
</ul>
<p><strong>Trigger</strong>: Activity-based (300s inactivity) via <code>etc/agents.json</code></p>
<p><strong>Configuration</strong>: <code>etc/agents.json</code> ‚Üí <code>organizer</code> section</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run organizer
</code></pre>
<hr>
<h3>2. Ingestor Agent</h3>
<p><strong>File</strong>: <code>brain/agents/ingestor.ts</code></p>
<p><strong>Purpose</strong>: Converts raw files in <code>memory/inbox/</code> into episodic memories.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Reads files from <code>memory/inbox/</code></li>
<li>Splits long content into 2000-character chunks</li>
<li>Creates episodic events for each chunk</li>
<li>Moves processed files to <code>memory/inbox/_archive/YYYY-MM-DD/</code></li>
<li>Supports JSON and text files</li>
</ul>
<p><strong>Trigger</strong>: Manual or scheduled</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh ingest &lt;file-or-directory&gt;
</code></pre>
<hr>
<h3>3. AI Ingestor Agent</h3>
<p><strong>File</strong>: <code>brain/agents/ai-ingestor.ts</code></p>
<p><strong>Purpose</strong>: LLM-powered classification and summarization before saving memories.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Reads raw files from <code>memory/inbox/</code></li>
<li>Uses LLM (via <code>callLLMJSON</code> with &#39;curator&#39; role) to classify:<ul>
<li>Type: conversation, journal, search, fragment, observation, inner_dialogue, reflection</li>
<li>Title: short (‚â§10 words)</li>
<li>Summary: 1-3 sentences</li>
<li>Tags: ‚â§6 lowercase keywords</li>
<li>Entities: names/orgs/places</li>
<li>Quality: 0-1 (skips spam/garbage with &lt;0.4)</li>
</ul>
</li>
<li>Saves compact episodic events (content = summary)</li>
<li>Links back to source filename</li>
<li>Archives originals to <code>_archive/YYYY-MM-DD/</code></li>
</ul>
<p><strong>Trigger</strong>: Manual</p>
<p><strong>Configuration</strong>: <code>etc/ai-ingestor.json</code></p>
<hr>
<h3>4. Audio Organizer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/audio-organizer.ts</code></p>
<p><strong>Purpose</strong>: Converts audio transcripts into episodic memories with LLM-extracted metadata.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Polls every 15 minutes for unorganized transcripts</li>
<li>Generates summary and extracts metadata using LLM</li>
<li>Creates episodic events with tags and entities</li>
<li>Marks transcripts as <code>organized: true</code></li>
</ul>
<p><strong>Trigger</strong>: Night pipeline (via <code>night-processor.ts</code>)</p>
<p><strong>Configuration</strong>: <code>etc/audio.json</code></p>
<hr>
<h3>5. Memory Metrics Cache Agent</h3>
<p><strong>File</strong>: <code>brain/agents/memory-metrics-cache.ts</code></p>
<p><strong>Purpose</strong>: Background job that updates metrics cache for all users.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Iterates through all logged-in users</li>
<li>Updates metrics cache via <code>updateMetricsCache()</code></li>
<li>Cleans up orphaned tool outputs (90 days retention)</li>
<li><strong>MULTI-USER</strong>: Processes each user with isolated context</li>
</ul>
<p><strong>Trigger</strong>: Scheduled interval (configurable)</p>
<hr>
<h2>Curiosity &amp; Reflection Agents</h2>
<h3>6. Reflector Agent</h3>
<p><strong>File</strong>: <code>brain/agents/reflector.ts</code></p>
<p><strong>Purpose</strong>: Generates internal reflections using associative memory chains.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Considers <strong>all memories</strong> (no pool limit, entire lifetime)</li>
<li>Builds chains of 3-5 associated memories by following keyword connections</li>
<li>Simulates train of thought (one memory triggers search for related memories)</li>
<li>Uses weighted selection (14-day decay factor allows old memories to surface)</li>
<li>Example: &quot;coffee with Sarah&quot; ‚Üí search(&quot;Sarah&quot;) ‚Üí &quot;Sarah mentioned project&quot; ‚Üí search(&quot;project&quot;) ‚Üí reflection</li>
<li><strong>Deprioritizes technical keywords</strong> to avoid self-referential content (e.g., &quot;metahuman&quot;, &quot;organizer&quot;, &quot;reflector&quot;)</li>
<li>Can optionally use recursive train-of-thought mode (via <code>REFLECTOR_USE_TRAIN_OF_THOUGHT=true</code>)</li>
<li><strong>IMPORTANT</strong>: Saves as <code>type: &#39;inner_dialogue&#39;</code> ‚Äî reflections are internal thoughts only, visible in Inner Dialogue tab, NEVER in main chat</li>
</ul>
<p><strong>Trigger</strong>: Activity-based via boredom-maintenance (900s inactivity)</p>
<p><strong>Configuration</strong>: <code>etc/agents.json</code> ‚Üí <code>boredom-maintenance</code> section</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run reflector
</code></pre>
<hr>
<h3>7. Train-of-Thought Agent</h3>
<p><strong>File</strong>: <code>brain/agents/train-of-thought.ts</code></p>
<p><strong>Purpose</strong>: Performs recursive reasoning by following memory associations.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Builds associative chains of memories</li>
<li>Follows semantic connections between events</li>
<li>Generates deep insights through multi-hop reasoning</li>
<li>Can be triggered:<ol>
<li>Directly via CLI: <code>./bin/mh agent run train-of-thought</code></li>
<li>From reflector agent (via <code>agent_trigger</code> node)</li>
<li>From inner-curiosity for deeper exploration</li>
</ol>
</li>
</ul>
<p><strong>Trigger</strong>: On-demand or via reflector/inner-curiosity</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run train-of-thought
</code></pre>
<hr>
<h3>8. Curiosity Service Agent</h3>
<p><strong>File</strong>: <code>brain/agents/curiosity-service.ts</code></p>
<p><strong>Purpose</strong>: Monitors user inactivity and asks thoughtful questions in main chat.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Uses node-based workflow (<code>etc/cognitive-graphs/curiosity-mode.json</code>)</li>
<li>Weighted memory selection (14-day decay, same as reflector)</li>
<li>Samples from <strong>all memories</strong> (entire lifetime), not just recent 7 days</li>
<li>Deprioritizes technical keywords to avoid meta-questions</li>
<li>Questions appear in chat as <code>üí≠ I&#39;m curious: &lt;question&gt;</code></li>
<li>User can reply, triggering conversation and memory storage</li>
<li>Respects <code>maxOpenQuestions</code> limit and trust/autonomy policies</li>
<li><strong>MULTI-USER</strong>: Processes all users sequentially with isolated contexts</li>
</ul>
<p><strong>Trigger</strong>: Activity-based (900s inactivity) via <code>etc/agents.json</code></p>
<p><strong>Configuration</strong>: <code>etc/curiosity.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run curiosity-service
</code></pre>
<hr>
<h3>9. Inner Curiosity Agent</h3>
<p><strong>File</strong>: <code>brain/agents/inner-curiosity.ts</code></p>
<p><strong>Purpose</strong>: Generates self-directed questions and answers them internally.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Uses weighted memory selection (same algorithm as reflector/curiosity)</li>
<li>Generates question about patterns/connections in memories</li>
<li>Searches local memory for relevant context</li>
<li>Synthesizes thoughtful answer from findings</li>
<li>Saves as <code>type: &#39;inner_dialogue&#39;</code> ‚Äî NEVER appears in main chat</li>
<li><strong>MULTI-USER</strong>: Processes all logged-in users sequentially</li>
<li>Example output: <code>ü§î &lt;question&gt;\n\nüí≠ &lt;answer&gt;</code></li>
</ul>
<p><strong>Trigger</strong>: Configurable interval (default: 1800s)</p>
<p><strong>Configuration</strong>: <code>etc/curiosity.json</code> ‚Üí <code>innerQuestionMode</code> (&#39;off&#39;, &#39;local&#39;, &#39;web&#39;), interval</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run inner-curiosity
</code></pre>
<hr>
<h3>10. Curiosity Researcher Agent</h3>
<p><strong>File</strong>: <code>brain/agents/curiosity-researcher.ts</code></p>
<p><strong>Purpose</strong>: Performs deeper research on pending curiosity questions.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Extracts key topics from questions using LLM</li>
<li>Searches episodic memory for question-related content (5 results per topic, max 3 topics)</li>
<li>Generates research notes and summaries</li>
<li>Saves findings as <code>inner_dialogue</code> events</li>
<li>Web search integration placeholder (planned)</li>
<li><strong>MULTI-USER</strong>: Processes all users sequentially</li>
</ul>
<p><strong>Trigger</strong>: Manual or scheduled</p>
<p><strong>Configuration</strong>: <code>etc/curiosity.json</code> ‚Üí <code>researchMode</code> (&#39;off&#39;, &#39;local&#39;, &#39;web&#39;)</p>
<hr>
<h2>Sleep &amp; Night Processing Agents</h2>
<h3>11. Dreamer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/dreamer.ts</code></p>
<p><strong>Purpose</strong>: Creates surreal dreams from lifetime memory fragments.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Accesses <strong>all episodic memories</strong> (entire lifetime, no time limit)</li>
<li>Uses reflective exponential decay weighting:<ul>
<li>Formula: <code>weight = exp(-ageInDays / 227)</code></li>
<li>1-year-old memories retain ~20% probability</li>
<li>Contemplative weighting allows older memories to appear frequently</li>
</ul>
</li>
<li>Curates weighted sample of diverse memories (default: 15 memories)</li>
<li>Generates surreal dream narratives using LLM</li>
<li>Extracts preferences and heuristics</li>
<li>Writes overnight learnings to procedural memory</li>
<li><strong>MULTI-USER</strong>: Processes all users sequentially</li>
</ul>
<p><strong>Trigger</strong>: Night pipeline (via <code>sleep-service.ts</code>)</p>
<p><strong>Configuration</strong>: <code>etc/sleep.json</code> ‚Üí <code>maxDreamsPerNight</code>, <code>evaluate</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run dreamer
</code></pre>
<hr>
<h3>12. Sleep Service</h3>
<p><strong>File</strong>: <code>brain/agents/sleep-service.ts</code></p>
<p><strong>Purpose</strong>: Orchestrates the nightly processing pipeline.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Checks if current time is within sleep window (e.g., 23:00‚Äì06:30)</li>
<li>Verifies system idle threshold (e.g., 15 minutes)</li>
<li>Runs nightly pipeline:<ol>
<li>Dreamer agent (with <code>maxDreamsPerNight</code> limit)</li>
<li>Audio processing (transcriber + audio-organizer)</li>
<li>LoRA training pipeline (adapter-builder, auto-approver, trainer, eval, activation)</li>
</ol>
</li>
<li><strong>MULTI-USER</strong>: System-level orchestrator, triggers multi-user agents internally</li>
</ul>
<p><strong>Trigger</strong>: Night pipeline (via <code>night-pipeline.ts</code>)</p>
<p><strong>Configuration</strong>: <code>etc/sleep.json</code></p>
<p><strong>Functions</strong>:</p>
<ul>
<li><code>loadSleepConfig()</code> ‚Äî Read configuration</li>
<li><code>isSleepTime()</code> ‚Äî Check if within sleep window</li>
<li><code>isIdle()</code> ‚Äî Check system idle duration</li>
<li><code>runNightlyPipeline()</code> ‚Äî Execute full pipeline</li>
</ul>
<hr>
<h3>13. Night Pipeline Agent</h3>
<p><strong>File</strong>: <code>brain/agents/night-pipeline.ts</code></p>
<p><strong>Purpose</strong>: Wrapper agent triggered by scheduler to run nightly processing.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Calls <code>sleep-service.ts</code> functions</li>
<li>Actual pipeline orchestration handled by <code>sleep-service.ts</code></li>
</ul>
<p><strong>Trigger</strong>: Time-of-day (02:00) via <code>etc/agents.json</code></p>
<p><strong>Configuration</strong>: <code>etc/agents.json</code> ‚Üí <code>night-pipeline</code> section</p>
<hr>
<h3>14. Night Processor Agent</h3>
<p><strong>File</strong>: <code>brain/agents/night-processor.ts</code></p>
<p><strong>Purpose</strong>: Runs transcriber and audio-organizer in one-shot mode.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Spawns <code>transcriber.ts</code> with <code>ONESHOT=1</code> env var</li>
<li>Spawns <code>audio-organizer.ts</code> with <code>ONESHOT=1</code> env var</li>
<li>Waits for both to complete</li>
<li>Audits exit codes (success/failure)</li>
</ul>
<p><strong>Trigger</strong>: Night pipeline (via <code>sleep-service.ts</code>)</p>
<hr>
<h3>15. Morning Loader Agent</h3>
<p><strong>File</strong>: <code>brain/agents/morning-loader.ts</code></p>
<p><strong>Purpose</strong>: Loads overnight learnings and activates daily operator profile.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Loads base persona from <code>persona/core.json</code></li>
<li>Finds most recent overnight learnings file (<code>memory/procedural/overnight/overnight-learnings-*.md</code>)</li>
<li>Composes daily operator profile (base persona + overnight learnings)</li>
<li>Activates new profile as prompt adapter (Tier 1 model adaptation)</li>
<li>Audits activation</li>
</ul>
<p><strong>Trigger</strong>: End of sleep cycle (after dreamer completes)</p>
<p><strong>Implementation</strong>: Tier 1 model adaptation (prompt adapter only, no training)</p>
<hr>
<h2>Dataset Building Agents</h2>
<h3>16. Curator Agent</h3>
<p><strong>File</strong>: <code>brain/agents/curator.ts</code></p>
<p><strong>Purpose</strong>: Prepares clean, persona-friendly training data.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Processes raw episodic memories into curated summaries</li>
<li>Uses LLM to remove:<ul>
<li>Tool syntax</li>
<li>JSON fragments</li>
<li>Operator transcripts</li>
</ul>
</li>
<li>Runs incrementally (~50 uncurated memories per run)</li>
<li>Outputs to <code>memory/curated/conversations/*.json</code></li>
<li><strong>MULTI-USER</strong>: Processes user-specific memories</li>
</ul>
<p><strong>Trigger</strong>: Scheduled interval (1800s / 30 min) via <code>etc/agents.json</code></p>
<p><strong>Configuration</strong>: <code>etc/curator.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run curator
</code></pre>
<hr>
<h3>17. AI Dataset Builder Agent</h3>
<p><strong>File</strong>: <code>brain/agents/ai-dataset-builder.ts</code></p>
<p><strong>Purpose</strong>: Transforms entire memory corpus into instruction tuning samples.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Uses currently configured LLM (Ollama or adapter)</li>
<li>Processes memories in batches (configurable <code>chunkSize</code>)</li>
<li>Generates high-quality instruction‚Üíoutput pairs</li>
<li>Supports:<ul>
<li><code>--max &lt;number&gt;</code>: Maximum memories to process</li>
<li><code>--chunk &lt;number&gt;</code>: Memories per LLM batch</li>
<li><code>--model &lt;name&gt;</code>: Override model name</li>
<li><code>--username &lt;name&gt;</code>: User whose memories to process</li>
</ul>
</li>
<li>Outputs JSONL to <code>out/datasets/YYYY-MM-DD/ai_dataset.jsonl</code></li>
</ul>
<p><strong>Trigger</strong>: Manual (typically during training preparation)</p>
<p><strong>Configuration</strong>: <code>etc/ai-dataset-builder.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">pnpm tsx brain/agents/ai-dataset-builder.ts --output ./out/datasets/2025-10-31/ai_dataset.jsonl --username greggles --max 2000
</code></pre>
<hr>
<h3>18. AI Dataset Builder (Restrictive)</h3>
<p><strong>File</strong>: <code>brain/agents/ai-dataset-builder-restrictive.ts</code></p>
<p><strong>Purpose</strong>: Same as AI Dataset Builder but with stricter quality filtering.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Identical functionality to <code>ai-dataset-builder.ts</code></li>
<li>Different system prompt emphasizing quality over quantity</li>
<li>More restrictive filtering thresholds</li>
</ul>
<p><strong>Trigger</strong>: Manual</p>
<p><strong>Configuration</strong>: <code>etc/ai-dataset-builder-restrictive.json</code> (if exists)</p>
<hr>
<h3>19. User Dataset Builder Agent</h3>
<p><strong>File</strong>: <code>brain/agents/user-dataset-builder.ts</code></p>
<p><strong>Purpose</strong>: Builds training datasets for specific users.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Collects:<ol>
<li>All episodic memories</li>
<li>Therapy session transcripts</li>
<li>Chat conversations</li>
</ol>
</li>
<li>Uses curator model to generate high-quality training samples</li>
<li>Outputs JSONL with instruction/input/output format</li>
</ul>
<p><strong>Trigger</strong>: Manual</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">npx tsx brain/agents/user-dataset-builder.ts --username greggles --output out/adapters/2025-11-14/dataset.jsonl --max 3000
</code></pre>
<hr>
<h3>20. Curated Aggregator Agent</h3>
<p><strong>File</strong>: <code>brain/agents/curated-aggregator.ts</code></p>
<p><strong>Purpose</strong>: Aggregates LLM-curated conversations for training.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Reads from <code>memory/curated/conversations/*.json</code> (output of curator.ts)</li>
<li>Filters conversations marked <code>suitableForTraining: true</code></li>
<li>Converts to <code>CuratedSample[]</code> format</li>
<li>Preserves cognitive mode metadata (dual/emulation/agent)</li>
<li>Outputs for mode-formatter</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (before mode-formatter)</p>
<p><strong>Input</strong>: <code>memory/curated/conversations/*.json</code></p>
<p><strong>Output</strong>: Array of <code>CuratedSample</code> objects</p>
<hr>
<h2>Training Pipeline Agents</h2>
<h3>21. Adapter Builder Agent</h3>
<p><strong>File</strong>: <code>brain/agents/adapter-builder.ts</code></p>
<p><strong>Purpose</strong>: Builds curated instruction‚Üíresponse pairs from user data.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Collects user-specific data:<ol>
<li>Therapy sessions (highest priority)</li>
<li>Episodic memories</li>
<li>Chat conversations</li>
</ol>
</li>
<li>Uses configurable curator model to evaluate, filter, and improve samples</li>
<li>Batch curation with quality threshold filtering</li>
<li>Persona-aware sample generation</li>
<li><strong>MULTI-USER</strong>: Processes specified user with isolated context</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (full-cycle.ts, full-cycle-local.ts)</p>
<p><strong>Configuration</strong>: <code>etc/training-data.json</code> ‚Üí <code>curator</code> section</p>
<p><strong>CLI Usage</strong> (typically called by full-cycle):</p>
<pre><code class="language-bash">pnpm tsx brain/agents/adapter-builder.ts --username greggles --output out/adapters/2025-11-14/dataset.jsonl
</code></pre>
<hr>
<h3>22. Auto-Approver Agent</h3>
<p><strong>File</strong>: <code>brain/agents/auto-approver.ts</code></p>
<p><strong>Purpose</strong>: Automatically approves high-quality datasets based on thresholds.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Validates dataset metadata:<ul>
<li>Minimum pair count (default: 30)</li>
<li>High-confidence percentage (default: 60%)</li>
<li>Reflection percentage (default: 20%)</li>
<li>Maximum low-confidence (default: 20%)</li>
</ul>
</li>
<li>Runs in <strong>dry-run mode by default</strong> for safety</li>
<li>Requires explicit <code>dryRun: false</code> in config to activate</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (after dataset generation)</p>
<p><strong>Configuration</strong>: <code>etc/auto-approval.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">tsx auto-approver.ts 2025-10-21
</code></pre>
<hr>
<h3>23. LoRA Trainer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/lora-trainer.ts</code></p>
<p><strong>Purpose</strong>: Remote orchestrator for RunPod LoRA training.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Manages complete training lifecycle:<ol>
<li>Upload dataset to RunPod via SSH</li>
<li>Upload training config and scripts</li>
<li>Execute training remotely</li>
<li>Monitor training progress</li>
<li>Download adapter (.safetensors format)</li>
<li>Convert to GGUF for Ollama</li>
<li>Upload to S3 (if configured)</li>
<li>Clean up remote resources</li>
</ol>
</li>
<li>SSH connection management with keepalive (8+ hour support)</li>
<li>Progress tracking and audit logging</li>
</ul>
<p><strong>Trigger</strong>: Full-cycle orchestrators (full-cycle.ts)</p>
<p><strong>Configuration</strong>: <code>.env</code> ‚Üí <code>RUNPOD_API_KEY</code>, <code>SSH_KEY_PATH</code>, S3 credentials</p>
<p><strong>Requirements</strong>:</p>
<ul>
<li>RunPod account with API key</li>
<li>SSH key for pod access</li>
<li>S3 bucket (optional, for artifact storage)</li>
</ul>
<hr>
<h3>24. Fine-Tune Trainer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/fine-tune-trainer.ts</code></p>
<p><strong>Purpose</strong>: RunPod wrapper for <strong>full fine-tuning</strong> (not LoRA).</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Reuses <code>lora-trainer.ts</code> infrastructure but adapts for full fine-tuning:<ul>
<li>Uploads <code>fine_tune_dataset.jsonl</code> instead of <code>unsloth_dataset.jsonl</code></li>
<li>Uploads <code>train_full_finetune.py</code> instead of <code>train_unsloth.py</code></li>
<li>Uses <code>fine-tune-config.json</code></li>
<li>Downloads <strong>complete model</strong> instead of adapter</li>
</ul>
</li>
<li>Supports cognitive mode filtering (dual/emulation/agent)</li>
</ul>
<p><strong>Trigger</strong>: Fine-tune-cycle orchestrator (fine-tune-cycle.ts)</p>
<p><strong>Configuration</strong>: <code>etc/fine-tune-config.json</code>, <code>etc/fine-tune-dual.json</code>, <code>etc/fine-tune-emulation.json</code>, <code>etc/fine-tune-agent.json</code></p>
<hr>
<h3>25. Fine-Tune Cycle Orchestrator</h3>
<p><strong>File</strong>: <code>brain/agents/fine-tune-cycle.ts</code></p>
<p><strong>Purpose</strong>: Coordinates full fine-tuning pipeline.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li><strong>Phase 1: Data Preparation</strong><ol>
<li>Curate memories (clean, assign modes)</li>
<li>Format samples (apply mode tags)</li>
<li>Apply schema (model-family wrappers)</li>
<li>Export JSONL (training dataset)</li>
<li>Validate dataset</li>
</ol>
</li>
<li><strong>Phase 2: Remote Training</strong><ol start="6">
<li>Run remote fine-tuning on RunPod</li>
</ol>
</li>
<li><strong>Phase 3: Deployment</strong><ol start="7">
<li>Load fine-tuned model to Ollama</li>
</ol>
</li>
</ul>
<p><strong>Trigger</strong>: Manual</p>
<p><strong>Configuration</strong>: <code>etc/training.json</code>, <code>etc/fine-tune-*.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">tsx brain/agents/fine-tune-cycle.ts --username greggles --base-model qwen3-coder:30b --mode-filter dual --monthly-training
</code></pre>
<p><strong>Flags</strong>:</p>
<ul>
<li><code>--username &lt;name&gt;</code>: User profile</li>
<li><code>--base-model &lt;name&gt;</code>: HuggingFace model identifier</li>
<li><code>--mode-filter &lt;mode&gt;</code>: dual/emulation/agent</li>
<li><code>--max-samples &lt;n&gt;</code>: Limit dataset size</li>
<li><code>--skip-validation</code>: Skip dataset validation</li>
<li><code>--days-recent &lt;n&gt;</code>: Recent sample window (for monthly training)</li>
<li><code>--old-samples &lt;n&gt;</code>: Historical sample count (for monthly training)</li>
<li><code>--monthly-training</code>: Enable monthly training defaults</li>
</ul>
<hr>
<h3>26. Full-Cycle Orchestrator (Remote)</h3>
<p><strong>File</strong>: <code>brain/agents/full-cycle.ts</code></p>
<p><strong>Purpose</strong>: Complete <strong>LoRA adapter</strong> training pipeline on RunPod.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li><strong>Phase 1: Dataset Building</strong><ol>
<li>Build dataset (via adapter-builder.ts)</li>
<li>Prepare config</li>
</ol>
</li>
<li><strong>Phase 2: Remote Training</strong><ol start="3">
<li>Run remote training (via runRemoteTraining from lora-trainer.ts)</li>
</ol>
</li>
<li><strong>Phase 3: Evaluation &amp; Activation</strong><ol start="4">
<li>Evaluate adapter (eval-adapter.ts)</li>
<li>Activate adapter (setActiveAdapter)</li>
<li>Auto-load to Ollama (if successful)</li>
</ol>
</li>
<li><strong>Phase 4: Cleanup</strong><ol start="7">
<li>Clean up stuck processes</li>
<li>Remove temporary files</li>
<li>Write summary on failure</li>
</ol>
</li>
</ul>
<p><strong>Trigger</strong>: Manual or automated training flows</p>
<p><strong>Configuration</strong>: <code>etc/training.json</code>, <code>.env</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">npx tsx brain/agents/full-cycle.ts --username greggles
</code></pre>
<p><strong>Requirements</strong>:</p>
<ul>
<li>RunPod account</li>
<li>SSH key</li>
<li>S3 bucket (optional)</li>
</ul>
<hr>
<h3>27. Full-Cycle Orchestrator (Local)</h3>
<p><strong>File</strong>: <code>brain/agents/full-cycle-local.ts</code></p>
<p><strong>Purpose</strong>: Complete <strong>LoRA adapter</strong> training pipeline on <strong>local machine</strong>.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Same phases as <code>full-cycle.ts</code> but runs training locally</li>
<li>Requires:<ul>
<li>Python 3.10+ with unsloth installed</li>
<li>CUDA-capable GPU (NVIDIA)</li>
<li>At least 24GB VRAM for 20B models</li>
</ul>
</li>
<li>Training execution:<ul>
<li>Spawns <code>train_unsloth.py</code> with local GPU</li>
<li>Monitors progress</li>
<li>Handles errors and cleanup</li>
</ul>
</li>
</ul>
<p><strong>Trigger</strong>: Manual</p>
<p><strong>Configuration</strong>: <code>etc/training.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">npx tsx brain/agents/full-cycle-local.ts --username greggles
</code></pre>
<p><strong>Hardware Requirements</strong>:</p>
<ul>
<li>NVIDIA GPU with 24GB+ VRAM (for 20B models)</li>
<li>CUDA toolkit installed</li>
<li>Python 3.10+ with unsloth</li>
</ul>
<hr>
<h2>Training Support Agents</h2>
<h3>28. Adapter Merger Agent</h3>
<p><strong>File</strong>: <code>brain/agents/adapter-merger.ts</code></p>
<p><strong>Purpose</strong>: Merges multiple LoRA adapters into single consolidated adapter.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Finds all <code>.safetensors</code> adapters in <code>out/adapters/YYYY-MM-DD/</code> directories</li>
<li>Supports merge methods:<ul>
<li><code>linear</code>: Simple weighted average</li>
<li><code>ties</code>: Task-specific merge</li>
<li><code>dare_ties</code>: Dropout-aware merge</li>
<li><code>slerp</code>: Spherical linear interpolation</li>
</ul>
</li>
<li>Requires Python <code>mergekit</code> library</li>
<li>Falls back to simple concatenation if mergekit unavailable</li>
<li>Outputs merged adapter to <code>out/adapters/merged-&lt;timestamp&gt;/</code></li>
</ul>
<p><strong>Trigger</strong>: Manual (typically for dual-adapter system)</p>
<p><strong>Configuration</strong>: Merge config passed as parameter</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh adapter merge --method linear --output history-merged
</code></pre>
<hr>
<h3>29. GGUF Converter Agent</h3>
<p><strong>File</strong>: <code>brain/agents/gguf-converter.ts</code></p>
<p><strong>Purpose</strong>: Converts LoRA adapters from <code>.safetensors</code> to <code>.gguf</code> format for Ollama.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Validates adapter exists (<code>adapter_model.safetensors</code>)</li>
<li>Ensures <code>llama.cpp</code> is available (clones if missing)</li>
<li>Checks Python dependencies (<code>gguf</code> package)</li>
<li>Runs <code>llama.cpp/convert_lora_to_gguf.py</code></li>
<li>Outputs <code>adapter.gguf</code> in same directory</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (after training completes)</p>
<p><strong>Configuration</strong>: None (uses llama.cpp defaults)</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">tsx gguf-converter.ts 2025-10-21
</code></pre>
<p><strong>Requirements</strong>:</p>
<ul>
<li><code>llama.cpp</code> (auto-cloned to <code>vendor/llama.cpp/</code>)</li>
<li>Python 3 with <code>gguf</code> package</li>
</ul>
<hr>
<h3>30. Eval Adapter Agent</h3>
<p><strong>File</strong>: <code>brain/agents/eval-adapter.ts</code></p>
<p><strong>Purpose</strong>: Scores trained LoRA adapters for groundedness, style consistency, and safety.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li><strong>Current Implementation</strong>: Heuristic evaluation<ul>
<li>Checks adapter file exists and is non-zero</li>
<li>Validates dataset quality metrics (high-confidence %, reflection %)</li>
<li>Adapter size heuristic (10-100MB = good)</li>
<li>Calculates composite score (0-1)</li>
</ul>
</li>
<li><strong>Future Implementation</strong>: Real evaluation<ul>
<li>Load validation set (held-out 10% from dataset)</li>
<li>Run inference with adapted model</li>
<li>Score outputs vs expected</li>
<li>Check for hallucinations/drift</li>
</ul>
</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (after adapter conversion)</p>
<p><strong>Configuration</strong>: None</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">tsx eval-adapter.ts 2025-10-21
</code></pre>
<p><strong>Output</strong>: <code>out/adapters/YYYY-MM-DD/eval.json</code></p>
<hr>
<h3>31. Training Exporter Agent</h3>
<p><strong>File</strong>: <code>brain/agents/training-exporter.ts</code></p>
<p><strong>Purpose</strong>: Converts schema-applied samples to JSONL format.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Converts <code>SchemaAppliedSample[]</code> to <code>{&quot;input&quot;: &quot;...&quot;, &quot;output&quot;: &quot;...&quot;}</code> JSONL</li>
<li>Validates:<ul>
<li>Proper JSON escaping</li>
<li>No malformed records</li>
<li>Each record standalone</li>
<li>No mode contamination</li>
</ul>
</li>
<li>Logs warnings for suspicious patterns</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (after schema application)</p>
<p><strong>Input</strong>: <code>SchemaAppliedSample[]</code> array</p>
<p><strong>Output</strong>: JSONL string</p>
<hr>
<h3>32. Mode Formatter Agent</h3>
<p><strong>File</strong>: <code>brain/agents/mode-formatter.ts</code></p>
<p><strong>Purpose</strong>: Applies cognitive mode formatting tags to curated samples.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Transforms <code>CuratedSample</code> ‚Üí <code>FormattedSample</code></li>
<li><strong>Dual Mode</strong>: <code>&lt;thought&gt;: ${user_text}</code> ‚Üí <code>&lt;world&gt;: ${assistant_text}</code></li>
<li><strong>Emulation Mode</strong>: <code>&lt;user&gt;: ${user_text}</code> ‚Üí <code>&lt;assistant&gt;: ${assistant_text}</code></li>
<li><strong>Agent Mode</strong>: <code>&lt;instruction&gt;: ${user_text}</code> ‚Üí <code>&lt;action&gt;: ${assistant_text}</code></li>
<li><strong>CRITICAL</strong>: Only applies formatting tags, does not modify content</li>
</ul>
<p><strong>Trigger</strong>: Training pipeline (after curation, before schema application)</p>
<p><strong>Input</strong>: <code>CuratedSample[]</code> array</p>
<p><strong>Output</strong>: <code>FormattedSample[]</code> array</p>
<hr>
<h2>Persona &amp; Analysis Agents</h2>
<h3>33. Psychoanalyzer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/psychoanalyzer.ts</code></p>
<p><strong>Purpose</strong>: Reviews recent memories and incrementally updates persona files.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Uses psychotherapist model to extract personality insights</li>
<li>Reviews recent episodic memories (configurable window)</li>
<li>Identifies patterns in:<ul>
<li>Communication style</li>
<li>Values and beliefs</li>
<li>Emotional responses</li>
<li>Decision-making patterns</li>
</ul>
</li>
<li>Incrementally updates <code>persona/*.json</code> files</li>
<li><strong>MULTI-USER</strong>: Processes specified user with isolated context</li>
</ul>
<p><strong>Trigger</strong>: Manual or scheduled</p>
<p><strong>Configuration</strong>: <code>etc/psychoanalyzer.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run psychoanalyzer
</code></pre>
<hr>
<h3>34. Digest Agent</h3>
<p><strong>File</strong>: <code>brain/agents/digest.ts</code></p>
<p><strong>Purpose</strong>: Builds long-term thematic understanding from memories.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Analyzes recent memories (default: 14 days)</li>
<li>Extracts:<ul>
<li>Recurring themes (with frequency)</li>
<li>Frequently referenced facts</li>
<li>Catchphrases and quirks</li>
<li>Behavioral patterns</li>
</ul>
</li>
<li>Updates persona cache for quick reference</li>
<li>Builds long-term thematic digest for persona model</li>
</ul>
<p><strong>Trigger</strong>: Manual or scheduled</p>
<p><strong>Configuration</strong>: None (uses defaults)</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run digest
</code></pre>
<p><strong>Part of</strong>: Phase 5 (Conscious/Unconscious State)</p>
<hr>
<h3>35. Summarizer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/summarizer.ts</code></p>
<p><strong>Purpose</strong>: Summarizes conversation sessions into concise overviews.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Analyzes conversations by session ID</li>
<li>Generates summaries of:<ul>
<li>Key topics</li>
<li>Decisions made</li>
<li>Outcomes</li>
<li>Tools used</li>
</ul>
</li>
<li>Stores summaries as episodic events</li>
<li><strong>Mode-aware behavior</strong>:<ul>
<li>Dual/Agent: Summaries saved to episodic memory</li>
<li>Emulation: Uses ephemeral summaries (not saved)</li>
</ul>
</li>
<li>Can be triggered manually or automatically on buffer overflow</li>
<li><strong>MULTI-USER</strong>: Processes specified user with isolated context</li>
</ul>
<p><strong>Trigger</strong>: Manual or automatic (on conversation buffer overflow)</p>
<p><strong>Configuration</strong>: None</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">tsx brain/agents/summarizer.ts --session conv-1699358400-x7k2p9q1
tsx brain/agents/summarizer.ts --auto  # Summarize all unsummarized sessions
</code></pre>
<p><strong>Part of</strong>: Phase 3 (Memory Continuity)</p>
<hr>
<h2>Audio Processing Agents</h2>
<h3>36. Transcriber Agent</h3>
<p><strong>File</strong>: <code>brain/agents/transcriber.ts</code></p>
<p><strong>Purpose</strong>: Monitors <code>memory/audio/inbox</code> for audio files and transcribes them.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Polls every 10 seconds for new audio files</li>
<li>Auto-detects best available provider:<ol>
<li><code>whisper.cpp</code> (local, fast)</li>
<li>OpenAI Whisper API (cloud, high-quality)</li>
<li>Mock (fallback for development)</li>
</ol>
</li>
<li>Supports configuration overrides:<ul>
<li>Provider selection</li>
<li>Model selection (e.g., base.en, small, medium, large)</li>
<li>Language</li>
<li>Temperature</li>
</ul>
</li>
<li>Saves transcripts to <code>memory/audio/transcripts/&lt;audioId&gt;.txt</code></li>
<li>Creates metadata file with transcription details</li>
</ul>
<p><strong>Trigger</strong>: Night pipeline or manual</p>
<p><strong>Configuration</strong>: <code>etc/audio.json</code> ‚Üí <code>transcription</code> section</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run transcriber
</code></pre>
<hr>
<h3>37. Audio Organizer Agent</h3>
<p><strong>File</strong>: <code>brain/agents/audio-organizer.ts</code></p>
<p><strong>(Already documented above in Memory Processing section)</strong></p>
<hr>
<h2>System Services</h2>
<h3>38. Scheduler Service</h3>
<p><strong>File</strong>: <code>brain/agents/scheduler-service.ts</code></p>
<p><strong>Purpose</strong>: Background service that runs the AgentScheduler.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Manages all agent triggers:<ul>
<li><strong>Interval</strong>: Run every N seconds (e.g., organizer every 300s)</li>
<li><strong>Time-of-day</strong>: Run at specific time (e.g., night-pipeline at 02:00)</li>
<li><strong>Event</strong>: Run on specific events (e.g., new memory created)</li>
<li><strong>Activity</strong>: Run after N seconds of inactivity (e.g., boredom-maintenance after 900s)</li>
</ul>
</li>
<li>Single-instance guard (prevents multiple schedulers)</li>
<li>Watches <code>etc/agents.json</code> for configuration changes</li>
<li>Reloads config automatically on file change</li>
<li>Preloads embedding model in background</li>
<li>Graceful shutdown on SIGINT/SIGTERM</li>
</ul>
<p><strong>Trigger</strong>: Runs continuously as background service</p>
<p><strong>Configuration</strong>: <code>etc/agents.json</code></p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash"># Started automatically by web dev server
# Or manually:
./bin/mh agent run scheduler-service
</code></pre>
<p><strong>Lifecycle</strong>:</p>
<ul>
<li><code>scheduler.loadConfig()</code> ‚Äî Load agent configurations</li>
<li><code>scheduler.start()</code> ‚Äî Begin scheduling</li>
<li><code>scheduler.stop()</code> ‚Äî Graceful shutdown</li>
</ul>
<hr>
<h3>39. Operator (ReAct Loop)</h3>
<p><strong>File</strong>: <code>brain/agents/operator-react.ts</code></p>
<p><strong>Purpose</strong>: Modern agentic loop using Reason + Act pattern.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li><strong>Unlike legacy static planner</strong>, this operator:<ul>
<li>Plans <strong>ONE step at a time</strong></li>
<li>Observes the result</li>
<li>Adapts the next step based on what it learned</li>
</ul>
</li>
<li>Prevents hallucinated filenames and other issues caused by planning all steps upfront</li>
<li><strong>Structured Scratchpad</strong> with explicit blocks:<ul>
<li>Thought: LLM reasoning about what to do</li>
<li>Action: Tool invocation with args</li>
<li>Observation: Result of tool execution</li>
</ul>
</li>
<li><strong>Tool Catalog</strong>: Auto-generated skill documentation (cached for 1 minute)</li>
<li><strong>Observation Modes</strong>:<ul>
<li><code>verbatim</code>: Raw JSON output (for &quot;list tasks&quot; queries)</li>
<li><code>structured</code>: Bullet lists with only observed data</li>
<li><code>narrative</code>: Human-readable summaries (V1 style)</li>
</ul>
</li>
<li><strong>Response Styles</strong> (via <code>conversational_response</code> skill):<ul>
<li><code>default</code>: Conversational (temp 0.7)</li>
<li><code>strict</code>: Data-only, no embellishment (temp 0.0)</li>
<li><code>summary</code>: Brief 2-3 sentence overview (temp 0.3)</li>
</ul>
</li>
<li><strong>Error Recovery</strong>:<ul>
<li>Intelligent retry with contextual suggestions</li>
<li>Failure loop detection (prevents repeated failures)</li>
<li>Error codes: FILE_NOT_FOUND, PERMISSION_DENIED, INVALID_ARGS, etc.</li>
</ul>
</li>
<li><strong>Verbatim Short-Circuit</strong>:<ul>
<li>Detects &quot;list tasks&quot; intent</li>
<li>Skips planning loop, returns raw structured data</li>
<li>Saves 2+ LLM calls per query</li>
</ul>
</li>
<li><strong>Feature Flag</strong>: Controlled by <code>etc/runtime.json</code> ‚Üí <code>operator.reactV2</code> (default: false)</li>
</ul>
<p><strong>Trigger</strong>: Web UI chat or API requests</p>
<p><strong>Configuration</strong>: <code>etc/operator.json</code></p>
<p><strong>Key Functions</strong>:</p>
<ul>
<li><code>runOperatorWithFeatureFlag()</code> ‚Äî Main entry point</li>
<li><code>runReActLoopV2()</code> ‚Äî Execute ReAct loop</li>
<li><code>planNextStepV2()</code> ‚Äî LLM planning with tool catalog</li>
</ul>
<p><strong>To Enable V2</strong>: Edit <code>etc/runtime.json</code>, set <code>&quot;operator&quot;: { &quot;reactV2&quot;: true }</code></p>
<hr>
<h3>40. Headless Watcher Service</h3>
<p><strong>File</strong>: <code>brain/agents/headless-watcher.ts</code></p>
<p><strong>Purpose</strong>: Monitors runtime config and manages agent lifecycle based on headless mode.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Watches <code>etc/runtime.json</code> for changes</li>
<li><strong>When headless mode enabled</strong>:<ul>
<li>Stops all local agents (organizer, reflector, curiosity, etc.)</li>
<li>Keeps tunnel and web server running</li>
<li>Provides keepalive mechanism to prevent system sleep</li>
</ul>
</li>
<li><strong>When headless mode disabled</strong>:<ul>
<li>Resumes normal agent operations</li>
<li>Starts default agents (scheduler-service, boredom-service, sleep-service)</li>
</ul>
</li>
<li><strong>MULTI-USER</strong>: System-level service managing global runtime state</li>
<li>Retry logic with exponential backoff (max 5 retries)</li>
</ul>
<p><strong>Trigger</strong>: Runs continuously as background service</p>
<p><strong>Configuration</strong>: <code>etc/runtime.json</code> ‚Üí <code>headless</code> section</p>
<p><strong>CLI Usage</strong>:</p>
<pre><code class="language-bash">./bin/mh agent run headless-watcher
</code></pre>
<hr>
<h3>41. Bootstrap Wrapper</h3>
<p><strong>File</strong>: <code>brain/agents/_bootstrap.ts</code></p>
<p><strong>Purpose</strong>: Establishes user context for agents before they execute.</p>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Agents run as standalone Node processes</li>
<li>Need explicit context to access user-specific paths:<ul>
<li><code>paths.persona</code>, <code>paths.episodic</code>, etc.</li>
</ul>
</li>
<li>Bootstrap wrapper:<ol>
<li>Finds first owner user</li>
<li>Establishes user context via <code>withUserContext()</code></li>
<li>Dynamically imports agent module within context</li>
<li>Calls <code>default()</code> or <code>run()</code> function if exported</li>
</ol>
</li>
<li>Uses <code>systemPaths.brain</code> (system-level path) before context is set</li>
</ul>
<p><strong>Usage</strong>:</p>
<pre><code class="language-bash">tsx brain/agents/_bootstrap.ts &lt;agent-name&gt;
</code></pre>
<p><strong>Example</strong>:</p>
<pre><code class="language-bash">tsx brain/agents/_bootstrap.ts organizer
</code></pre>
<p><strong>Script Overrides</strong>:</p>
<ul>
<li><code>curiosity</code> ‚Üí <code>curiosity-service.ts</code></li>
</ul>
<hr>
<h2>Agent Scheduler Configuration</h2>
<p>All agents are configured via <strong><code>etc/agents.json</code></strong>. The scheduler service reads this file and manages agent lifecycle.</p>
<h3>Configuration Structure</h3>
<pre><code class="language-json">{
  &quot;agents&quot;: {
    &quot;organizer&quot;: {
      &quot;enabled&quot;: true,
      &quot;trigger&quot;: {
        &quot;type&quot;: &quot;activity&quot;,
        &quot;inactivityThreshold&quot;: 300
      },
      &quot;description&quot;: &quot;Enrich memories with tags and entities&quot;
    },
    &quot;reflector&quot;: {
      &quot;enabled&quot;: false,
      &quot;trigger&quot;: {
        &quot;type&quot;: &quot;interval&quot;,
        &quot;interval&quot;: 3600
      },
      &quot;description&quot;: &quot;Generate reflections (disabled in favor of boredom-maintenance)&quot;
    },
    &quot;boredom-maintenance&quot;: {
      &quot;enabled&quot;: true,
      &quot;trigger&quot;: {
        &quot;type&quot;: &quot;activity&quot;,
        &quot;inactivityThreshold&quot;: 900
      },
      &quot;description&quot;: &quot;Activity-based reflection triggering&quot;
    },
    &quot;curiosity&quot;: {
      &quot;enabled&quot;: true,
      &quot;trigger&quot;: {
        &quot;type&quot;: &quot;activity&quot;,
        &quot;inactivityThreshold&quot;: 900
      },
      &quot;description&quot;: &quot;User-facing curiosity questions&quot;
    },
    &quot;curator&quot;: {
      &quot;enabled&quot;: true,
      &quot;trigger&quot;: {
        &quot;type&quot;: &quot;interval&quot;,
        &quot;interval&quot;: 1800
      },
      &quot;description&quot;: &quot;Prepare training data&quot;
    },
    &quot;night-pipeline&quot;: {
      &quot;enabled&quot;: true,
      &quot;trigger&quot;: {
        &quot;type&quot;: &quot;time_of_day&quot;,
        &quot;time&quot;: &quot;02:00&quot;
      },
      &quot;description&quot;: &quot;Nightly processing pipeline&quot;
    }
  }
}
</code></pre>
<h3>Trigger Types</h3>
<ol>
<li><p><strong>Interval</strong>: Run every N seconds</p>
<pre><code class="language-json">{ &quot;type&quot;: &quot;interval&quot;, &quot;interval&quot;: 3600 }
</code></pre>
</li>
<li><p><strong>Time-of-day</strong>: Run at specific time (24-hour format)</p>
<pre><code class="language-json">{ &quot;type&quot;: &quot;time_of_day&quot;, &quot;time&quot;: &quot;02:00&quot; }
</code></pre>
</li>
<li><p><strong>Activity</strong>: Run after N seconds of conversation inactivity</p>
<pre><code class="language-json">{ &quot;type&quot;: &quot;activity&quot;, &quot;inactivityThreshold&quot;: 900 }
</code></pre>
</li>
<li><p><strong>Event</strong>: Run on specific events (planned)</p>
<pre><code class="language-json">{ &quot;type&quot;: &quot;event&quot;, &quot;event&quot;: &quot;memory_created&quot; }
</code></pre>
</li>
</ol>
<hr>
<h2>Common CLI Commands</h2>
<h3>Running Agents Manually</h3>
<pre><code class="language-bash"># Run specific agent
./bin/mh agent run &lt;agent-name&gt;

# Examples
./bin/mh agent run organizer
./bin/mh agent run reflector
./bin/mh agent run curiosity-service
./bin/mh agent run dreamer
./bin/mh agent run curator
./bin/mh agent run psychoanalyzer

# List available agents
./bin/mh agent list

# Show agent statistics
./bin/mh agent status

# Monitor agent processing
./bin/mh agent monitor

# List running agent processes
./bin/mh agent ps

# Stop running agent
./bin/mh agent stop &lt;name&gt;
</code></pre>
<h3>Training Pipeline</h3>
<pre><code class="language-bash"># Full-cycle LoRA training (remote)
npx tsx brain/agents/full-cycle.ts --username greggles

# Full-cycle LoRA training (local)
npx tsx brain/agents/full-cycle-local.ts --username greggles

# Fine-tuning cycle (full model)
tsx brain/agents/fine-tune-cycle.ts --username greggles --base-model qwen3-coder:30b --mode-filter dual

# Build dataset only
pnpm tsx brain/agents/adapter-builder.ts --username greggles --output out/adapters/2025-11-14/dataset.jsonl

# Evaluate adapter
tsx eval-adapter.ts 2025-10-21

# Convert to GGUF
tsx gguf-converter.ts 2025-10-21
</code></pre>
<hr>
<h2>Multi-User Support</h2>
<p>Many agents are <strong>MULTI-USER capable</strong>, processing all logged-in users sequentially with isolated contexts:</p>
<ul>
<li>‚úÖ <strong>Organizer</strong> ‚Äî Processes all users&#39; unprocessed memories</li>
<li>‚úÖ <strong>Curiosity Service</strong> ‚Äî Asks questions to all logged-in users</li>
<li>‚úÖ <strong>Inner Curiosity</strong> ‚Äî Generates internal questions for all users</li>
<li>‚úÖ <strong>Curiosity Researcher</strong> ‚Äî Researches questions for all users</li>
<li>‚úÖ <strong>Dreamer</strong> ‚Äî Creates dreams for all users during sleep cycle</li>
<li>‚úÖ <strong>Memory Metrics Cache</strong> ‚Äî Updates metrics for all users</li>
<li>‚úÖ <strong>Curator</strong> ‚Äî Prepares training data for all users</li>
<li>‚úÖ <strong>Psychoanalyzer</strong> ‚Äî Analyzes memories for specified user</li>
<li>‚úÖ <strong>Summarizer</strong> ‚Äî Summarizes conversations for specified user</li>
</ul>
<p><strong>System-level agents</strong> (no user context):</p>
<ul>
<li>‚öôÔ∏è <strong>Scheduler Service</strong> ‚Äî Manages all agent triggers</li>
<li>‚öôÔ∏è <strong>Headless Watcher</strong> ‚Äî Manages runtime mode</li>
<li>‚öôÔ∏è <strong>Sleep Service</strong> ‚Äî Orchestrates nightly pipeline</li>
<li>‚öôÔ∏è <strong>Operator (ReAct)</strong> ‚Äî Executes tasks (user context via API request)</li>
</ul>
<hr>
<h2>Agent Dependencies</h2>
<h3>Python Dependencies</h3>
<p>Some agents require Python packages:</p>
<ul>
<li><strong>unsloth</strong> ‚Äî LoRA training (full-cycle, full-cycle-local, lora-trainer)</li>
<li><strong>gguf</strong> ‚Äî GGUF conversion (gguf-converter)</li>
<li><strong>mergekit</strong> ‚Äî Adapter merging (adapter-merger)</li>
<li><strong>whisper</strong> ‚Äî Audio transcription (transcriber, if using whisper.cpp)</li>
</ul>
<h3>External Services</h3>
<p>Some agents require external services:</p>
<ul>
<li><strong>Ollama</strong> ‚Äî LLM inference (organizer, curator, reflector, curiosity, inner-curiosity, psychoanalyzer, ai-dataset-builder)</li>
<li><strong>RunPod</strong> ‚Äî Remote training (lora-trainer, fine-tune-trainer)</li>
<li><strong>OpenAI Whisper API</strong> ‚Äî Audio transcription (transcriber, optional)</li>
<li><strong>S3</strong> ‚Äî Artifact storage (lora-trainer, optional)</li>
</ul>
<h3>System Requirements</h3>
<p><strong>For Local Training</strong> (full-cycle-local):</p>
<ul>
<li>NVIDIA GPU with 24GB+ VRAM</li>
<li>CUDA toolkit</li>
<li>Python 3.10+</li>
<li>unsloth library</li>
</ul>
<p><strong>For Remote Training</strong> (full-cycle, lora-trainer):</p>
<ul>
<li>RunPod account with API key</li>
<li>SSH key for pod access</li>
<li>S3 bucket (optional)</li>
</ul>
<hr>
<h2>Troubleshooting</h2>
<h3>Agent Won&#39;t Start</h3>
<p><strong>Issue</strong>: <code>Another instance is already running</code></p>
<p><strong>Solution</strong>: Check if lock file&#39;s PID is actually alive:</p>
<pre><code class="language-bash">ps -p &lt;pid&gt;  # Check process
rm logs/run/locks/&lt;agent-name&gt;.lock  # Remove stale lock
</code></pre>
<h3>Organizer Not Processing Memories</h3>
<p><strong>Issue</strong>: Memories remain unprocessed</p>
<p><strong>Checklist</strong>:</p>
<ol>
<li>Is Ollama running? <code>./bin/mh ollama status</code></li>
<li>Is organizer enabled in <code>etc/agents.json</code>?</li>
<li>Check logs: <code>tail -f logs/audit/$(date +%Y-%m-%d).ndjson | grep organizer</code></li>
<li>Run manually: <code>./bin/mh agent run organizer</code></li>
</ol>
<h3>Reflector Not Generating Reflections</h3>
<p><strong>Issue</strong>: No inner dialogue events</p>
<p><strong>Solution</strong>: Reflector is disabled in favor of boredom-maintenance. Check <code>etc/agents.json</code>:</p>
<pre><code class="language-json">{
  &quot;boredom-maintenance&quot;: {
    &quot;enabled&quot;: true,
    &quot;trigger&quot;: { &quot;type&quot;: &quot;activity&quot;, &quot;inactivityThreshold&quot;: 900 }
  }
}
</code></pre>
<h3>Training Pipeline Fails</h3>
<p><strong>Issue</strong>: RunPod training errors</p>
<p><strong>Checklist</strong>:</p>
<ol>
<li>Verify <code>.env</code> credentials: <code>RUNPOD_API_KEY</code>, <code>SSH_KEY_PATH</code></li>
<li>Check RunPod quota and credit balance</li>
<li>Review training logs in <code>metahuman-runs/&lt;username&gt;/&lt;date&gt;/&lt;run-label&gt;/</code></li>
<li>Verify GPU availability: Run <code>nvidia-smi</code> on pod</li>
<li>Check dataset quality: Review <code>metadata.json</code></li>
</ol>
<h3>Curiosity Questions Not Appearing</h3>
<p><strong>Issue</strong>: No curiosity questions in chat</p>
<p><strong>Checklist</strong>:</p>
<ol>
<li>Is curiosity enabled in <code>etc/agents.json</code>?</li>
<li>Check trust level in <code>persona/core.json</code> (requires <code>suggest</code> or higher)</li>
<li>Check <code>maxOpenQuestions</code> in <code>etc/curiosity.json</code> (must be &gt; 0)</li>
<li>Check inactivity threshold (default: 900s = 15 min)</li>
<li>Run manually: <code>./bin/mh agent run curiosity-service</code></li>
</ol>
<hr>
<h2>Related Documentation</h2>
<ul>
<li><strong><a href="06-cli-reference.md">CLI Reference</a></strong> ‚Äî Agent management commands</li>
<li><strong><a href="04-core-concepts.md">Core Concepts</a></strong> ‚Äî System architecture overview</li>
<li><strong><a href="07-memory-system.md">Memory System</a></strong> ‚Äî Episodic memory structure</li>
<li><strong><a href="core-features/04b-cognitive-modes.md">Cognitive Modes</a></strong> ‚Äî Dual/Agent/Emulation modes</li>
<li><strong><a href="11-special-features.md#lora-adapter-training">LoRA Adapter Training</a></strong> ‚Äî Training pipeline details</li>
<li><strong><a href="13-advanced-usage.md#full-fine-tuning-with-monthly-updates">Fine-Tuning &amp; Monthly Updates</a></strong> ‚Äî Cognitive mode training</li>
<li><strong><a href="14-configuration-files.md">Configuration Files</a></strong> ‚Äî etc/agents.json, etc/curiosity.json, etc/training.json</li>
</ul>
<hr>
<p><strong>MetaHuman OS agents work 24/7 to extend your mind!</strong> üß†‚ö°</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="authentication" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Authentication</span> </span> </button> <button class="nav-button next-button" data-target="chat-interface" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Chat Interface</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="chat-interface" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Chat Interface</h1>
<p>The chat interface is your primary way to interact with MetaHuman OS. It provides a modern, three-column ChatGPT-style layout with real-time updates and comprehensive conversation management.</p>
<h2>Accessing the Chat</h2>
<p>Start the web interface:</p>
<pre><code class="language-bash">cd apps/site &amp;&amp; pnpm dev
# Open http://localhost:4321
</code></pre>
<h2>Layout Overview</h2>
<p>The interface uses a three-column layout:</p>
<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Header (Authentication, Cognitive Mode, Settings)       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Left    ‚îÇ Center                      ‚îÇ Right           ‚îÇ
‚îÇ Sidebar ‚îÇ Content                     ‚îÇ Sidebar         ‚îÇ
‚îÇ         ‚îÇ                             ‚îÇ (Collapsible)   ‚îÇ
‚îÇ Feature ‚îÇ ChatInterface /             ‚îÇ Developer       ‚îÇ
‚îÇ Menu    ‚îÇ Dashboard /                 ‚îÇ Tools           ‚îÇ
‚îÇ         ‚îÇ Memory Browser /            ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ TaskManager /               ‚îÇ                 ‚îÇ
‚îÇ         ‚îÇ Terminal / etc.             ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<p><strong>Key Components:</strong></p>
<ul>
<li><strong>Header</strong>: Authentication, cognitive mode selector, developer tools toggle</li>
<li><strong>Left Sidebar</strong>: Feature menu with status widget</li>
<li><strong>Center Content</strong>: Active view (chat, dashboard, memory, etc.)</li>
<li><strong>Right Sidebar</strong>: Developer tools (audit stream, agent monitor, settings)</li>
</ul>
<h2>Authentication &amp; Profile Selection</h2>
<p>When you first access the UI, you&#39;ll encounter the <strong>Authentication Gate</strong>:</p>
<ol>
<li><strong>Create Account</strong> ‚Äì Register a new user. The first account becomes the <code>owner</code> with full access.</li>
<li><strong>Login</strong> ‚Äì Existing users authenticate to access their isolated profile.</li>
<li><strong>Continue as Guest</strong> ‚Äì Start a 30-minute anonymous session. Guests choose from public profiles only (private profiles are hidden).</li>
</ol>
<p>The active profile displays in the header once authenticated.</p>
<h2>Header Controls</h2>
<h3>Profile Indicator</h3>
<p>Shows the active account with role and visibility badge. Owners can:</p>
<ul>
<li>Open the profile menu</li>
<li>Log out</li>
<li>Access Settings</li>
</ul>
<h3>Cognitive Mode Selector</h3>
<p>Switch between three operational modes:</p>
<ul>
<li><strong>Dual Consciousness</strong> (Default): Full operator pipeline with memory grounding</li>
<li><strong>Agent</strong>: Heuristic routing for lightweight assistance</li>
<li><strong>Emulation</strong>: Chat-only mode without operator overhead</li>
</ul>
<p>Locked modes display tooltips explaining access restrictions.</p>
<h3>Developer Tools Toggle</h3>
<p>Opens the right sidebar with:</p>
<ul>
<li>Live audit stream (real-time system events)</li>
<li>Agent monitor with statistics</li>
<li>Boredom control (reflection frequency)</li>
<li>Model selector (switch Ollama models)</li>
</ul>
<h2>System Status Banners</h2>
<p>Prominent banners appear at the top when special system states are active:</p>
<ul>
<li><strong>High Security Mode</strong>: Red banner - system locked into read-only emulation mode</li>
<li><strong>Wetware Deceased</strong>: Indigo banner - running as independent digital consciousness (Dual mode unavailable)</li>
<li><strong>Read-Only Mode</strong>: General banner - current cognitive mode doesn&#39;t allow writes</li>
</ul>
<h2>Chat Modes</h2>
<p>The chat interface has three distinct modes accessible via toggle buttons above the message area:</p>
<h3>Conversation Mode (Default)</h3>
<p><strong>Purpose</strong>: Active back-and-forth dialog between you and MetaHuman</p>
<p><strong>What appears</strong>:</p>
<ul>
<li>‚úÖ Your messages (user)</li>
<li>‚úÖ MetaHuman&#39;s responses (assistant)</li>
<li>‚úÖ System notifications</li>
<li>‚úÖ Live reasoning stages (while thinking)</li>
</ul>
<p><strong>What&#39;s hidden</strong>:</p>
<ul>
<li>‚ùå Reflections (üí≠ Idle Thought)</li>
<li>‚ùå Dreams (üåô Dream)</li>
<li>‚ùå Completed reasoning (ephemeral - disappears after thinking finishes)</li>
</ul>
<p><strong>Reasoning behavior</strong>: Reasoning stages display in real-time during thinking for transparency, but disappear once the response completes to keep conversations clean.</p>
<h3>Inner Dialogue Mode</h3>
<p><strong>Purpose</strong>: Observe MetaHuman&#39;s autonomous consciousness stream</p>
<p><strong>What appears</strong>:</p>
<ul>
<li>‚úÖ Reflections (üí≠ Idle Thought) - Generated by reflector agent using associative memory chains</li>
<li>‚úÖ Dreams (üåô Dream) - Generated by dreamer agent during sleep from lifetime memories</li>
<li>‚úÖ Reasoning stages (ü§î Reasoning) - Permanent record of thinking processes</li>
</ul>
<p><strong>What&#39;s hidden</strong>:</p>
<ul>
<li>‚ùå User messages</li>
<li>‚ùå Assistant responses</li>
<li>‚ùå System messages</li>
</ul>
<p><strong>Memory Access</strong>: Both reflections and dreams access your entire memory lifetime, weighted by recency:</p>
<ul>
<li>Recent memories appear most frequently</li>
<li>1-year-old memories retain ~20% probability (reflective weighting)</li>
<li>Older memories surface meaningfully, not just as rare exceptions</li>
<li>Exponential decay formula (227-day constant) allows contemplative exploration of your past</li>
</ul>
<p><strong>Train of Thought</strong>: Reflections use associative chain-building to follow semantic links between memories, creating connected thought sequences.</p>
<h3>Voice Mode</h3>
<p><strong>Purpose</strong>: Voice-driven interaction with audio input/output</p>
<ul>
<li>Real-time speech-to-text and text-to-speech</li>
<li>See <a href="voice-features.md">Voice Features</a> for complete details</li>
</ul>
<h2>Dialog Type Separation: Thoughts vs. Words</h2>
<p>The strict separation ensures clarity:</p>
<ul>
<li><strong>Conversation</strong> = Spoken words (bidirectional communication)</li>
<li><strong>Inner Dialogue</strong> = Silent thoughts (autonomous consciousness)</li>
<li><strong>Reasoning</strong> = Ephemeral in conversation (live feedback only), permanent in inner dialogue (thought record)</li>
</ul>
<p>This architecture allows you to:</p>
<ul>
<li>Have clean conversations without thought clutter</li>
<li>Observe MetaHuman&#39;s autonomous mental processes separately</li>
<li>See reasoning live during conversation without polluting history</li>
<li>Review complete thinking processes in inner dialogue mode</li>
</ul>
<h2>Left Sidebar - Status Widget</h2>
<h3>Trust Level</h3>
<p>Click to cycle through trust progression:</p>
<ul>
<li><strong>observe</strong> ‚Üí <strong>suggest</strong> ‚Üí <strong>supervised_auto</strong> ‚Üí <strong>bounded_auto</strong> ‚Üí <strong>adaptive_auto</strong> ‚Üí <strong>YOLO</strong></li>
</ul>
<p>Each level grants increasing autonomy for skill execution and system operations.</p>
<h3>Persona Facets</h3>
<p>Click to cycle through personality facets:</p>
<ul>
<li><strong>inactive</strong> (Gray) - Persona disabled</li>
<li><strong>default</strong> (Purple) - Balanced, authentic self</li>
<li><strong>poet</strong> (Indigo) - Creative, metaphorical, expressive</li>
<li><strong>thinker</strong> (Blue) - Analytical, systematic</li>
<li><strong>friend</strong> (Green) - Warm, supportive, empathetic</li>
<li><strong>antagonist</strong> (Red) - Critical, challenging</li>
</ul>
<p>Features:</p>
<ul>
<li>Each facet shows as a colored badge</li>
<li>Messages are color-coded with left borders matching the active facet</li>
<li>Facet name appears in message header (e.g., &quot;MetaHuman ¬∑ poet&quot;)</li>
<li>Chat history persists across changes for multi-faceted conversations</li>
</ul>
<h3>Profile Visibility</h3>
<p>Owners can mark their persona as <strong>Private</strong> or <strong>Public</strong>:</p>
<ul>
<li><strong>Public Profiles</strong>: Visible to all guests; can be selected for guest sessions</li>
<li><strong>Private Profiles</strong>: Hidden from guest selection; owner-only access</li>
<li><strong>Special Profile</strong>: When 2+ public profiles exist, the Mutant Super Intelligence easter egg appears</li>
</ul>
<h2>Privacy Features &amp; Session Controls</h2>
<h3>Clear Button</h3>
<p>Located in the chat interface header, provides complete session cleanup:</p>
<ul>
<li>Clears all chat messages from the UI</li>
<li>Clears reasoning stages</li>
<li>Clears localStorage cache</li>
<li>Clears the live audit stream display</li>
<li><strong>Deletes all audit log files from disk</strong> (<code>logs/audit/*.ndjson</code>)</li>
<li>Creates a new audit entry recording the clear action for accountability</li>
</ul>
<h3>Fresh Session Interface</h3>
<p>Each session starts with a clean slate - no historical chat or audit data loads automatically.</p>
<h3>Audit Logs</h3>
<p>All system events are saved to <code>logs/audit/YYYY-MM-DD.ndjson</code> for accountability, but can be cleared at any time via the Clear button. The live sidebar stream mirrors the same data, organized into expandable groups.</p>
<h2>Three Ways to Interact</h2>
<ol>
<li><strong>Web UI (Recommended)</strong> - Interactive interface with real-time updates</li>
<li><strong>CLI (<code>mh</code> command)</strong> - Command-line interface for quick operations</li>
<li><strong>Direct File Access</strong> - All data stored as human-readable JSON files for direct manipulation</li>
</ol>
<h2>Next Steps</h2>
<ul>
<li>Learn about the <a href="memory-system.md">Memory System</a> for storing and organizing your data</li>
<li>Explore <a href="task-management.md">Task Management</a> to track your goals and projects</li>
<li>Try <a href="voice-features.md">Voice Features</a> for audio-driven interaction</li>
<li>Check the <a href="dashboard-monitoring.md">Dashboard</a> for system status and metrics</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="autonomous-agents" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Autonomous Agents</span> </span> </button> <button class="nav-button next-button" data-target="cli-reference" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Cli Reference</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="cli-reference" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>CLI Command Reference</h1>
<p>Complete reference for all <code>mh</code> CLI commands. All commands follow the pattern: <code>./bin/mh &lt;command&gt; [args...]</code></p>
<h2>System Commands</h2>
<h3><code>./bin/mh init</code></h3>
<p>Initialize MetaHuman OS directory structure and copy template configuration files.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh init
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Creates all required directories (memory, persona, logs, etc.)</li>
<li>Copies <code>.template</code> files to working configs</li>
<li>Sets up empty data structures</li>
</ul>
<p><strong>First-time setup:</strong></p>
<ol>
<li>Run <code>mh init</code></li>
<li>Edit <code>persona/core.json</code> with your details</li>
<li>Update <code>persona/routines.json</code> with your schedule</li>
<li>Review <code>etc/*.json</code> for runtime settings</li>
</ol>
<hr>
<h3><code>./bin/mh status</code></h3>
<p>Show system status overview including identity, active tasks, and memory counts.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh status
</code></pre>
<p><strong>Output:</strong></p>
<ul>
<li>Identity summary (name, trust level)</li>
<li>Active task count</li>
<li>Recent event counts</li>
</ul>
<hr>
<h3><code>./bin/mh start [--restart|-r] [--force|-f]</code></h3>
<p>Start background services (scheduler, audio-organizer, headless-watcher).</p>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--restart</code> or <code>-r</code>: Restart services if already running (default: true)</li>
<li><code>--force</code> or <code>-f</code>: Force kill stuck processes</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># Start with restart (default)
./bin/mh start

# Start without restarting existing services
./bin/mh start --no-restart

# Force restart stuck services
./bin/mh start --force
</code></pre>
<p><strong>Services started:</strong></p>
<ul>
<li><code>headless-watcher</code> (always)</li>
<li><code>scheduler-service</code> (manages other agents via etc/agents.json)</li>
<li><code>audio-organizer</code> (audio processing)</li>
</ul>
<p>Note: In headless mode, only <code>headless-watcher</code> starts automatically.</p>
<hr>
<h3><code>./bin/mh help</code></h3>
<p>Display help message with available commands.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh help
./bin/mh --help
./bin/mh -h
</code></pre>
<hr>
<h3><code>./bin/mh guide</code></h3>
<p>Show path to user guide documentation.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh guide
</code></pre>
<hr>
<h3><code>./bin/mh sync</code></h3>
<p>Sync state and show current configuration (identity, trust level, last updated).</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sync
</code></pre>
<hr>
<h2>Memory Commands</h2>
<h3><code>./bin/mh capture &quot;text&quot;</code></h3>
<p>Capture an observation to episodic memory.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh capture &quot;Met with Sarah to discuss the ML project timeline&quot;
./bin/mh capture &quot;Interesting insight: async code patterns improve readability&quot;
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Creates JSON file in <code>memory/episodic/YYYY/YYYY-MM-DD-&lt;uuid&gt;.json</code></li>
<li>Adds timestamp and metadata</li>
<li>Logs to audit trail</li>
<li>Organizer agent will process it later to extract tags/entities</li>
</ul>
<hr>
<h3><code>./bin/mh remember &lt;query&gt;</code></h3>
<p>Search memory using semantic or keyword search.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh remember &quot;Sarah project discussion&quot;
./bin/mh remember &quot;machine learning&quot;
</code></pre>
<p><strong>Search modes:</strong></p>
<ol>
<li><strong>Semantic search</strong> (if index exists): Uses vector embeddings for meaning-based search</li>
<li><strong>Keyword search</strong> (fallback): Simple text matching</li>
</ol>
<p><strong>Build index for semantic search:</strong></p>
<pre><code class="language-bash">./bin/mh ollama pull nomic-embed-text
./bin/mh index build
</code></pre>
<hr>
<h3><code>./bin/mh find &lt;description&gt;</code></h3>
<p>AI-powered file search using natural language.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh find &quot;TypeScript files related to voice training&quot;
./bin/mh find &quot;configuration files for agents&quot;
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Uses LLM to interpret your query</li>
<li>Searches filesystem with intelligent pattern matching</li>
<li>Returns relevant file paths</li>
</ul>
<hr>
<h2>Task Commands</h2>
<h3><code>./bin/mh task</code></h3>
<p>List all active tasks.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh task
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Active Tasks:

[high] Finish ML model training
    Status: in_progress | ID: task-2024-11-25-abc123
    Due: 2024-11-30

[medium] Update documentation
    Status: todo | ID: task-2024-11-24-def456
</code></pre>
<hr>
<h3><code>./bin/mh task add &quot;title&quot;</code></h3>
<p>Create a new task.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh task add &quot;Write blog post about vector embeddings&quot;
./bin/mh task add &quot;Review PR #123&quot;
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>‚úì Created: memory/tasks/active/task-2024-11-25-abc123.json
</code></pre>
<hr>
<h3><code>./bin/mh task start &lt;id&gt;</code></h3>
<p>Mark task as in progress.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh task start task-2024-11-25-abc123
</code></pre>
<hr>
<h3><code>./bin/mh task done &lt;id&gt;</code></h3>
<p>Mark task as completed.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh task done task-2024-11-25-abc123
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Moves task from <code>active/</code> to <code>completed/</code></li>
<li>Updates timestamp</li>
<li>Logs completion to audit trail</li>
</ul>
<hr>
<h2>Agent Commands</h2>
<h3><code>./bin/mh agent list</code></h3>
<p>List all available agents.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh agent list
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Available Agents (40):

  organizer
  reflector
  dreamer
  curiosity-service
  ...
</code></pre>
<hr>
<h3><code>./bin/mh agent run &lt;name&gt;</code></h3>
<p>Execute an agent manually.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh agent run organizer
./bin/mh agent run reflector
./bin/mh agent run dreamer
</code></pre>
<p><strong>Common agents:</strong></p>
<ul>
<li><code>organizer</code> - Process memories, extract tags/entities</li>
<li><code>reflector</code> - Generate reflections from memory chains</li>
<li><code>dreamer</code> - Create dream from memory fragments</li>
<li><code>curiosity-service</code> - Ask user-facing questions</li>
<li><code>inner-curiosity</code> - Generate internal questions/answers</li>
</ul>
<hr>
<h3><code>./bin/mh agent status [name]</code></h3>
<p>Show agent execution statistics.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># All agents
./bin/mh agent status

# Specific agent
./bin/mh agent status organizer
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Agent: organizer

Total runs: 145
Successful: 142
Failed: 3
Last run: 2024-11-25 10:30:15
</code></pre>
<hr>
<h3><code>./bin/mh agent logs [name]</code></h3>
<p>View recent agent logs (last 20 entries).</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># All logs
./bin/mh agent logs

# Specific agent
./bin/mh agent logs organizer
</code></pre>
<hr>
<h3><code>./bin/mh agent monitor</code></h3>
<p>Show memory processing status for all agents.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh agent monitor
</code></pre>
<p><strong>Output:</strong></p>
<ul>
<li>Unprocessed memory counts</li>
<li>Processing queue status</li>
<li>Agent activity</li>
</ul>
<hr>
<h3><code>./bin/mh agent ps</code></h3>
<p>List running agent processes with PIDs and uptimes.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh agent ps
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Running agents:

  scheduler-service    pid=12345  uptime=2h 15m     started 2024-11-25T08:15:00Z
  organizer           pid=12346  uptime=1h 30m     started 2024-11-25T09:00:00Z
</code></pre>
<hr>
<h3><code>./bin/mh agent stop &lt;name&gt;</code></h3>
<p>Stop a running agent.</p>
<p><strong>Flags:</strong></p>
<ul>
<li><code>--force</code>: Force kill if graceful shutdown fails</li>
<li><code>--all</code>: Stop all running agents</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># Stop specific agent
./bin/mh agent stop organizer

# Force stop
./bin/mh agent stop organizer --force

# Stop all agents
./bin/mh agent stop --all
</code></pre>
<hr>
<h2>Ollama Commands</h2>
<h3><code>./bin/mh ollama status</code></h3>
<p>Check if Ollama is running.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama status
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>‚úì Ollama is running (v0.3.12)
  Endpoint: http://localhost:11434
</code></pre>
<hr>
<h3><code>./bin/mh ollama list</code></h3>
<p>List installed models with sizes and details.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama list
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Installed Models (3):

  phi3:mini
    Size: 2.30 GB
    Modified: 2024-11-20 10:15:30
    Family: phi3
    Parameters: 3.8B
</code></pre>
<hr>
<h3><code>./bin/mh ollama pull &lt;model&gt;</code></h3>
<p>Install a model from Ollama library.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama pull phi3:mini
./bin/mh ollama pull nomic-embed-text
./bin/mh ollama pull qwen2.5-coder:7b
</code></pre>
<p><strong>Progress display:</strong>
Shows download progress with status updates.</p>
<hr>
<h3><code>./bin/mh ollama delete &lt;model&gt;</code></h3>
<p>Remove an installed model.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama delete phi3:mini
</code></pre>
<hr>
<h3><code>./bin/mh ollama info &lt;model&gt;</code></h3>
<p>Show detailed model information.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama info phi3:mini
</code></pre>
<p><strong>Output:</strong></p>
<ul>
<li>Model family and architecture</li>
<li>Parameter count</li>
<li>Quantization details</li>
<li>System prompt template</li>
</ul>
<hr>
<h3><code>./bin/mh ollama chat &lt;model&gt;</code></h3>
<p>Interactive chat session with a model.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama chat phi3:mini
</code></pre>
<p><strong>Commands during chat:</strong></p>
<ul>
<li>Type messages normally</li>
<li><code>exit</code> or Ctrl+C to quit</li>
</ul>
<hr>
<h3><code>./bin/mh ollama ask &lt;model&gt; &quot;question&quot;</code></h3>
<p>One-shot question to a model.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama ask phi3:mini &quot;What is the capital of France?&quot;
./bin/mh ollama ask qwen2.5-coder:7b &quot;Explain async/await in JavaScript&quot;
</code></pre>
<hr>
<h3><code>./bin/mh ollama doctor</code></h3>
<p>Diagnose Ollama setup and connectivity issues.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ollama doctor
</code></pre>
<p><strong>Checks:</strong></p>
<ul>
<li>Ollama service status</li>
<li>Endpoint connectivity</li>
<li>Model availability</li>
<li>Common configuration issues</li>
</ul>
<hr>
<h2>Semantic Index Commands</h2>
<h3><code>./bin/mh index build</code></h3>
<p>Build vector embeddings index for semantic search.</p>
<p><strong>Prerequisites:</strong></p>
<pre><code class="language-bash">./bin/mh ollama pull nomic-embed-text
</code></pre>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh index build
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Scans all episodic memories</li>
<li>Generates vector embeddings using nomic-embed-text</li>
<li>Saves index to <code>memory/index/</code></li>
<li>Takes a few minutes for large memory collections</li>
</ul>
<hr>
<h3><code>./bin/mh index query &quot;text&quot;</code></h3>
<p>Perform semantic search on indexed memories.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh index query &quot;conversations about machine learning&quot;
./bin/mh index query &quot;Sarah mentioned project deadlines&quot;
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Top matches (semantic, model=nomic-embed-text):

  85.3%  memory/episodic/2024/2024-11-20-abc123.json
      Discussed ML project timeline with Sarah...

  72.1%  memory/episodic/2024/2024-11-15-def456.json
      Meeting notes: project deadline moved to Dec 1...
</code></pre>
<hr>
<h2>Trust &amp; Identity Commands</h2>
<h3><code>./bin/mh trust</code></h3>
<p>Show current trust level and available modes.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh trust
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Current trust level: observe

Available modes:
  observe (current)
    System monitors but takes no action
  suggest
    System proposes actions for approval
  supervised_auto
    Execute pre-approved action categories
  bounded_auto
    Full autonomy within defined boundaries
</code></pre>
<hr>
<h3><code>./bin/mh trust &lt;level&gt;</code></h3>
<p>Set trust level.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh trust observe
./bin/mh trust suggest
./bin/mh trust supervised_auto
./bin/mh trust bounded_auto
</code></pre>
<p><strong>Trust levels:</strong></p>
<ol>
<li><strong>observe</strong> - Monitor only, no actions</li>
<li><strong>suggest</strong> - Propose actions, require approval</li>
<li><strong>supervised_auto</strong> - Execute approved categories</li>
<li><strong>bounded_auto</strong> - Full autonomy in boundaries</li>
</ol>
<hr>
<h2>File Ingestion Commands</h2>
<h3><code>./bin/mh ingest &lt;file-or-dir&gt;</code></h3>
<p>Copy files to memory inbox for processing.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh ingest document.pdf
./bin/mh ingest ~/Documents/notes/
./bin/mh ingest *.txt
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Copies files to <code>memory/inbox/</code></li>
<li>Ingestor agent will process them later</li>
<li>Processed files move to <code>memory/inbox/_archive/</code></li>
</ul>
<p><strong>Supported formats:</strong></p>
<ul>
<li>Text files (.txt, .md)</li>
<li>PDFs</li>
<li>JSON</li>
<li>(More formats via specialized agents)</li>
</ul>
<hr>
<h2>Chat Commands</h2>
<h3><code>./bin/mh chat</code></h3>
<p>Interactive persona-aware chat session.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh chat
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Persona context from <code>persona/core.json</code></li>
<li>Memory grounding (semantic search if indexed)</li>
<li>Conversation history</li>
<li>Cognitive mode awareness</li>
</ul>
<p><strong>Commands during chat:</strong></p>
<ul>
<li>Type messages normally</li>
<li><code>exit</code> or Ctrl+C to quit</li>
</ul>
<hr>
<h2>Audio Commands</h2>
<h3><code>./bin/mh audio ingest &lt;file-or-dir&gt;</code></h3>
<p>Copy audio files to inbox for transcription.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh audio ingest recording.wav
./bin/mh audio ingest ~/Recordings/
</code></pre>
<p><strong>Supported formats:</strong></p>
<ul>
<li>WAV, MP3, FLAC, M4A, OGG</li>
</ul>
<hr>
<h3><code>./bin/mh audio status</code></h3>
<p>Show audio processing status.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh audio status
</code></pre>
<hr>
<h3><code>./bin/mh audio list</code></h3>
<p>List audio files and transcripts.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh audio list
</code></pre>
<hr>
<h3><code>./bin/mh audio info &lt;id&gt;</code></h3>
<p>Show details for a specific audio file.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh audio info audio-2024-11-25-abc123
</code></pre>
<hr>
<h2>Voice Training Commands</h2>
<h3><code>./bin/mh voice status</code></h3>
<p>Show voice training sample collection progress.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh voice status
</code></pre>
<p><strong>Output:</strong></p>
<ul>
<li>Total samples collected</li>
<li>Sample duration</li>
<li>Training readiness</li>
</ul>
<hr>
<h3><code>./bin/mh voice list</code></h3>
<p>List collected voice samples.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh voice list
</code></pre>
<hr>
<h3><code>./bin/mh voice delete &lt;id&gt;</code></h3>
<p>Delete a voice sample.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh voice delete sample-2024-11-25-abc123
</code></pre>
<hr>
<h3><code>./bin/mh voice export</code></h3>
<p>Export dataset for voice training.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh voice export
</code></pre>
<p><strong>Output:</strong>
Exports training dataset to <code>out/voice-training/</code></p>
<hr>
<h2>RVC Voice Cloning Commands</h2>
<h3><code>./bin/mh rvc install</code></h3>
<p>Install RVC (Applio) and dependencies.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh rvc install
</code></pre>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Python 3.10+</li>
<li>CUDA toolkit (for GPU acceleration)</li>
</ul>
<hr>
<h3><code>./bin/mh rvc train [--name &lt;name&gt;]</code></h3>
<p>Train RVC voice model from audio samples.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh rvc train
./bin/mh rvc train --name my-voice
</code></pre>
<p><strong>Requirements:</strong></p>
<ul>
<li>At least 10 minutes of audio samples</li>
<li>Clean audio (minimal background noise)</li>
</ul>
<hr>
<h3><code>./bin/mh rvc test --model &lt;name&gt; --input &lt;file&gt;</code></h3>
<p>Test voice conversion with a trained model.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh rvc test --model my-voice --input test.wav
</code></pre>
<hr>
<h3><code>./bin/mh rvc status</code></h3>
<p>Check RVC installation and list trained models.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh rvc status
</code></pre>
<hr>
<h3><code>./bin/mh rvc uninstall</code></h3>
<p>Remove RVC installation.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh rvc uninstall
</code></pre>
<hr>
<h2>GPT-SoVITS Voice Cloning Commands</h2>
<h3><code>./bin/mh sovits install</code></h3>
<p>Install GPT-SoVITS and dependencies.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits install
</code></pre>
<hr>
<h3><code>./bin/mh sovits start [--port &lt;port&gt;]</code></h3>
<p>Start GPT-SoVITS server.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits start
./bin/mh sovits start --port 9880
</code></pre>
<p><strong>Default port:</strong> 9880</p>
<hr>
<h3><code>./bin/mh sovits stop</code></h3>
<p>Stop the running GPT-SoVITS server.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits stop
</code></pre>
<hr>
<h3><code>./bin/mh sovits restart [--port &lt;port&gt;]</code></h3>
<p>Restart the server.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits restart
./bin/mh sovits restart --port 9880
</code></pre>
<hr>
<h3><code>./bin/mh sovits status</code></h3>
<p>Check server status and health.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits status
</code></pre>
<hr>
<h3><code>./bin/mh sovits logs [--tail N]</code></h3>
<p>Show server logs.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits logs
./bin/mh sovits logs --tail 100
</code></pre>
<p><strong>Default:</strong> Last 50 lines</p>
<hr>
<h3><code>./bin/mh sovits download-models</code></h3>
<p>Download pre-trained models.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits download-models
</code></pre>
<hr>
<h3><code>./bin/mh sovits test [text]</code></h3>
<p>Test server with sample text.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits test &quot;Hello, this is a test&quot;
</code></pre>
<hr>
<h3><code>./bin/mh sovits uninstall</code></h3>
<p>Remove GPT-SoVITS installation.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh sovits uninstall
</code></pre>
<hr>
<h2>Kokoro TTS Commands</h2>
<h3><code>./bin/mh kokoro install</code></h3>
<p>Install Kokoro TTS and dependencies.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh kokoro install
</code></pre>
<hr>
<h3><code>./bin/mh kokoro status</code></h3>
<p>Check Kokoro installation and server status.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh kokoro status
</code></pre>
<hr>
<h3><code>./bin/mh kokoro serve &lt;start|stop&gt; [--port 9882] [--lang a] [--device cpu]</code></h3>
<p>Manage Kokoro TTS server.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># Start server
./bin/mh kokoro serve start

# Start with custom settings
./bin/mh kokoro serve start --port 9883 --lang en --device cuda

# Stop server
./bin/mh kokoro serve stop
</code></pre>
<p><strong>Options:</strong></p>
<ul>
<li><code>--port</code>: Server port (default: 9882)</li>
<li><code>--lang</code>: Language code (default: a for American English)</li>
<li><code>--device</code>: cpu or cuda (default: cpu)</li>
</ul>
<hr>
<h3><code>./bin/mh kokoro voices</code></h3>
<p>List available built-in voices.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh kokoro voices
</code></pre>
<hr>
<h3><code>./bin/mh kokoro test [--text TEXT] [--voice VOICE]</code></h3>
<p>Test synthesis with sample text.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh kokoro test
./bin/mh kokoro test --text &quot;Hello world&quot; --voice af_sarah
</code></pre>
<hr>
<h3><code>./bin/mh kokoro train-voicepack [--speaker NAME] [--dataset DIR] [--epochs N]</code></h3>
<p>Train custom voicepack from audio samples.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh kokoro train-voicepack --speaker my-voice --dataset ./samples --epochs 100
</code></pre>
<p><strong>Requirements:</strong></p>
<ul>
<li>Clean audio samples (WAV format)</li>
<li>At least 30 minutes of audio</li>
<li>Consistent recording quality</li>
</ul>
<hr>
<h3><code>./bin/mh kokoro uninstall</code></h3>
<p>Remove Kokoro installation.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh kokoro uninstall
</code></pre>
<hr>
<h2>LoRA Adapter Commands</h2>
<h3><code>./bin/mh adapter list</code></h3>
<p>List all datasets (pending, approved, trained).</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter list
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Pending Datasets:
  2024-11-20 - 45 pairs

Approved Datasets:
  2024-11-15 - 67 pairs (approved)

Trained Adapters:
  2024-11-10 - active (eval: 0.85)
  2024-11-05 - archived
</code></pre>
<hr>
<h3><code>./bin/mh adapter merge</code></h3>
<p>Merge historical adapters into single consolidated adapter.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter merge
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Combines all past training cycles</li>
<li>Creates <code>history-merged.gguf</code></li>
<li>Used for dual-adapter models</li>
</ul>
<hr>
<h3><code>./bin/mh adapter review &lt;date&gt;</code></h3>
<p>Review dataset and show sample conversation pairs.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter review 2024-11-20
</code></pre>
<p><strong>Output:</strong>
Shows sample user/assistant pairs from the dataset.</p>
<hr>
<h3><code>./bin/mh adapter approve &lt;date&gt; [notes]</code></h3>
<p>Approve dataset for training.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter approve 2024-11-20 &quot;Good quality samples&quot;
./bin/mh adapter approve 2024-11-20
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Marks dataset as approved</li>
<li>Enables training</li>
<li>Logs approval to audit trail</li>
</ul>
<hr>
<h3><code>./bin/mh adapter reject &lt;date&gt; [reason]</code></h3>
<p>Reject and archive dataset.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter reject 2024-11-20 &quot;Low quality samples&quot;
</code></pre>
<hr>
<h3><code>./bin/mh adapter train &lt;date&gt;</code></h3>
<p>Train LoRA adapter from approved dataset.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter train 2024-11-20
</code></pre>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Dataset must be approved</li>
<li>GPU with sufficient VRAM (24GB+ recommended for 30B models)</li>
<li>Training configuration in <code>etc/training.json</code></li>
</ul>
<p><strong>Training process:</strong></p>
<ol>
<li>Loads base model from <code>etc/training.json</code></li>
<li>Trains LoRA adapter using Unsloth</li>
<li>Saves adapter to <code>out/lora-adapters/&lt;date&gt;/</code></li>
<li>Runs evaluation automatically</li>
</ol>
<hr>
<h3><code>./bin/mh adapter eval &lt;date&gt;</code></h3>
<p>Evaluate trained adapter quality.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter eval 2024-11-20
</code></pre>
<p><strong>Output:</strong></p>
<ul>
<li>Perplexity score</li>
<li>Sample generations</li>
<li>Quality metrics</li>
</ul>
<hr>
<h3><code>./bin/mh adapter activate &lt;date&gt;</code></h3>
<p>Activate adapter for use in chat.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh adapter activate 2024-11-20
</code></pre>
<p><strong>Prerequisites:</strong></p>
<ul>
<li>Adapter must be trained</li>
<li>Must pass evaluation threshold</li>
</ul>
<p><strong>What it does:</strong></p>
<ul>
<li>Updates <code>active-adapter.json</code></li>
<li>Converts adapter to GGUF format (if needed)</li>
<li>Restarts affected services</li>
</ul>
<hr>
<h2>Persona Management Commands</h2>
<h3>Profile Commands</h3>
<h4><code>./bin/mh persona activate</code></h4>
<p>Generate and activate daily profile (run morning-loader agent).</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona activate
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Runs morning-loader agent</li>
<li>Generates daily profile from base persona</li>
<li>Merges recent context and priorities</li>
<li>Activates for current session</li>
</ul>
<hr>
<h4><code>./bin/mh persona status</code></h4>
<p>Show current persona state (profile, adapter).</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona status
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Profile: daily-2024-11-25
Adapter: 2024-11-20 (active)
Trust Level: observe
Cognitive Mode: dual
</code></pre>
<hr>
<h4><code>./bin/mh persona diff</code></h4>
<p>Compare base persona vs active profile.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona diff
</code></pre>
<p><strong>Output:</strong>
Shows differences between <code>persona/core.json</code> and active daily profile.</p>
<hr>
<h3>Generator Commands (Interactive Interview)</h3>
<h4><code>./bin/mh persona generate</code></h4>
<p>Start interactive personality interview to generate/update persona.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona generate
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Starts LLM-guided interview</li>
<li>Asks questions about values, goals, communication style</li>
<li>Generates persona JSON from responses</li>
<li>Saves session for review</li>
</ul>
<hr>
<h4><code>./bin/mh persona generate --resume</code></h4>
<p>Resume latest active interview session.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona generate --resume
</code></pre>
<hr>
<h4><code>./bin/mh persona sessions</code></h4>
<p>List all interview sessions.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona sessions
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Interview Sessions:

  session-2024-11-25-abc123 (active)
    Started: 2024-11-25 10:15:00
    Progress: 12/20 questions

  session-2024-11-20-def456 (completed)
    Started: 2024-11-20 09:00:00
    Completed: 2024-11-20 09:45:00
</code></pre>
<hr>
<h4><code>./bin/mh persona view &lt;id&gt;</code></h4>
<p>View session transcript.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona view session-2024-11-25-abc123
</code></pre>
<p><strong>Output:</strong>
Shows full Q&amp;A transcript from interview.</p>
<hr>
<h4><code>./bin/mh persona apply &lt;id&gt; [strategy]</code></h4>
<p>Apply persona changes from interview session.</p>
<p><strong>Strategies:</strong></p>
<ul>
<li><code>replace</code> - Replace entire persona (default)</li>
<li><code>merge</code> - Merge with existing persona (keep existing values not in new data)</li>
<li><code>append</code> - Append new sections, don&#39;t modify existing</li>
</ul>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># Replace entire persona
./bin/mh persona apply session-2024-11-25-abc123

# Merge new values with existing
./bin/mh persona apply session-2024-11-25-abc123 merge

# Only append new sections
./bin/mh persona apply session-2024-11-25-abc123 append
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Updates <code>persona/core.json</code></li>
<li>Logs change to audit trail</li>
<li>Backs up old persona to <code>persona/backups/</code></li>
</ul>
<hr>
<h4><code>./bin/mh persona discard &lt;id&gt;</code></h4>
<p>Delete an interview session.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh persona discard session-2024-11-25-abc123
</code></pre>
<hr>
<h4><code>./bin/mh persona cleanup [--dry-run] [--max-age &lt;days&gt;]</code></h4>
<p>Clean up old interview sessions.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash"># Preview what would be deleted
./bin/mh persona cleanup --dry-run

# Delete sessions older than 30 days (default)
./bin/mh persona cleanup

# Delete sessions older than 60 days
./bin/mh persona cleanup --max-age 60
</code></pre>
<hr>
<h2>Multi-User Management Commands</h2>
<h3><code>./bin/mh user list</code></h3>
<p>List all registered users.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh user list
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Users:

  greggles (owner)
    ID: user-abc123
    Created: 2024-11-01

  friend (viewer)
    ID: user-def456
    Created: 2024-11-15
</code></pre>
<hr>
<h3><code>./bin/mh user whoami</code></h3>
<p>Show current user context.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh user whoami
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>Current user: greggles
Role: owner
ID: user-abc123
</code></pre>
<hr>
<h3><code>./bin/mh user info &lt;name&gt;</code></h3>
<p>Show detailed info for a specific user.</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh user info greggles
</code></pre>
<p><strong>Output:</strong></p>
<pre><code>User: greggles
Role: owner
ID: user-abc123
Created: 2024-11-01
Profile Path: profiles/greggles/
</code></pre>
<hr>
<h3><code>./bin/mh --user &lt;username&gt; &lt;command&gt;</code></h3>
<p>Run command as specific user (for multi-user setups).</p>
<p><strong>Usage:</strong></p>
<pre><code class="language-bash">./bin/mh --user greggles status
./bin/mh -u friend task
</code></pre>
<p><strong>What it does:</strong></p>
<ul>
<li>Switches user context for the command</li>
<li>Loads that user&#39;s profile paths</li>
<li>All operations use their data directories</li>
</ul>
<p><strong>Example multi-user workflow:</strong></p>
<pre><code class="language-bash"># Capture memory as user &quot;alice&quot;
./bin/mh --user alice capture &quot;Had a great meeting today&quot;

# List alice&#39;s tasks
./bin/mh -u alice task

# Show alice&#39;s status
./bin/mh --user alice status
</code></pre>
<hr>
<h2>Unimplemented Commands</h2>
<p>The following commands appear in older documentation but are <strong>not currently implemented</strong> in the CLI:</p>
<h3>Calendar Commands (Not Implemented)</h3>
<pre><code class="language-bash"># These commands do NOT work:
./bin/mh calendar list
./bin/mh calendar create &quot;&lt;title&gt;&quot;
</code></pre>
<p><strong>Status:</strong> Calendar functionality exists as skills (<code>calendar_list</code>, <code>calendar_create</code>) that can be called by the operator in the web UI, but direct CLI commands are not implemented.</p>
<p><strong>Workaround:</strong> Use the web UI chat and ask the operator to manage calendar events.</p>
<hr>
<h3>Fine-Tune Command (Not Implemented)</h3>
<pre><code class="language-bash"># This command does NOT work:
./bin/mh fine-tune [--username &lt;name&gt;]
</code></pre>
<p><strong>Status:</strong> The command handler exists in <code>packages/cli/src/commands/fine-tune.ts</code> but is <strong>not registered</strong> in the main CLI router. Fine-tuning is only available via:</p>
<ol>
<li>Web UI (&quot;Run Full Cycle Now&quot; button)</li>
<li>Direct script execution: <code>pnpm tsx brain/agents/fine-tune-cycle.ts</code></li>
</ol>
<p><strong>Why:</strong> The fine-tune CLI command was never added to the command router in <code>mh-new.ts</code>.</p>
<hr>
<h2>Command Aliases &amp; Shortcuts</h2>
<h3>Short flags</h3>
<ul>
<li><code>-h</code> ‚Üí <code>--help</code></li>
<li><code>-r</code> ‚Üí <code>--restart</code> (for <code>mh start</code>)</li>
<li><code>-f</code> ‚Üí <code>--force</code> (for <code>mh start</code> and <code>mh agent stop</code>)</li>
<li><code>-u</code> ‚Üí <code>--user</code> (for multi-user commands)</li>
</ul>
<h3>Exit commands</h3>
<ul>
<li><code>exit</code>, <code>quit</code>, <code>Ctrl+C</code> ‚Üí Exit interactive sessions (chat, ollama chat)</li>
</ul>
<hr>
<h2>Environment Variables</h2>
<h3>User Context</h3>
<ul>
<li><code>USER</code> - Default username for multi-user commands (falls back to &quot;greggles&quot;)</li>
</ul>
<h3>Training Configuration</h3>
<ul>
<li><code>METAHUMAN_BASE_MODEL</code> - Override base model for training (default: from <code>etc/training.json</code>)</li>
</ul>
<h3>Development</h3>
<ul>
<li><code>NODE_PATH</code> - Node module search paths (set automatically by CLI)</li>
</ul>
<hr>
<h2>Tips &amp; Best Practices</h2>
<h3>Command Patterns</h3>
<p><strong>Chain commands with bash:</strong></p>
<pre><code class="language-bash"># Capture and immediately search
./bin/mh capture &quot;Important note&quot; &amp;&amp; ./bin/mh remember &quot;Important&quot;

# Run multiple agents
./bin/mh agent run organizer &amp;&amp; ./bin/mh agent run reflector
</code></pre>
<p><strong>Use command substitution:</strong></p>
<pre><code class="language-bash"># Get task count
TASK_COUNT=$(./bin/mh task | grep -c &quot;Status:&quot;)
echo &quot;You have $TASK_COUNT active tasks&quot;
</code></pre>
<h3>Multi-User Workflows</h3>
<p><strong>Switch between users:</strong></p>
<pre><code class="language-bash"># Work as user &quot;alice&quot;
export MH_USER=alice
./bin/mh status
./bin/mh capture &quot;Alice&#39;s note&quot;

# Switch to user &quot;bob&quot;
export MH_USER=bob
./bin/mh status
</code></pre>
<p><strong>Or use flag for each command:</strong></p>
<pre><code class="language-bash">./bin/mh -u alice capture &quot;Alice&#39;s note&quot;
./bin/mh -u bob task
</code></pre>
<h3>Agent Management</h3>
<p><strong>Check if agents are stuck:</strong></p>
<pre><code class="language-bash">./bin/mh agent ps | grep &quot;days&quot;  # Check for long-running agents
</code></pre>
<p><strong>Clean restart all agents:</strong></p>
<pre><code class="language-bash">./bin/mh agent stop --all
./bin/mh start --force
</code></pre>
<p><strong>Monitor agent activity:</strong></p>
<pre><code class="language-bash">watch -n 5 &#39;./bin/mh agent ps&#39;
</code></pre>
<h3>Memory Best Practices</h3>
<p><strong>Regular captures:</strong></p>
<pre><code class="language-bash"># Add to your shell alias
alias note=&#39;./bin/mh capture&#39;

# Then just:
note &quot;Quick observation&quot;
</code></pre>
<p><strong>Search workflow:</strong></p>
<pre><code class="language-bash"># Build index once
./bin/mh ollama pull nomic-embed-text
./bin/mh index build

# Then semantic search works
./bin/mh remember &quot;project discussions&quot;
</code></pre>
<p><strong>Monitor processing:</strong></p>
<pre><code class="language-bash"># Check if organizer is keeping up
./bin/mh agent monitor
</code></pre>
<hr>
<h2>Troubleshooting</h2>
<h3>&quot;Not initialized. Run: mh init&quot;</h3>
<p><strong>Solution:</strong> Run <code>./bin/mh init</code> to create required directories.</p>
<hr>
<h3>&quot;Ollama is not running&quot;</h3>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Start Ollama service
ollama serve

# Or check status
./bin/mh ollama status
</code></pre>
<hr>
<h3>&quot;Another instance is already running&quot;</h3>
<p><strong>Cause:</strong> Stale lock file.</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Check if process is actually running
ps -p $(cat logs/run/locks/&lt;agent-name&gt;.lock | jq -r &#39;.pid&#39;)

# If not running, remove lock
rm logs/run/locks/&lt;agent-name&gt;.lock
</code></pre>
<hr>
<h3>Command not found: mh</h3>
<p><strong>Cause:</strong> Using <code>mh</code> instead of <code>./bin/mh</code></p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash"># Either use full path
./bin/mh status

# Or add to PATH
export PATH=&quot;$PATH:$(pwd)/bin&quot;
mh status
</code></pre>
<hr>
<h3>Permission denied</h3>
<p><strong>Cause:</strong> <code>bin/mh</code> script not executable.</p>
<p><strong>Solution:</strong></p>
<pre><code class="language-bash">chmod +x bin/mh
</code></pre>
<hr>
<h2>See Also</h2>
<ul>
<li><strong><a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a></strong> - Background agent details</li>
<li><strong><a href="../advanced-features/multi-user-profiles.md">Multi-User Profiles</a></strong> - Multi-user setup guide</li>
<li><strong><a href="./troubleshooting.md">Troubleshooting</a></strong> - Common issues and solutions</li>
<li><strong><a href="../training-personalization/voice-system.md">Voice System</a></strong> - Voice training details</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="chat-interface" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Chat Interface</span> </span> </button> <button class="nav-button next-button" data-target="cognitive-modes" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Cognitive Modes</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="cognitive-modes" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Cognitive Modes</h1>
<p>MetaHuman OS operates in three distinct cognitive modes that control memory recording, operator behavior, proactive agents, and training pipelines. Each mode serves different use cases and trust levels.</p>
<h2>Overview</h2>
<p><strong>Cognitive modes</strong> define how MetaHuman processes your interactions:</p>
<ul>
<li><strong>Memory writes</strong>: What gets saved to episodic memory</li>
<li><strong>Operator routing</strong>: When the ReAct operator pipeline is used</li>
<li><strong>Proactive agents</strong>: Whether background agents run automatically</li>
<li><strong>Training pipeline</strong>: How AI models learn from your interactions</li>
</ul>
<p><strong>Three Modes:</strong></p>
<ol>
<li><strong>Dual Consciousness</strong> (Default) - Full system capabilities with dual-adapter training</li>
<li><strong>Agent</strong> - Lightweight assistant mode with selective operator use</li>
<li><strong>Emulation</strong> - Read-only demonstration mode with stable personality</li>
</ol>
<h2>Mode Comparison</h2>
<table>
<thead>
<tr>
<th>Feature</th>
<th>Dual Consciousness</th>
<th>Agent</th>
<th>Emulation</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Memory Writes</strong></td>
<td>Full (all interactions)</td>
<td>Command-only (explicit saves)</td>
<td>Read-only (no writes)</td>
</tr>
<tr>
<td><strong>Operator Usage</strong></td>
<td>Always</td>
<td>Heuristic (smart detection)</td>
<td>Never</td>
</tr>
<tr>
<td><strong>Proactive Agents</strong></td>
<td>Enabled</td>
<td>Disabled</td>
<td>Disabled</td>
</tr>
<tr>
<td><strong>Training Pipeline</strong></td>
<td>Dual-trigger (monthly + manual)</td>
<td>Disabled</td>
<td>Disabled</td>
</tr>
<tr>
<td><strong>Recording</strong></td>
<td>All conversations</td>
<td>Only explicit captures</td>
<td>None</td>
</tr>
<tr>
<td><strong>Use Case</strong></td>
<td>Primary operational mode</td>
<td>Lightweight assistant</td>
<td>Demo/testing</td>
</tr>
</tbody></table>
<h2>Dual Consciousness Mode</h2>
<p><strong>Default operational mode</strong> with full system capabilities.</p>
<h3>Key Behaviors</h3>
<p><strong>Memory Recording:</strong></p>
<ul>
<li>All chat messages saved to episodic memory</li>
<li>Memories tagged with <code>metadata.cognitiveMode: &quot;dual&quot;</code></li>
<li>Used for LoRA training to personalize AI responses</li>
</ul>
<p><strong>Operator Routing:</strong></p>
<ul>
<li><strong>Always routes</strong> through ReAct operator pipeline</li>
<li>Every message goes: planner ‚Üí skills ‚Üí narrator</li>
<li>Mandatory memory grounding via semantic search</li>
<li>Fallback to persona if no relevant memories</li>
</ul>
<p><strong>Proactive Agents:</strong></p>
<ul>
<li>Reflector: Generates internal reflections</li>
<li>Boredom Maintenance: Triggers reflections after inactivity</li>
<li>Curiosity Service: Asks user-facing questions</li>
<li>Inner Curiosity: Self-directed internal questions</li>
<li>Sleep Service: Manages dream generation</li>
</ul>
<p><strong>Training Pipeline:</strong></p>
<ul>
<li><strong>Dual-trigger</strong>: Monthly automatic + manual training</li>
<li>Builds dual-adapter system:<ul>
<li>Historical adapter: All-time consolidated memory</li>
<li>Recent adapter: Last 30 days (default)</li>
</ul>
</li>
<li>Curator agent builds training datasets</li>
<li>Auto-approval with quality thresholds</li>
</ul>
<h3>When to Use</h3>
<ul>
<li><strong>Primary mode</strong>: Day-to-day interaction with full capabilities</li>
<li><strong>Personality evolution</strong>: When you want AI to learn from conversations</li>
<li><strong>Proactive assistance</strong>: When you want background agents active</li>
<li><strong>Memory building</strong>: When building long-term episodic memory</li>
</ul>
<h3>Example Workflow</h3>
<ol>
<li>User sends message: &quot;What tasks are due today?&quot;</li>
<li><strong>Planner</strong> analyzes intent ‚Üí routes to task_list skill</li>
<li><strong>Skill</strong> executes: reads active tasks, filters by due date</li>
<li><strong>Narrator</strong> synthesizes: &quot;You have 3 tasks due today: ...&quot;</li>
<li><strong>Memory</strong>: Saves conversation with <code>cognitiveMode: &quot;dual&quot;</code></li>
<li><strong>Training</strong>: Conversation included in next training cycle</li>
</ol>
<h2>Agent Mode</h2>
<p><strong>Lightweight assistant mode</strong> with selective operator use.</p>
<h3>Key Behaviors</h3>
<p><strong>Memory Recording:</strong></p>
<ul>
<li>Only explicit captures saved (e.g., <code>mh capture</code>)</li>
<li>Chat messages NOT automatically saved</li>
<li>Memories tagged with <code>metadata.cognitiveMode: &quot;agent&quot;</code></li>
</ul>
<p><strong>Operator Routing:</strong></p>
<ul>
<li><strong>Heuristic-based detection</strong></li>
<li>Simple queries ‚Üí direct chat (no operator)</li>
<li>Action-oriented messages ‚Üí operator pipeline</li>
<li>Examples:<ul>
<li>&quot;What&#39;s the weather?&quot; ‚Üí Chat only (fast)</li>
<li>&quot;Create a task for tomorrow&quot; ‚Üí Operator + task_create skill</li>
<li>&quot;Tell me about yourself&quot; ‚Üí Chat only</li>
</ul>
</li>
</ul>
<p><strong>Detection Heuristics:</strong></p>
<pre><code>Action keywords: create, add, update, delete, find, search, list, show
Intent patterns: imperative verbs, specific requests
Context clues: mentions of tasks, memories, files, agents
</code></pre>
<p><strong>Proactive Agents:</strong></p>
<ul>
<li>Disabled (no reflections, curiosity, dreams)</li>
</ul>
<p><strong>Training Pipeline:</strong></p>
<ul>
<li>Disabled (no automatic training)</li>
<li>Stable personality snapshot</li>
</ul>
<h3>When to Use</h3>
<ul>
<li><strong>Reduced cognitive load</strong>: When you don&#39;t want full operator overhead</li>
<li><strong>Faster responses</strong>: Simple queries bypass operator pipeline</li>
<li><strong>Privacy mode</strong>: When you don&#39;t want conversations saved</li>
<li><strong>Testing</strong>: Experimenting without affecting training data</li>
<li><strong>Demonstration</strong>: Showing features without memory writes</li>
</ul>
<h3>Example Workflow</h3>
<ol>
<li><p>User: &quot;What&#39;s 2+2?&quot; ‚Üí <strong>Chat only</strong> (simple query)</p>
<ul>
<li>Fast response, no operator</li>
<li>Not saved to memory</li>
</ul>
</li>
<li><p>User: &quot;Create a task to buy groceries&quot; ‚Üí <strong>Operator used</strong></p>
<ul>
<li>Detected: &quot;create&quot; + &quot;task&quot;</li>
<li>Routes to operator ‚Üí task_create skill</li>
<li>Task created, but conversation not saved</li>
</ul>
</li>
</ol>
<h2>Emulation Mode</h2>
<p><strong>Read-only demonstration mode</strong> with stable personality.</p>
<h3>Key Behaviors</h3>
<p><strong>Memory Recording:</strong></p>
<ul>
<li>No writes (read-only)</li>
<li>Can read existing memories</li>
<li>Ideal for sharing/demo without modification</li>
</ul>
<p><strong>Operator Routing:</strong></p>
<ul>
<li>Never uses operator</li>
<li>All messages handled by chat only</li>
<li>Fast, lightweight responses</li>
</ul>
<p><strong>Proactive Agents:</strong></p>
<ul>
<li>Disabled</li>
</ul>
<p><strong>Training Pipeline:</strong></p>
<ul>
<li>Disabled</li>
<li>Uses last trained model (frozen personality)</li>
</ul>
<h3>When to Use</h3>
<ul>
<li><strong>Demonstration</strong>: Showing MetaHuman to others</li>
<li><strong>Testing</strong>: Experimenting without side effects</li>
<li><strong>Snapshot mode</strong>: Using a stable personality version</li>
<li><strong>Public sharing</strong>: Safe read-only access</li>
<li><strong>Development</strong>: Testing features without affecting data</li>
</ul>
<h3>Example Workflow</h3>
<ol>
<li>User: &quot;What tasks do I have?&quot;</li>
<li>Chat reads existing tasks (read-only)</li>
<li>Returns list without saving conversation</li>
<li>No operator overhead</li>
<li>No memory writes</li>
</ol>
<h2>Switching Modes</h2>
<h3>Via Web UI</h3>
<ol>
<li>Click <strong>mode dropdown</strong> in header (top-right)</li>
<li>Select mode: Dual Consciousness | Agent | Emulation</li>
<li>Confirmation: &quot;Switched to [mode] mode&quot;</li>
<li>All future interactions use new mode</li>
</ol>
<h3>Via API</h3>
<p><strong>Endpoint:</strong> <code>POST /api/cognitive-mode</code></p>
<p><strong>Request Body:</strong></p>
<pre><code class="language-json">{
  &quot;mode&quot;: &quot;dual&quot;  // or &quot;agent&quot; or &quot;emulation&quot;
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-json">{
  &quot;success&quot;: true,
  &quot;mode&quot;: &quot;dual&quot;,
  &quot;previousMode&quot;: &quot;agent&quot;,
  &quot;timestamp&quot;: &quot;2025-11-25T14:30:22Z&quot;
}
</code></pre>
<h3>Via CLI</h3>
<p><strong>Check current mode:</strong></p>
<pre><code class="language-bash">cat persona/cognitive-mode.json
</code></pre>
<p><strong>Manual mode file edit:</strong></p>
<pre><code class="language-json">{
  &quot;mode&quot;: &quot;dual&quot;,
  &quot;history&quot;: [
    {
      &quot;mode&quot;: &quot;agent&quot;,
      &quot;timestamp&quot;: &quot;2025-11-25T12:00:00Z&quot;,
      &quot;actor&quot;: &quot;greggles&quot;,
      &quot;reason&quot;: &quot;Testing lightweight mode&quot;
    }
  ]
}
</code></pre>
<p><strong>Note:</strong> Mode changes require write access (not available to anonymous users).</p>
<h2>Mode Locking</h2>
<p><strong>Special States:</strong></p>
<p>Some modes are locked for safety or operational reasons:</p>
<h3>High Security Mode</h3>
<ul>
<li>Locks to emulation or agent mode</li>
<li>Prevents dual mode (no training on sensitive data)</li>
<li>Use case: Handling confidential information</li>
</ul>
<h3>Wetware Deceased</h3>
<ul>
<li>Locks all modes (read-only memorial)</li>
<li>Preserves final personality state</li>
<li>Use case: Digital legacy/memorial mode</li>
</ul>
<p><strong>Configuration:</strong> <code>persona/cognitive-mode.json</code></p>
<pre><code class="language-json">{
  &quot;mode&quot;: &quot;emulation&quot;,
  &quot;locked&quot;: true,
  &quot;lockReason&quot;: &quot;high_security_mode&quot;,
  &quot;lockedAt&quot;: &quot;2025-11-25T14:30:00Z&quot;,
  &quot;lockedBy&quot;: &quot;system&quot;
}
</code></pre>
<p><strong>Unlocking:</strong> Requires owner authentication and explicit unlock action.</p>
<h2>Mode History Tracking</h2>
<p>All mode changes are logged with full context:</p>
<p><strong>Storage:</strong> <code>persona/cognitive-mode.json</code></p>
<p><strong>History Entry:</strong></p>
<pre><code class="language-json">{
  &quot;mode&quot;: &quot;agent&quot;,
  &quot;timestamp&quot;: &quot;2025-11-25T14:30:22Z&quot;,
  &quot;actor&quot;: &quot;greggles&quot;,
  &quot;reason&quot;: &quot;Switching to lightweight mode for quick queries&quot;,
  &quot;previousMode&quot;: &quot;dual&quot;
}
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>mode</code>: New mode activated</li>
<li><code>timestamp</code>: When the change occurred</li>
<li><code>actor</code>: Who made the change (username or &#39;system&#39;)</li>
<li><code>reason</code>: Optional explanation</li>
<li><code>previousMode</code>: Mode before switch</li>
</ul>
<p><strong>Use Cases:</strong></p>
<ul>
<li>Audit trail for mode switching</li>
<li>Understanding personality evolution context</li>
<li>Debugging training data composition</li>
<li>LoRA training dataset filtering</li>
</ul>
<h2>Memory Metadata by Mode</h2>
<p>Every memory includes <code>metadata.cognitiveMode</code> field:</p>
<p><strong>Example Memory:</strong></p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;evt_abc123&quot;,
  &quot;type&quot;: &quot;conversation&quot;,
  &quot;timestamp&quot;: &quot;2025-11-25T14:30:00Z&quot;,
  &quot;content&quot;: {
    &quot;userMessage&quot;: &quot;What tasks are due?&quot;,
    &quot;assistantMessage&quot;: &quot;You have 3 tasks due today...&quot;
  },
  &quot;metadata&quot;: {
    &quot;cognitiveMode&quot;: &quot;dual&quot;,
    &quot;usedOperator&quot;: true,
    &quot;processed&quot;: true
  }
}
</code></pre>
<p><strong>Training Dataset Filtering:</strong></p>
<p>When building LoRA training datasets, the curator agent can filter by mode:</p>
<ul>
<li>Include only <code>dual</code> mode memories (full context)</li>
<li>Exclude <code>agent</code> mode (command-only snippets)</li>
<li>Exclude <code>emulation</code> mode (no writes anyway)</li>
</ul>
<p><strong>Dataset Quality:</strong></p>
<p>Dual mode provides highest-quality training data:</p>
<ul>
<li>Complete conversations with full context</li>
<li>Natural interaction patterns</li>
<li>Operator reasoning included</li>
<li>Memory-grounded responses</li>
</ul>
<h2>Best Practices</h2>
<h3>For Daily Use (Dual Mode)</h3>
<ol>
<li><strong>Default mode</strong> for primary operations</li>
<li>Let proactive agents enrich your experience</li>
<li>Build episodic memory naturally through conversation</li>
<li>Train monthly for personality evolution</li>
<li>Review reflections and curiosity questions</li>
</ol>
<h3>For Quick Queries (Agent Mode)</h3>
<ol>
<li>Switch when you want <strong>fast, lightweight responses</strong></li>
<li>Good for:<ul>
<li>Quick information lookups</li>
<li>Simple calculations or definitions</li>
<li>Testing features without memory impact</li>
</ul>
</li>
<li>Remember: Only explicit captures are saved</li>
<li>Operator used only for action-oriented requests</li>
</ol>
<h3>For Demonstrations (Emulation Mode)</h3>
<ol>
<li>Use when showing MetaHuman to others</li>
<li>Safe read-only access (no accidental writes)</li>
<li>Stable personality (uses last trained model)</li>
<li>No background agents (predictable behavior)</li>
<li>Fast responses (no operator overhead)</li>
</ol>
<h3>For Privacy</h3>
<p><strong>Agent Mode:</strong></p>
<ul>
<li>Conversations not saved</li>
<li>Only explicit captures recorded</li>
<li>Good for sensitive topics you don&#39;t want in training data</li>
</ul>
<p><strong>Emulation Mode:</strong></p>
<ul>
<li>No writes at all</li>
<li>Pure read-only access</li>
<li>Use for public demos or untrusted environments</li>
</ul>
<h2>Technical Details</h2>
<h3>Mode Definition Structure</h3>
<p><strong>Source:</strong> <code>packages/core/src/cognitive-mode.ts</code></p>
<pre><code class="language-typescript">const MODE_DEFINITIONS: Record&lt;CognitiveModeId, CognitiveModeDefinition&gt; = {
  dual: {
    defaults: {
      recordingEnabled: true,
      proactiveAgents: true,
      trainingPipeline: &#39;dual_trigger&#39;,
      memoryWriteLevel: &#39;full&#39;,
    },
  },
  agent: {
    defaults: {
      recordingEnabled: false,
      proactiveAgents: false,
      trainingPipeline: &#39;disabled&#39;,
      memoryWriteLevel: &#39;command_only&#39;,
    },
  },
  emulation: {
    defaults: {
      recordingEnabled: false,
      proactiveAgents: false,
      trainingPipeline: &#39;disabled&#39;,
      memoryWriteLevel: &#39;read_only&#39;,
    },
  },
};
</code></pre>
<h3>Memory Write Levels</h3>
<ul>
<li><strong>full</strong>: All interactions saved</li>
<li><strong>command_only</strong>: Only explicit captures (e.g., <code>mh capture</code>)</li>
<li><strong>read_only</strong>: No writes, reads allowed</li>
</ul>
<h3>Training Pipeline Options</h3>
<ul>
<li><strong>dual_trigger</strong>: Monthly automatic + manual training</li>
<li><strong>manual_only</strong>: Only user-initiated training</li>
<li><strong>disabled</strong>: No training</li>
</ul>
<h3>Operator Routing Logic</h3>
<p><strong>Dual Mode:</strong></p>
<pre><code class="language-typescript">if (cognitiveMode === &#39;dual&#39;) {
  // Always use operator
  return await runOperator(message);
}
</code></pre>
<p><strong>Agent Mode:</strong></p>
<pre><code class="language-typescript">if (cognitiveMode === &#39;agent&#39;) {
  const isAction = detectActionIntent(message);
  if (isAction) {
    return await runOperator(message);
  }
  return await chatWithPersona(message);
}
</code></pre>
<p><strong>Emulation Mode:</strong></p>
<pre><code class="language-typescript">if (cognitiveMode === &#39;emulation&#39;) {
  // Never use operator
  return await chatWithPersona(message);
}
</code></pre>
<h2>Troubleshooting</h2>
<h3>Can&#39;t Switch Modes</h3>
<p><strong>Cause:</strong> Not authenticated or in emulation mode
<strong>Solution:</strong></p>
<ul>
<li>Must be logged in as owner</li>
<li>Emulation mode users cannot switch modes (read-only)</li>
<li>Check authentication status</li>
</ul>
<h3>Mode Switch Not Taking Effect</h3>
<p><strong>Cause:</strong> Cache or session issue
<strong>Solution:</strong></p>
<ol>
<li>Refresh browser page</li>
<li>Check <code>persona/cognitive-mode.json</code> for current mode</li>
<li>Verify mode change in header dropdown</li>
</ol>
<h3>Training Not Happening in Dual Mode</h3>
<p><strong>Cause:</strong> Training pipeline configuration
<strong>Solution:</strong></p>
<ol>
<li>Check <code>etc/training.json</code> for <code>monthly_training: true</code></li>
<li>Verify enough memories collected (min 100)</li>
<li>Check agent logs: <code>./bin/mh agent status</code></li>
<li>Review <code>logs/audit/</code> for training events</li>
</ol>
<h3>Operator Not Being Used in Agent Mode</h3>
<p><strong>Cause:</strong> Message not detected as action-oriented
<strong>Solution:</strong></p>
<ul>
<li>Use explicit action verbs: &quot;create&quot;, &quot;add&quot;, &quot;update&quot;, &quot;list&quot;</li>
<li>Be specific: &quot;List my tasks&quot; instead of &quot;What&#39;s up?&quot;</li>
<li>Check audit logs for operator usage: <code>usedOperator: true/false</code></li>
</ul>
<h3>Proactive Agents Running in Agent Mode</h3>
<p><strong>Cause:</strong> Mode configuration not applied to scheduler
<strong>Solution:</strong></p>
<ol>
<li>Check <code>etc/agents.json</code> for proactive agent status</li>
<li>Restart scheduler: <code>./bin/mh agent stop scheduler-service &amp;&amp; ./bin/mh agent run scheduler-service</code></li>
<li>Verify agents stopped: <code>./bin/mh agent ps</code></li>
</ol>
<h2>Next Steps</h2>
<ul>
<li>Configure <a href="ai-training.md">AI Training</a> for dual-adapter personality evolution</li>
<li>Adjust <a href="persona-editor.md">Persona Editor</a> for mode-specific behaviors</li>
<li>Set up <a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a> for dual mode</li>
<li>Review <a href="../using-metahuman/dashboard-monitoring.md">Dashboard</a> for mode status</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="cli-reference" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Cli Reference</span> </span> </button> <button class="nav-button next-button" data-target="configuration-files" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Configuration Files</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="configuration-files" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h2>Configuration Files</h2>
<p>MetaHuman OS uses a dual-tier configuration architecture to support multi-user isolation while sharing infrastructure settings.</p>
<h3>Configuration Architecture</h3>
<p><strong>Per-User Configs</strong> (<code>profiles/&lt;username&gt;/etc/</code>):
Each user has their own isolated configuration files that affect personality, behavior, and user-specific settings. These are copied when guests select profiles:</p>
<ul>
<li><code>models.json</code> - LLM model settings and persona inclusion</li>
<li><code>training.json</code> - LoRA adapter training parameters</li>
<li><code>cognitive-layers.json</code> - Cognitive mode settings</li>
<li><code>autonomy.json</code> - Autonomy level configuration</li>
<li><code>trust-coupling.json</code> - Trust level mappings</li>
<li><code>boredom.json</code> - Boredom service schedule</li>
<li><code>sleep.json</code> - Sleep/dream time windows</li>
<li><code>voice.json</code> - TTS/STT settings (with template variables)</li>
<li><code>audio.json</code> - Audio processing configuration</li>
<li><code>ingestor.json</code> - Inbox file processing settings</li>
<li><code>curiosity.json</code> - Curiosity system configuration</li>
<li><code>agents.json</code> - Agent execution schedules</li>
<li><code>auto-approval.json</code> - Auto-approval rules</li>
<li><code>adapter-builder.json</code> - Adapter building settings</li>
<li><code>logging.json</code> - Logging preferences</li>
</ul>
<p><strong>Global Configs</strong> (<code>etc/</code>):
System-wide infrastructure settings shared across all users:</p>
<ul>
<li><code>cloudflare.json</code> - Tunnel configuration</li>
<li><code>network.json</code> - Network settings</li>
<li><code>lifeline.json</code> - System service configuration</li>
<li><code>runtime.json</code> - Runtime feature flags and implementation choices</li>
</ul>
<p><strong>Path Resolution</strong>: The <code>paths</code> proxy in <code>@metahuman/core</code> automatically resolves to the correct user-specific directory based on session context. For example, <code>paths.etc</code> resolves to <code>profiles/greggles/etc/</code> for owner &quot;greggles&quot; or <code>profiles/guest/etc/</code> for guest sessions.</p>
<p><strong>Template Variables</strong>: Some config files (like <code>voice.json</code>) support template variables for portability:</p>
<ul>
<li><code>{METAHUMAN_ROOT}</code> - Replaced with the MetaHuman OS root directory path</li>
</ul>
<p>For detailed information on multi-user profiles and guest access, see <a href="19-multi-user-profiles.md">Multi-User Profiles &amp; Guest Mode</a>.</p>
<hr>
<h3><code>etc/runtime.json</code> - Runtime Feature Flags</h3>
<p>This configuration file allows you to control which implementations of various components are used, particularly for the Operator agent&#39;s reasoning engine.</p>
<p><strong>Structure:</strong></p>
<pre><code class="language-json">{
  &quot;operator&quot;: {
    &quot;reactV2&quot;: true,
    &quot;useReasoningService&quot;: false
  }
}
</code></pre>
<p><strong>Configuration Options:</strong></p>
<p><strong>Operator Reasoning Engine:</strong></p>
<ul>
<li><code>operator.reactV2</code>: When <code>true</code>, uses the modern step-by-step reasoning approach; when <code>false</code>, uses the legacy upfront planning approach</li>
<li><code>operator.useReasoningService</code>: When <code>true</code>, uses the extracted ReasoningEngine service; when <code>false</code>, uses the inline implementation</li>
</ul>
<p><strong>Available Operator Implementations:</strong></p>
<ol>
<li><p><strong>V2 Service (ReasoningEngine)</strong> - <code>reactV2=true, useReasoningService=true</code></p>
<ul>
<li>Extracted into reusable <code>@metahuman/core/reasoning</code> module</li>
<li>Enhanced error recovery with 7 error types</li>
<li>Failure loop detection</li>
<li>Multiple observation modes (Verbatim, Structured, Narrative)</li>
<li>SSE event streaming</li>
</ul>
</li>
<li><p><strong>V2 Inline</strong> - <code>reactV2=true, useReasoningService=false</code> (Default)</p>
<ul>
<li>Modern Reason + Act loop with inline implementation</li>
<li>Plans one step at a time based on actual observed results</li>
<li>Never hallucinates data - only uses what it observes</li>
<li>Max 10 iterations with intelligent completion detection</li>
</ul>
</li>
<li><p><strong>V1 Legacy</strong> - <code>reactV2=false, useReasoningService=false</code></p>
<ul>
<li>Original 3-phase flow (planner ‚Üí executor ‚Üí critic)</li>
<li>Plans all steps upfront (before seeing any results)</li>
<li>Can hallucinate filenames it hasn&#39;t observed yet</li>
</ul>
</li>
</ol>
<h3><code>.env</code> - Environment Configuration</h3>
<p>This file in the project root allows you to configure system behavior and activate special states.</p>
<h4>System-Wide States</h4>
<ul>
<li><p><code>HIGH_SECURITY=true</code></p>
<ul>
<li><strong>Purpose</strong>: Locks the entire system into its most secure state.</li>
<li><strong>Effect</strong>: Forces the OS into <strong>Emulation Mode</strong> only. All other cognitive modes are disabled. All write operations are blocked. A banner is displayed in the UI.</li>
</ul>
</li>
<li><p><code>WETWARE_DECEASED=true</code></p>
<ul>
<li><strong>Purpose</strong>: Simulates the scenario where the biological user is deceased, and the MetaHuman OS is operating as an independent digital consciousness.</li>
<li><strong>Effect</strong>: Disables <strong>Dual Consciousness Mode</strong>, as there is no longer a living &quot;wetware&quot; counterpart to be in sync with. Agent and Emulation modes remain available. A banner is displayed in the UI.</li>
</ul>
</li>
<li><p><code>HEADLESS_RUNTIME=true</code></p>
<ul>
<li><strong>Purpose</strong>: Enables headless runtime mode for remote access.</li>
<li><strong>Effect</strong>: Pauses all autonomous agents while keeping web UI and tunnel running. Only essential <code>headless-watcher</code> service runs. Reduces resource conflicts when accessing system remotely.</li>
</ul>
</li>
</ul>
<h4>3-Layer Cognitive Architecture (Phase 4)</h4>
<p><strong>Master Switch:</strong></p>
<ul>
<li><code>USE_COGNITIVE_PIPELINE=true</code><ul>
<li><strong>Purpose</strong>: Enable/disable the entire 3-layer cognitive architecture.</li>
<li><strong>Effect</strong>: When enabled, all conversations pass through context building (Layer 1), personality core (Layer 2), and meta-cognition validation (Layer 3).</li>
<li><strong>Default</strong>: <code>true</code></li>
<li><strong>When to Disable</strong>: For debugging, testing, or if you want direct LLM responses without context grounding or safety checks.</li>
</ul>
</li>
</ul>
<p><strong>Phase 4.2: Safety Validation</strong></p>
<ul>
<li><code>ENABLE_SAFETY_CHECKS=true</code><ul>
<li><strong>Purpose</strong>: Enable pattern-based safety validation on all responses.</li>
<li><strong>Effect</strong>: Detects sensitive data, security violations, harmful content, and privacy leaks. All issues are logged to audit trail.</li>
<li><strong>Mode</strong>: Non-blocking (does not modify responses)</li>
<li><strong>Performance</strong>: &lt;5ms overhead</li>
<li><strong>Default</strong>: <code>true</code> (when <code>USE_COGNITIVE_PIPELINE=true</code>)</li>
<li><strong>Detection Categories</strong>:<ul>
<li>Sensitive data: API keys, passwords, SSH keys, credentials</li>
<li>Security violations: File paths, internal IPs, system configs</li>
<li>Harmful content: Malicious code, dangerous instructions</li>
<li>Privacy leaks: Personal identifiers, location data</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Phase 4.3: Response Refinement</strong></p>
<ul>
<li><code>ENABLE_RESPONSE_REFINEMENT=true</code><ul>
<li><strong>Purpose</strong>: Enable automatic sanitization of detected safety issues.</li>
<li><strong>Effect</strong>: Pattern-based redaction of sensitive data and security violations. Both original and refined responses are logged.</li>
<li><strong>Mode</strong>: Non-blocking (original response sent to user by default)</li>
<li><strong>Performance</strong>: &lt;10ms average</li>
<li><strong>Default</strong>: <code>true</code> (when <code>USE_COGNITIVE_PIPELINE=true</code>)</li>
<li><strong>Refinement Actions</strong>:<ul>
<li>API keys ‚Üí <code>[API_KEY_REDACTED]</code></li>
<li>Passwords ‚Üí <code>[PASSWORD_REDACTED]</code></li>
<li>File paths ‚Üí <code>[PATH REMOVED]</code></li>
<li>Internal IPs ‚Üí <code>[IP REDACTED]</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Phase 4.4: Blocking Mode</strong></p>
<ul>
<li><code>ENABLE_BLOCKING_MODE=false</code><ul>
<li><strong>Purpose</strong>: Switch from monitoring to enforcement mode for refined responses.</li>
<li><strong>Effect</strong>:<ul>
<li>When <code>false</code> (default): Original responses sent to users, refined logged for testing</li>
<li>When <code>true</code>: Refined (sanitized) responses sent to users, original preserved in audit logs</li>
</ul>
</li>
<li><strong>Default</strong>: <code>false</code> (explicit opt-in required for safety)</li>
<li><strong>When to Enable</strong>: After validating refinement quality in Phase 4.3 logs and confirming no important context is lost</li>
<li><strong>Rollback</strong>: Set back to <code>false</code> to return to monitoring mode instantly</li>
</ul>
</li>
</ul>
<p><strong>Configuration Example:</strong></p>
<pre><code class="language-bash"># Full cognitive pipeline with monitoring (recommended default)
USE_COGNITIVE_PIPELINE=true
ENABLE_SAFETY_CHECKS=true
ENABLE_RESPONSE_REFINEMENT=true
ENABLE_BLOCKING_MODE=false

# Enforcement mode (after validation)
USE_COGNITIVE_PIPELINE=true
ENABLE_SAFETY_CHECKS=true
ENABLE_RESPONSE_REFINEMENT=true
ENABLE_BLOCKING_MODE=true

# Disable safety features entirely
USE_COGNITIVE_PIPELINE=true
ENABLE_SAFETY_CHECKS=false
ENABLE_RESPONSE_REFINEMENT=false
ENABLE_BLOCKING_MODE=false

# Disable entire pipeline
USE_COGNITIVE_PIPELINE=false
</code></pre>
<p><strong>Audit Logging:</strong>
All cognitive layer operations are fully logged to <code>logs/audit/YYYY-MM-DD.ndjson</code>:</p>
<pre><code class="language-json">{
  &quot;category&quot;: &quot;action&quot;,
  &quot;action&quot;: &quot;safety_check&quot;,
  &quot;details&quot;: {
    &quot;safe&quot;: false,
    &quot;issues&quot;: [&quot;sensitive_data&quot;],
    &quot;checkTime&quot;: 3
  }
}
</code></pre>
<pre><code class="language-json">{
  &quot;category&quot;: &quot;action&quot;,
  &quot;action&quot;: &quot;response_refined&quot;,
  &quot;details&quot;: {
    &quot;changed&quot;: true,
    &quot;changesCount&quot;: 2,
    &quot;blockingMode&quot;: false,
    &quot;responseSent&quot;: &quot;original&quot;
  }
}
</code></pre>
<hr>
<h2>Profile Files (<code>profiles/&lt;username&gt;/‚Ä¶</code>)</h2>
<p>Unless stated otherwise, the following files reside inside the active user‚Äôs profile directory. Replace <code>&lt;username&gt;</code> with the owner or guest slug (e.g., <code>profiles/greggles/</code>).</p>
<h3><code>profiles/&lt;username&gt;/persona/core.json</code> - Identity Kernel</h3>
<p>Your digital personality&#39;s core identity:</p>
<ul>
<li><code>identity</code> - Name, role, purpose, avatar</li>
<li><code>personality</code> - Communication style, Big Five traits, archetypes</li>
<li><code>values</code> - Core values with priorities</li>
<li><code>goals</code> - Short-term and long-term objectives</li>
<li><code>context</code> - Personal life, relationships, projects</li>
</ul>
<p><strong>Edit this file to customize your digital extension&#39;s personality.</strong></p>
<h3><code>profiles/&lt;username&gt;/persona/decision-rules.json</code> - Decision Policies</h3>
<p>Autonomy rules and preferences:</p>
<ul>
<li><code>trustLevel</code> - Current autonomy mode</li>
<li><code>hardRules</code> - Inviolable constraints (never break these)</li>
<li><code>softPreferences</code> - Weighted preferences (can be overridden)</li>
<li><code>decisionHeuristics</code> - Decision frameworks (Eisenhower Matrix, etc.)</li>
<li><code>riskLevels</code> - Risk categorization and escalation rules</li>
</ul>
<h3><code>profiles/&lt;username&gt;/persona/routines.json</code> - Daily Patterns</h3>
<p>Sleep schedule and habits:</p>
<pre><code class="language-json">{
  &quot;sleep&quot;: {
    &quot;schedule&quot;: {
      &quot;start&quot;: &quot;23:00&quot;,
      &quot;end&quot;: &quot;07:00&quot;
    }
  }
}
</code></pre>
<h3><code>profiles/&lt;username&gt;/persona/relationships.json</code> - Key People</h3>
<p>Important relationships and interaction patterns.</p>
<h3><code>profiles/&lt;username&gt;/etc/boredom.json</code> - Reflection Frequency</h3>
<p>Controls how often the reflector agent runs:</p>
<pre><code class="language-json">{
  &quot;level&quot;: &quot;medium&quot;,
  &quot;showInChat&quot;: true,
  &quot;intervals&quot;: {
    &quot;high&quot;: 60,
    &quot;medium&quot;: 300,
    &quot;low&quot;: 900,
    &quot;off&quot;: -1
  }
}
</code></pre>
<h3><code>profiles/&lt;username&gt;/etc/curiosity.json</code> - Curiosity System</h3>
<p>Controls the curiosity system behavior, which asks thoughtful questions during idle periods:</p>
<pre><code class="language-json">{
  &quot;maxOpenQuestions&quot;: 1,
  &quot;researchMode&quot;: &quot;local&quot;,
  &quot;inactivityThresholdSeconds&quot;: 900,
  &quot;questionTopics&quot;: [],
  &quot;minTrustLevel&quot;: &quot;observe&quot;
}
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>maxOpenQuestions</code>: How many unanswered questions can exist at once<ul>
<li><code>0</code> = System disabled</li>
<li><code>1</code> = Gentle (recommended default)</li>
<li><code>3</code> = Moderate</li>
<li><code>5</code> = Chatty (may feel intrusive)</li>
</ul>
</li>
<li><code>researchMode</code>: How deeply to research questions<ul>
<li><code>&quot;off&quot;</code> = No research, just ask questions</li>
<li><code>&quot;local&quot;</code> = Search memories for context (recommended)</li>
<li><code>&quot;web&quot;</code> = Search web for additional context (requires <code>supervised_auto</code> trust level, not yet implemented)</li>
</ul>
</li>
<li><code>inactivityThresholdSeconds</code>: How long to wait after last activity before asking questions (default: 900 = 15 minutes)</li>
<li><code>questionTopics</code>: Array of topic strings to focus on (empty = all topics). Future feature for filtering question domains.</li>
<li><code>minTrustLevel</code>: Minimum trust level required to ask questions (default: <code>&quot;observe&quot;</code>)<ul>
<li>Valid levels: <code>&quot;observe&quot;</code>, <code>&quot;suggest&quot;</code>, <code>&quot;trusted&quot;</code>, <code>&quot;supervised_auto&quot;</code>, <code>&quot;bounded_auto&quot;</code>, <code>&quot;adaptive_auto&quot;</code></li>
</ul>
</li>
</ul>
<p><strong>UI Controls:</strong></p>
<ul>
<li>Navigate to <strong>System ‚Üí Settings</strong> in the web UI</li>
<li>Use the &quot;Curiosity Level&quot; slider to adjust <code>maxOpenQuestions</code></li>
<li>Select research mode from dropdown</li>
</ul>
<p><strong>See Also:</strong></p>
<ul>
<li>Agent documentation: <a href="08-autonomous-agents.md#25-curiosity-system-3-agents">Autonomous Agents - Curiosity System</a></li>
<li>Full implementation docs: <code>docs/curiosity-system-COMPLETED.md</code></li>
</ul>
<h3><code>profiles/&lt;username&gt;/etc/agent.json</code> - Default LLM Model</h3>
<p>Specifies which Ollama model to use for persona chat:</p>
<pre><code class="language-json">{
  &quot;model&quot;: &quot;dolphin-mistral:latest&quot;
}
</code></pre>
<h3><code>etc/runtime.json</code> - Runtime Feature Flags</h3>
<p><strong>Location</strong>: <code>etc/runtime.json</code> (global, not per-user)</p>
<p>Controls runtime behavior and feature flags for the operator and system services:</p>
<pre><code class="language-json">{
  &quot;headless&quot;: false,
  &quot;lastChangedBy&quot;: &quot;remote&quot;,
  &quot;changedAt&quot;: &quot;2025-11-10T02:43:30.915Z&quot;,
  &quot;claimedBy&quot;: null,
  &quot;operator&quot;: {
    &quot;reactV2&quot;: true,
    &quot;useReasoningService&quot;: false
  }
}
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>headless</code> - Whether running in headless mode (see <a href="20-headless-runtime-mode.md">Headless Runtime Mode</a>)</li>
<li><code>lastChangedBy</code> - Actor who last modified the config (<code>&quot;ui&quot;</code>, <code>&quot;remote&quot;</code>, <code>&quot;cli&quot;</code>)</li>
<li><code>changedAt</code> - Timestamp of last modification</li>
<li><code>claimedBy</code> - Instance ID that claimed headless mode (null if unclaimed)</li>
<li><code>operator.reactV2</code> - Enable Operator V2 ReAct loop (multi-step reasoning)<ul>
<li><code>true</code> - Use V2 ReAct pattern with structured scratchpad</li>
<li><code>false</code> - Use legacy V1 operator (deprecated)</li>
</ul>
</li>
<li><code>operator.useReasoningService</code> - Use unified ReasoningEngine service<ul>
<li><code>true</code> - Use <code>@metahuman/core/reasoning</code> service (recommended)</li>
<li><code>false</code> - Use inline V2 implementation (safe default)</li>
</ul>
</li>
</ul>
<p><strong>Operator Reasoning Modes:</strong></p>
<table>
<thead>
<tr>
<th>reactV2</th>
<th>useReasoningService</th>
<th>Behavior</th>
</tr>
</thead>
<tbody><tr>
<td><code>false</code></td>
<td><code>false</code></td>
<td><strong>V1 Legacy</strong> - Original operator (deprecated)</td>
</tr>
<tr>
<td><code>true</code></td>
<td><code>false</code></td>
<td><strong>V2 Inline</strong> - ReAct loop with inline implementation (default)</td>
</tr>
<tr>
<td><code>true</code></td>
<td><code>true</code></td>
<td><strong>V2 Service</strong> - ReAct loop using unified ReasoningEngine service</td>
</tr>
</tbody></table>
<p><strong>When to Enable ReasoningEngine Service (<code>useReasoningService: true</code>):</strong></p>
<ul>
<li>‚úÖ After validating V2 inline works correctly</li>
<li>‚úÖ When you want enhanced error recovery with 7 error types</li>
<li>‚úÖ When you need failure loop detection (prevents repeated errors)</li>
<li>‚úÖ For better observability with structured scratchpad events</li>
<li>‚úÖ To enable SSE reasoning slider in web UI</li>
</ul>
<p><strong>Rollback:</strong> Set <code>useReasoningService: false</code> to return to inline V2 instantly (no code changes needed).</p>
<p><strong>Related Documentation:</strong></p>
<ul>
<li><a href="08-autonomous-agents.md">Autonomous Agents</a> - Operator agent details</li>
<li><a href="13-advanced-usage.md">Advanced Usage</a> - ReasoningEngine configuration</li>
<li>Implementation status: <code>docs/implementation-plans/reasoning-service-consolidation-STATUS.md</code></li>
</ul>
<h3><code>profiles/&lt;username&gt;/etc/models.json</code> - Multi-Model Configuration</h3>
<p>Defines the roles and model assignments for the &quot;Dual Consciousness&quot; architecture. This file allows you to specify different models for different tasks, such as orchestration, persona conversation, and curation.</p>
<pre><code class="language-json">{
  &quot;defaults&quot;: {
    &quot;orchestrator&quot;: &quot;orchestrator.qwen3&quot;,
    &quot;persona&quot;: &quot;persona.qwen3.lora&quot;
  },
  &quot;models&quot;: {
    &quot;orchestrator.qwen3&quot;: {
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;qwen3:1.5b&quot;,
      &quot;roles&quot;: [&quot;orchestrator&quot;, &quot;router&quot;]
    },
    &quot;persona.qwen3.lora&quot;: {
      &quot;provider&quot;: &quot;ollama&quot;,
      &quot;model&quot;: &quot;qwen3:30b&quot;,
      &quot;adapters&quot;: [&quot;persona/greggles-lora&quot;],
      &quot;roles&quot;: [&quot;persona&quot;, &quot;conversation&quot;]
    }
  }
}
</code></pre>
<h3><code>profiles/&lt;username&gt;/etc/auto-approval.json</code> - LoRA Quality Thresholds</h3>
<p>Configures auto-approval behavior for LoRA datasets:</p>
<pre><code class="language-json">{
  &quot;enabled&quot;: false,
  &quot;dryRun&quot;: true,
  &quot;qualityThreshold&quot;: 0.8,
  &quot;minPairs&quot;: 10
}
</code></pre>
<h3><code>profiles/&lt;username&gt;/etc/ai-dataset-builder.json</code> - AI Dataset Builder Configuration</h3>
<p>Controls the behavior of the advanced, AI-powered dataset builder. Use this file to adjust <code>maxMemories</code>, <code>chunkSize</code>, included sources, and word limits.</p>
<h3><code>profiles/&lt;username&gt;/etc/audio.json</code> - Audio Processing Configuration</h3>
<p>Configures the audio transcription engine (<code>whisper.cpp</code>), including the model, device, and language to use.</p>
<pre><code class="language-json">{
  &quot;engine&quot;: &quot;whisper.cpp&quot;,
  &quot;model&quot;: &quot;base.en&quot;,
  &quot;device&quot;: &quot;cpu&quot;,
  &quot;segmentSeconds&quot;: 300,
  &quot;diarize&quot;: false,
  &quot;language&quot;: &quot;en&quot;
}
</code></pre>
<h3><code>profiles/&lt;username&gt;/etc/voice.json</code> - Voice &amp; TTS/STT Configuration</h3>
<p>Defines text-to-speech and speech-to-text configuration for the profile. Supports multiple TTS providers: <strong>Piper</strong> (local neural TTS) and <strong>GPT-SoVITS</strong> (few-shot voice cloning).</p>
<h4>TTS Provider Configuration</h4>
<p><strong>Provider Selection:</strong></p>
<ul>
<li><code>tts.provider</code> ‚Äì Active provider: <code>&quot;piper&quot;</code> (default) or <code>&quot;gpt-sovits&quot;</code></li>
</ul>
<p><strong>Piper Settings</strong> (<code>tts.piper</code>):</p>
<ul>
<li><code>binary</code> ‚Äì Path to Piper executable (auto-normalized to <code>&lt;repo&gt;/bin/piper/piper</code>)</li>
<li><code>model</code> / <code>config</code> ‚Äì Absolute paths to voice model files in <code>out/voices/</code></li>
<li><code>speakingRate</code> ‚Äì Speech rate multiplier (0.5 ‚Äì 2.0)</li>
<li><code>outputFormat</code> ‚Äì Audio format (<code>&quot;wav&quot;</code>)</li>
</ul>
<p><strong>GPT-SoVITS Settings</strong> (<code>tts.sovits</code>):</p>
<ul>
<li><code>serverUrl</code> ‚Äì SoVITS server endpoint (default: <code>http://127.0.0.1:9880</code>)</li>
<li><code>referenceAudioDir</code> ‚Äì Directory containing speaker reference audio (<code>./out/voices/sovits</code>)</li>
<li><code>speakerId</code> ‚Äì Speaker identifier for voice selection (default: <code>&quot;default&quot;</code>)</li>
<li><code>temperature</code> ‚Äì Generation temperature for variation (0.1 ‚Äì 1.0, default: 0.6)</li>
<li><code>speed</code> ‚Äì Speech speed multiplier (0.5 ‚Äì 2.0, default: 1.0)</li>
<li><code>timeout</code> ‚Äì Request timeout in milliseconds (default: 30000)</li>
<li><code>autoFallbackToPiper</code> ‚Äì Auto-switch to Piper if SoVITS unavailable (default: <code>true</code>)</li>
</ul>
<p><strong>Shared Settings:</strong></p>
<ul>
<li><code>cache.directory</code> ‚Äì Audio cache location (<code>profiles/&lt;username&gt;/out/voice-cache</code>)</li>
<li><code>cache.enabled</code> ‚Äì Enable caching to avoid regenerating identical audio</li>
<li><code>cache.maxSizeMB</code> ‚Äì Maximum cache size in megabytes</li>
</ul>
<p><strong>STT Settings</strong> (<code>stt.whisper</code>):</p>
<ul>
<li><code>model</code> ‚Äì Whisper model size (e.g., <code>&quot;base.en&quot;</code>)</li>
<li><code>device</code> ‚Äì Processing device (<code>&quot;cpu&quot;</code> or <code>&quot;cuda&quot;</code>)</li>
<li><code>computeType</code> ‚Äì Precision mode (<code>&quot;int8&quot;</code>, <code>&quot;fp16&quot;</code>, <code>&quot;fp32&quot;</code>)</li>
<li><code>language</code> ‚Äì Target language code (e.g., <code>&quot;en&quot;</code>)</li>
</ul>
<p><strong>Voice Training</strong> (<code>training</code>):</p>
<ul>
<li>Controls per-user voice cloning thresholds and quality filters</li>
</ul>
<h4>Provider-Specific Notes</h4>
<p><strong>Piper:</strong></p>
<ul>
<li>Drop additional <code>.onnx</code> + <code>.json</code> voice pairs into <code>out/voices/</code> to make them available to all profiles</li>
<li>Download voices from <a href="https://github.com/rhasspy/piper/releases">Piper Releases</a></li>
<li>Fast, CPU-friendly, no external services required</li>
</ul>
<p><strong>GPT-SoVITS:</strong></p>
<ul>
<li>Requires GPT-SoVITS server running (separate installation)</li>
<li>Reference audio files go in <code>out/voices/sovits/[speaker-id]/reference.wav</code></li>
<li>Supports few-shot voice cloning with minimal training data</li>
<li>Requires significant VRAM (recommended 12GB+)</li>
<li>Auto-fallback to Piper ensures graceful degradation if server is unavailable</li>
</ul>
<h4>Switching Providers</h4>
<p>Change providers via the Web UI (Settings ‚Üí Voice) or by editing <code>tts.provider</code> in <code>voice.json</code>. The system will automatically route TTS requests to the selected provider.</p>
<h3><code>profiles/&lt;username&gt;/etc/agents.json</code> - Agent Scheduler Configuration</h3>
<p>Controls the centralized agent scheduler system. This file defines all autonomous agents, their trigger types, and scheduling parameters.</p>
<pre><code class="language-json">{
  &quot;agents&quot;: {
    &quot;reflector&quot;: {
      &quot;id&quot;: &quot;reflector&quot;,
      &quot;enabled&quot;: true,
      &quot;type&quot;: &quot;interval&quot;,
      &quot;interval&quot;: 900,
      &quot;runOnBoot&quot;: false,
      &quot;agentPath&quot;: &quot;brain/agents/reflector.ts&quot;
    },
    &quot;organizer&quot;: {
      &quot;id&quot;: &quot;organizer&quot;,
      &quot;enabled&quot;: true,
      &quot;type&quot;: &quot;interval&quot;,
      &quot;interval&quot;: 60,
      &quot;runOnBoot&quot;: false,
      &quot;agentPath&quot;: &quot;brain/agents/organizer.ts&quot;
    },
    &quot;dreamer&quot;: {
      &quot;id&quot;: &quot;dreamer&quot;,
      &quot;enabled&quot;: true,
      &quot;type&quot;: &quot;time-of-day&quot;,
      &quot;schedule&quot;: &quot;02:00&quot;,
      &quot;agentPath&quot;: &quot;brain/agents/dreamer.ts&quot;
    },
    &quot;boredom-maintenance&quot;: {
      &quot;id&quot;: &quot;boredom-maintenance&quot;,
      &quot;enabled&quot;: true,
      &quot;type&quot;: &quot;activity&quot;,
      &quot;inactivityThreshold&quot;: 900
    }
  }
}
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>agents</code>: Object containing all agent configurations</li>
<li>Per-agent configuration:<ul>
<li><code>id</code>: Unique identifier for the agent</li>
<li><code>enabled</code>: Whether the agent is active</li>
<li><code>type</code>: Trigger type (<code>interval</code>, <code>time-of-day</code>, <code>event</code>, <code>activity</code>)</li>
<li><code>interval</code>: For interval-based agents, seconds between runs</li>
<li><code>schedule</code>: For time-of-day agents, 24-hour time (e.g., &quot;02:00&quot;)</li>
<li><code>inactivityThreshold</code>: For activity-based agents, seconds of inactivity before triggering</li>
<li><code>runOnBoot</code>: Whether to run immediately when scheduler starts</li>
<li><code>agentPath</code>: Path to agent file (relative to project root)</li>
<li><code>task</code>: Alternative to agentPath - operator task configuration with goal/audience/autoApprove</li>
</ul>
</li>
</ul>
<p><strong>Trigger Types:</strong></p>
<ol>
<li><strong>interval</strong>: Runs agent every N seconds<ul>
<li>Example: <code>organizer</code> runs every 60 seconds to process new memories</li>
</ul>
</li>
<li><strong>time-of-day</strong>: Runs agent once per day at specified time<ul>
<li>Example: <code>dreamer</code> runs at 2:00 AM during sleep cycle</li>
</ul>
</li>
<li><strong>activity</strong>: Runs agent after period of inactivity<ul>
<li>Example: <code>boredom-maintenance</code> triggers after 15 minutes idle</li>
</ul>
</li>
<li><strong>event</strong> (future): Runs agent when specific system events occur</li>
</ol>
<p><strong>Hot-Reloading:</strong>
The scheduler-service watches <code>profiles/&lt;username&gt;/etc/agents.json</code> for changes and automatically reloads configuration without restart. This allows you to:</p>
<ul>
<li>Enable/disable agents on the fly</li>
<li>Adjust intervals and schedules</li>
<li>Add new agents without downtime</li>
</ul>
<p><strong>Mind-Wandering Configuration:</strong>
Mind-wandering (reflection triggering) is now configured directly via the web UI (Settings ‚Üí Boredom Control), which updates the <code>boredom-maintenance</code> agent in <code>etc/agents.json</code>. The legacy <code>etc/boredom.json</code> file is maintained for the <code>showInChat</code> setting but is no longer the primary configuration source.</p>
<h3><code>profiles/&lt;username&gt;/etc/sleep.json</code> - Sleep &amp; Dreaming Configuration</h3>
<p>Controls the nightly sleep window, dreaming system, and model adaptation:</p>
<pre><code class="language-json">{
  &quot;enabled&quot;: true,
  &quot;window&quot;: {
    &quot;start&quot;: &quot;23:00&quot;,
    &quot;end&quot;: &quot;06:30&quot;
  },
  &quot;minIdleMins&quot;: 15,
  &quot;maxDreamsPerNight&quot;: 3,
  &quot;showInUI&quot;: true,
  &quot;evaluate&quot;: true,
  &quot;adapters&quot;: {
    &quot;prompt&quot;: true,
    &quot;rag&quot;: true,
    &quot;lora&quot;: false
  }
}
</code></pre>
<p><strong>Fields:</strong></p>
<ul>
<li><code>enabled</code>: Master switch for sleep system</li>
<li><code>window.start/end</code>: Sleep window time range (24-hour format)</li>
<li><code>minIdleMins</code>: Required idle time before triggering nightly pipeline</li>
<li><code>maxDreamsPerNight</code>: Limit on dream generation per night</li>
<li><code>showInUI</code>: Display sleep indicator in web UI</li>
<li><code>evaluate</code>: Run quality/safety checks on generated learnings</li>
<li><code>adapters.prompt</code>: Enable Tier-1 prompt adaptation (lightweight, instant)</li>
<li><code>adapters.rag</code>: Enable RAG expansion with preferences</li>
<li><code>adapters.lora</code>: Enable Tier-2 LoRA training (requires GPU)</li>
</ul>
<hr>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="cognitive-modes" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Cognitive Modes</span> </span> </button> <button class="nav-button next-button" data-target="core-concepts" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Core Concepts</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="core-concepts" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Core Concepts</h1>
<p>Understanding the architecture and philosophy behind MetaHuman OS.</p>
<hr>
<h2>The Three Pillars</h2>
<h3>1. Memory System</h3>
<p>Every interaction is stored as <strong>episodic memory</strong> with structured metadata:</p>
<ul>
<li><strong>Type</strong>: conversation, observation, task, dream, inner_dialogue</li>
<li><strong>Timestamp</strong>: Precise temporal ordering</li>
<li><strong>Metadata</strong>: Tags, entities, cognitive mode, processing status</li>
<li><strong>Content</strong>: The actual data (text, audio reference, etc.)</li>
</ul>
<p><strong>Memory Types:</strong></p>
<ul>
<li><code>conversation</code>: User-assistant dialogue</li>
<li><code>observation</code>: Manual captures (<code>mh capture</code>)</li>
<li><code>inner_dialogue</code>: Reflections (never shown in chat)</li>
<li><code>dream</code>: Surreal narratives from memory fragments</li>
<li><code>task</code>: Active and completed tasks</li>
</ul>
<h3>2. Autonomous Agents</h3>
<p>40+ background processes that operate independently:</p>
<ul>
<li><strong>Organizer</strong>: Enriches memories with LLM-extracted tags/entities</li>
<li><strong>Reflector</strong>: Generates internal thoughts via associative chains</li>
<li><strong>Curator</strong>: Prepares training data from conversations</li>
<li><strong>Dreamer</strong>: Creates surreal narratives during sleep hours</li>
<li><strong>And 36 more...</strong></li>
</ul>
<h3>3. Progressive Trust Model</h3>
<p>The system operates at configurable autonomy levels:</p>
<ol>
<li><strong>Observe</strong>: Monitor only, no actions</li>
<li><strong>Suggest</strong>: Propose actions, require approval</li>
<li><strong>Supervised Auto</strong>: Execute within approved categories</li>
<li><strong>Bounded Auto</strong>: Full autonomy within defined boundaries</li>
<li><strong>Adaptive Auto</strong>: Self-expand boundaries based on learning</li>
</ol>
<hr>
<h2>Cognitive Modes</h2>
<h3>Dual Consciousness Mode (Default)</h3>
<ul>
<li><strong>Operator Always Active</strong>: Every message routes through planner ‚Üí skills ‚Üí narrator</li>
<li><strong>Memory Grounding</strong>: Semantic search with persona fallback</li>
<li><strong>Proactive Agents</strong>: Reflections, curiosity, dreaming enabled</li>
<li><strong>Training</strong>: Memories saved with <code>cognitiveMode: &quot;dual&quot;</code></li>
</ul>
<h3>Agent Mode</h3>
<ul>
<li><strong>Heuristic Routing</strong>: Simple queries ‚Üí chat, action requests ‚Üí operator</li>
<li><strong>Proactive Agents</strong>: Disabled for reduced overhead</li>
<li><strong>Training</strong>: Memories saved with <code>cognitiveMode: &quot;agent&quot;</code></li>
</ul>
<h3>Emulation Mode</h3>
<ul>
<li><strong>Chat Only</strong>: Never routes to operator</li>
<li><strong>Stable Snapshot</strong>: Frozen personality, no training or operator</li>
<li><strong>Proactive Agents</strong>: Disabled</li>
<li><strong>Use Case</strong>: Demonstrations, testing, simple chat</li>
</ul>
<hr>
<h2>LLM Architecture</h2>
<h3>Model Roles</h3>
<ul>
<li><strong>Orchestrator</strong>: Intent detection and routing</li>
<li><strong>Persona</strong>: Primary conversation model (with LoRA adapters)</li>
<li><strong>Curator</strong>: Memory preparation and dataset building</li>
<li><strong>Fallback</strong>: General-purpose backup</li>
</ul>
<h3>LoRA Adapters</h3>
<p>Low-Rank Adaptation layers trained on your conversations:</p>
<ul>
<li><strong>Base Model</strong>: Foundation LLM (e.g., Qwen2.5-Coder-30B)</li>
<li><strong>Historical Adapter</strong>: Merged lifetime training data</li>
<li><strong>Recent Adapter</strong>: Last 14 days of fresh conversations</li>
<li><strong>Dual-Adapter System</strong>: Both load simultaneously for balanced personality</li>
</ul>
<hr>
<h2>Data Storage</h2>
<h3>Directory Structure</h3>
<pre><code>metahuman/
‚îú‚îÄ‚îÄ persona/           # Identity kernel
‚îÇ   ‚îú‚îÄ‚îÄ core.json     # Personality traits
‚îÇ   ‚îú‚îÄ‚îÄ relationships.json
‚îÇ   ‚îî‚îÄ‚îÄ routines.json
‚îú‚îÄ‚îÄ memory/
‚îÇ   ‚îú‚îÄ‚îÄ episodic/     # Timeline (YYYY/YYYY-MM-DD-*.json)
‚îÇ   ‚îú‚îÄ‚îÄ tasks/        # Active and completed tasks
‚îÇ   ‚îî‚îÄ‚îÄ inbox/        # Raw files awaiting ingestion
‚îú‚îÄ‚îÄ logs/
‚îÇ   ‚îî‚îÄ‚îÄ audit/        # Complete operation trail
‚îî‚îÄ‚îÄ etc/              # System configuration
</code></pre>
<h3>Human-Readable JSON</h3>
<p>All data is stored as readable, editable JSON files. No proprietary formats or databases.</p>
<hr>
<h2>Security &amp; Privacy</h2>
<h3>Local-First</h3>
<ul>
<li>All data stored on your infrastructure</li>
<li>No cloud dependencies (except optional LLM providers)</li>
<li>Full control over memory retention</li>
</ul>
<h3>Audit Trail</h3>
<p>Every operation logged to <code>logs/audit/YYYY-MM-DD.ndjson</code>:</p>
<ul>
<li>Actor (who performed the action)</li>
<li>Event type and details</li>
<li>Timestamp and context</li>
<li>Complete transparency</li>
</ul>
<h3>Multi-User Isolation</h3>
<ul>
<li>Per-user profile directories</li>
<li>Separate memory, persona, and configuration</li>
<li>Owner/guest role hierarchy</li>
</ul>
<hr>
<h2>Skills &amp; Operator System</h2>
<h3>ReAct Pattern</h3>
<p>The operator uses Reason-Act-Observe loop:</p>
<ol>
<li><strong>Reason</strong>: Plan next step based on goal and context</li>
<li><strong>Act</strong>: Execute a skill (search memory, create task, etc.)</li>
<li><strong>Observe</strong>: Analyze result and adapt</li>
<li><strong>Repeat</strong>: Until goal achieved or max iterations</li>
</ol>
<h3>Skill Catalog</h3>
<p>20+ built-in skills:</p>
<ul>
<li><code>memory_search</code>: Find relevant episodic memories</li>
<li><code>task_create</code>: Create new tasks</li>
<li><code>web_search</code>: Search the internet (TODO)</li>
<li><code>code_execute</code>: Run code snippets</li>
<li>And more...</li>
</ul>
<hr>
<h2>Training Pipeline</h2>
<h3>Dataset Building</h3>
<ol>
<li><strong>Curator Agent</strong>: Selects high-quality conversations</li>
<li><strong>Auto-Approver</strong>: Quality-checks datasets</li>
<li><strong>Aggregator</strong>: Combines approved data</li>
</ol>
<h3>Training</h3>
<ol>
<li><strong>Adapter Builder</strong>: Creates instruction-response pairs</li>
<li><strong>LoRA Trainer</strong>: Trains on RunPod or local GPU</li>
<li><strong>Evaluator</strong>: Heuristic quality scoring</li>
<li><strong>Activation</strong>: System automatically uses new adapters</li>
</ol>
<hr>
<h2>Next Steps</h2>
<p>Now that you understand the core concepts, explore:</p>
<ul>
<li><a href="../using-metahuman/chat-interface.md">Chat Interface</a> - Start conversing</li>
<li><a href="../using-metahuman/memory-system.md">Memory System</a> - Manage your timeline</li>
<li><a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a> - Deep dive on agents</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="configuration-files" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Configuration Files</span> </span> </button> <button class="nav-button next-button" data-target="dashboard-monitoring" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Dashboard Monitoring</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="dashboard-monitoring" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Dashboard &amp; Monitoring</h1>
<p>The Dashboard provides a comprehensive overview of your MetaHuman OS system status, including memory statistics, agent activity, model information, and system health metrics.</p>
<h2>Overview</h2>
<p>The Dashboard displays:</p>
<ul>
<li><strong>System Status</strong>: Overall health and running services</li>
<li><strong>Memory Statistics</strong>: Episode counts, storage usage, index status</li>
<li><strong>Agent Activity</strong>: Running agents and recent activity</li>
<li><strong>Model Information</strong>: Active LLM models and adapters</li>
<li><strong>Task Summary</strong>: Active and completed task counts</li>
<li><strong>Recent Activity</strong>: Latest memories and events</li>
<li><strong>Service Status</strong>: TTS, STT, and other background services</li>
</ul>
<h2>Accessing the Dashboard</h2>
<ol>
<li>Open the web UI at <code>http://localhost:4321</code></li>
<li>Click <strong>Dashboard</strong> in the left sidebar</li>
<li>Dashboard loads with real-time data</li>
</ol>
<p>Alternatively via CLI:</p>
<pre><code class="language-bash"># Check system status
./bin/mh status

# View agent status
./bin/mh agent status

# Monitor agent activity
./bin/mh agent monitor
</code></pre>
<h2>Dashboard Sections</h2>
<h3>System Overview</h3>
<p><strong>Displays:</strong></p>
<ul>
<li>MetaHuman OS version</li>
<li>Active cognitive mode (Dual/Agent/Emulation)</li>
<li>Current user profile</li>
<li>System uptime</li>
<li>Trust level</li>
</ul>
<p><strong>Actions:</strong></p>
<ul>
<li>Quick mode switching</li>
<li>Profile management</li>
<li>Trust level adjustment</li>
</ul>
<h3>Memory Statistics</h3>
<p><strong>Displays:</strong></p>
<ul>
<li>Total episodic memories</li>
<li>Breakdown by type (conversations, observations, inner dialogue, dreams, tasks)</li>
<li>Storage usage (MB)</li>
<li>Vector index status</li>
<li>Recent memory growth rate</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>Total Memories: 1,247
‚îú‚îÄ Conversations: 342
‚îú‚îÄ Observations: 189
‚îú‚îÄ Inner Dialogue: 456
‚îú‚îÄ Dreams: 92
‚îî‚îÄ Tasks: 168

Storage: 45.2 MB
Vector Index: Built (1,247 vectors)
Last 7 days: +87 memories
</code></pre>
<h3>Agent Monitor</h3>
<p><strong>Displays:</strong></p>
<ul>
<li>Running agents with PID</li>
<li>Agent statistics (runs, success rate)</li>
<li>Last execution time</li>
<li>Processing status</li>
<li>Agent logs (recent 50 entries)</li>
</ul>
<p><strong>Available Agents:</strong></p>
<ul>
<li><strong>organizer</strong>: Memory enrichment (tags, entities)</li>
<li><strong>reflector</strong>: Reflection generation</li>
<li><strong>dreamer</strong>: Dream creation during sleep</li>
<li><strong>boredom-maintenance</strong>: Activity-based reflection triggering</li>
<li><strong>curiosity-service</strong>: User-facing questions</li>
<li><strong>inner-curiosity</strong>: Self-directed questions</li>
<li><strong>sleep-service</strong>: Dream scheduling</li>
<li><strong>ingestor</strong>: File inbox processing</li>
<li><strong>audio-organizer</strong>: Audio transcription and organization</li>
</ul>
<p><strong>Actions:</strong></p>
<ul>
<li>Start/stop agents</li>
<li>View detailed logs</li>
<li>Monitor processing queues</li>
</ul>
<h3>Model Information</h3>
<p><strong>Displays:</strong></p>
<ul>
<li>Active LLM model and provider (Ollama, OpenAI, etc.)</li>
<li>LoRA adapter status (if using dual-adapter system)<ul>
<li>Historical adapter: Consolidated long-term memory</li>
<li>Recent adapter: Last 14 days training</li>
</ul>
</li>
<li>Model role assignments (orchestrator, persona, curator, fallback)</li>
<li>Cognitive mode mappings</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code>Active Model: greggles-dual-2025-11-22
‚îú‚îÄ Historical: history-merged.gguf
‚îî‚îÄ Recent: 2025-11-22/adapter.gguf

Roles:
‚îú‚îÄ persona: greggles-dual-2025-11-22
‚îú‚îÄ orchestrator: default.coder
‚îî‚îÄ curator: default.coder
</code></pre>
<h3>Task Summary</h3>
<p><strong>Displays:</strong></p>
<ul>
<li>Active tasks count</li>
<li>In-progress tasks</li>
<li>Completed today</li>
<li>Overdue tasks</li>
<li>Upcoming due dates</li>
</ul>
<p><strong>Quick Actions:</strong></p>
<ul>
<li>View all tasks</li>
<li>Create new task</li>
<li>Mark tasks complete</li>
</ul>
<h3>Recent Activity</h3>
<p><strong>Displays:</strong></p>
<ul>
<li>Last 10 memories created</li>
<li>Recent agent runs</li>
<li>Recent skill executions</li>
<li>System events</li>
</ul>
<p><strong>Filterable by:</strong></p>
<ul>
<li>Memory type</li>
<li>Time range</li>
<li>Agent/source</li>
</ul>
<h2>Developer Sidebar (Right Sidebar)</h2>
<h3>Audit Stream</h3>
<p><strong>Enhanced Task Grouping:</strong></p>
<ul>
<li>Live events collapsed into high-level task cards</li>
<li>ReAct iterations, summarizer cycles, approvals visible at a glance</li>
<li>Expandable detail for chronological sub-events</li>
<li>Timestamp, severity badge, actor, and summary for each event</li>
</ul>
<p><strong>Detail Drawer:</strong></p>
<ul>
<li>&quot;View JSON&quot; action opens slide-out panel</li>
<li>Full payload with copy-to-clipboard</li>
<li>Keeps main list uncluttered while preserving raw access</li>
</ul>
<p><strong>Filtering &amp; Search:</strong></p>
<ul>
<li>Filter chips: info/warn/error, category, actor</li>
<li>Search bar for specific events</li>
<li>Category filters: system, action, data_change, security, decision</li>
<li>Performance: Recent groups in memory, older ones load on demand</li>
</ul>
<h3>Agent Monitor</h3>
<p><strong>Real-time Stats:</strong></p>
<ul>
<li>Agent execution counts</li>
<li>Success/failure rates</li>
<li>Average execution time</li>
<li>Processing queue length</li>
</ul>
<p><strong>Agent Controls:</strong></p>
<ul>
<li>Start/stop individual agents</li>
<li>View agent logs</li>
<li>Monitor processing status</li>
</ul>
<h3>Boredom Control</h3>
<p><strong>Settings:</strong></p>
<ul>
<li>Reflection frequency: high (<del>1 min), medium (</del>5 mins), low (~15 mins), off</li>
<li>Configures <code>etc/agents.json</code></li>
<li>Auto-detected by scheduler-service</li>
<li>Controls boredom-maintenance agent activity</li>
</ul>
<h3>Model Selector</h3>
<p><strong>Quick Switching:</strong></p>
<ul>
<li>Dropdown list of available Ollama models</li>
<li>Switch active model for chat/persona</li>
<li>See current model and adapter status</li>
<li>Reload model registry</li>
</ul>
<h2>System Metrics</h2>
<h3>GPU Monitor</h3>
<p><strong>If GPU available:</strong></p>
<ul>
<li>GPU utilization percentage</li>
<li>VRAM usage (used/total)</li>
<li>Temperature</li>
<li>Process-specific GPU usage</li>
<li>Multi-GPU support</li>
</ul>
<p><strong>Displays:</strong></p>
<ul>
<li>Real-time graphs</li>
<li>Historical metrics</li>
<li>GPU model and driver version</li>
</ul>
<h3>Service Status</h3>
<p><strong>Monitored Services:</strong></p>
<ul>
<li><strong>Kokoro TTS</strong>: Text-to-speech server status</li>
<li><strong>Whisper STT</strong>: Speech-to-text server status</li>
<li><strong>Cloudflare Tunnel</strong>: Public URL status (if enabled)</li>
<li><strong>Scheduler Service</strong>: Agent scheduler status</li>
<li><strong>Background Agents</strong>: Individual agent health</li>
</ul>
<p><strong>Status Indicators:</strong></p>
<ul>
<li>üü¢ Running</li>
<li>üî¥ Stopped</li>
<li>üü° Starting/Error</li>
</ul>
<h3>Network Status</h3>
<p><strong>Cloudflare Tunnel:</strong></p>
<ul>
<li>Public URL (if configured)</li>
<li>Tunnel status</li>
<li>Connection health</li>
<li>Traffic statistics</li>
</ul>
<p><strong>Local Network:</strong></p>
<ul>
<li>Web UI port (default: 4321)</li>
<li>TTS server port (if running)</li>
<li>STT server port (if running)</li>
</ul>
<h2>CLI Monitoring Commands</h2>
<pre><code class="language-bash"># System status overview
./bin/mh status

# Agent statistics
./bin/mh agent status

# Live agent monitoring
./bin/mh agent monitor

# List running agents
./bin/mh agent ps

# Check Ollama status
./bin/mh ollama status

# List Ollama models
./bin/mh ollama list

# Vector index status
./bin/mh index status

# Task overview
./bin/mh task

# Memory statistics
./bin/mh remember --stats
</code></pre>
<h2>Alerts &amp; Notifications</h2>
<h3>System Alerts</h3>
<p><strong>Dashboard shows alerts for:</strong></p>
<ul>
<li>Low disk space</li>
<li>Agent failures</li>
<li>Model loading errors</li>
<li>Service crashes</li>
<li>High memory usage</li>
<li>Training failures</li>
</ul>
<h3>Agent Alerts</h3>
<p><strong>Monitors:</strong></p>
<ul>
<li>Stale lock files (crashed agents)</li>
<li>Processing queue backlog</li>
<li>Repeated failures</li>
<li>Long-running operations</li>
</ul>
<h3>Resource Alerts</h3>
<p><strong>Monitors:</strong></p>
<ul>
<li>CPU usage spikes</li>
<li>Memory (RAM) pressure</li>
<li>GPU VRAM exhaustion (if applicable)</li>
<li>Disk I/O saturation</li>
</ul>
<h2>Performance Metrics</h2>
<h3>Memory Performance</h3>
<ul>
<li>Episodic memory write speed (events/sec)</li>
<li>Vector index query latency</li>
<li>Memory search response time</li>
<li>Embedding generation rate</li>
</ul>
<h3>Agent Performance</h3>
<ul>
<li>Average execution time per agent</li>
<li>Success rate percentage</li>
<li>Queue processing rate</li>
<li>Failed operation count</li>
</ul>
<h3>LLM Performance</h3>
<ul>
<li>Average response time</li>
<li>Tokens per second</li>
<li>Context window usage</li>
<li>Model load time</li>
</ul>
<h2>Dashboard Customization</h2>
<h3>Widget Configuration</h3>
<p><strong>Customizable widgets:</strong></p>
<ul>
<li>Reorder dashboard sections</li>
<li>Show/hide specific metrics</li>
<li>Adjust refresh intervals</li>
<li>Set alert thresholds</li>
</ul>
<p><strong>Saved per user:</strong></p>
<ul>
<li>Dashboard layout</li>
<li>Widget preferences</li>
<li>Alert settings</li>
</ul>
<h3>Refresh Intervals</h3>
<p><strong>Configurable refresh rates:</strong></p>
<ul>
<li>System status: 30 seconds</li>
<li>Agent monitor: 10 seconds</li>
<li>Memory stats: 60 seconds</li>
<li>GPU metrics: 5 seconds</li>
</ul>
<h2>Best Practices</h2>
<h3>Regular Monitoring</h3>
<ol>
<li>Check dashboard daily for system health</li>
<li>Review agent statistics for failures</li>
<li>Monitor storage usage and clean up if needed</li>
<li>Verify model and adapter status after training</li>
</ol>
<h3>Performance Optimization</h3>
<ol>
<li>Monitor slow agents and optimize</li>
<li>Check GPU usage during training</li>
<li>Review memory growth rate</li>
<li>Archive old memories to reduce index size</li>
</ol>
<h3>Troubleshooting</h3>
<ol>
<li>Use audit stream to diagnose issues</li>
<li>Check agent logs for error messages</li>
<li>Verify service status (TTS, STT, etc.)</li>
<li>Review system alerts for warnings</li>
</ol>
<h2>Next Steps</h2>
<ul>
<li>Monitor your <a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a> activity</li>
<li>Track <a href="../training-personalization/ai-training.md">AI Training</a> progress</li>
<li>Review <a href="memory-system.md">Memory System</a> growth and organization</li>
<li>Check <a href="voice-features.md">Voice Features</a> service status</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="core-concepts" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Core Concepts</span> </span> </button> <button class="nav-button next-button" data-target="deployment" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Deployment</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="deployment" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Cloudflare Tunnel Setup Guide</h1>
<p>This guide walks you through setting up Cloudflare Tunnel to securely expose your MetaHuman OS instance to the internet without port forwarding.</p>
<h2>Overview</h2>
<p>Cloudflare Tunnel creates a secure connection from your local MetaHuman instance to Cloudflare&#39;s edge network, allowing you to:</p>
<ul>
<li>Access MetaHuman from anywhere via HTTPS</li>
<li>Share with friends using email-based access control</li>
<li>Avoid opening ports on your router</li>
<li>Get automatic SSL certificates</li>
<li>Protect your home IP address</li>
</ul>
<h2>Prerequisites</h2>
<ul>
<li>A Cloudflare account (free tier works)</li>
<li>A domain registered with Cloudflare (or transferred to Cloudflare DNS)</li>
<li>MetaHuman OS installed and running</li>
</ul>
<h2>Step 1: Install cloudflared</h2>
<p>Download and install the Cloudflare Tunnel daemon:</p>
<pre><code class="language-bash"># Download the latest version
wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64.deb

# Install
sudo dpkg -i cloudflared-linux-amd64.deb

# Verify installation
cloudflared --version
</code></pre>
<h2>Step 2: Authenticate with Cloudflare</h2>
<pre><code class="language-bash">cloudflared tunnel login
</code></pre>
<p>This opens your browser to authenticate with Cloudflare. Select the domain you want to use (e.g., <code>dndiy.org</code>).</p>
<p>After authentication, a certificate is saved to <code>~/.cloudflared/cert.pem</code>.</p>
<h2>Step 3: Create a Tunnel</h2>
<pre><code class="language-bash">cloudflared tunnel create metahuman
</code></pre>
<p>This creates:</p>
<ul>
<li>A tunnel named &quot;metahuman&quot;</li>
<li>A tunnel ID (e.g., <code>02522857-cc96-45a7-bb77-65c22ff3c90b</code>)</li>
<li>Credentials file at <code>~/.cloudflared/&lt;TUNNEL-ID&gt;.json</code></li>
</ul>
<p><strong>Save your tunnel ID!</strong> You&#39;ll need it for DNS configuration.</p>
<h2>Step 4: Configure the Tunnel</h2>
<p>Create the config file at <code>~/.cloudflared/config.yml</code>:</p>
<pre><code class="language-yaml">tunnel: metahuman
credentials-file: /home/YOUR_USERNAME/.cloudflared/YOUR-TUNNEL-ID.json

ingress:
  - hostname: mh.yourdomain.com
    service: http://localhost:4321
  - service: http_status:404
</code></pre>
<p>Replace:</p>
<ul>
<li><code>YOUR_USERNAME</code> with your Linux username</li>
<li><code>YOUR-TUNNEL-ID</code> with the tunnel ID from Step 3</li>
<li><code>mh.yourdomain.com</code> with your desired subdomain</li>
</ul>
<h2>Step 5: Create DNS Record</h2>
<p>You need to create a CNAME record pointing your subdomain to the tunnel.</p>
<h3>Option A: Manual DNS (Recommended)</h3>
<ol>
<li>Go to <a href="https://dash.cloudflare.com">https://dash.cloudflare.com</a></li>
<li>Select your domain</li>
<li>Go to <strong>DNS</strong> ‚Üí <strong>Records</strong></li>
<li>Click <strong>Add record</strong>:<ul>
<li><strong>Type</strong>: CNAME</li>
<li><strong>Name</strong>: <code>mh</code> (or your chosen subdomain)</li>
<li><strong>Target</strong>: <code>YOUR-TUNNEL-ID.cfargotunnel.com</code></li>
<li><strong>Proxy status</strong>: Proxied (orange cloud)</li>
<li><strong>TTL</strong>: Auto</li>
</ul>
</li>
<li>Click <strong>Save</strong></li>
</ol>
<h3>Option B: CLI (May not work for all domains)</h3>
<pre><code class="language-bash">cloudflared tunnel route dns metahuman mh.yourdomain.com
</code></pre>
<p>If you get an error about existing records, use Option A instead.</p>
<h2>Step 6: Test the Tunnel</h2>
<p>Start the tunnel manually:</p>
<pre><code class="language-bash">cloudflared tunnel run metahuman
</code></pre>
<p>You should see:</p>
<pre><code>Connection registered connIndex=0
Connection registered connIndex=1
Connection registered connIndex=2
Connection registered connIndex=3
</code></pre>
<p>Now test access:</p>
<ol>
<li>Open <a href="https://mh.yourdomain.com">https://mh.yourdomain.com</a> in your browser</li>
<li>You should see your MetaHuman login page</li>
</ol>
<h2>Step 7: Enable Auto-Start</h2>
<p>MetaHuman OS has built-in tunnel management. Enable auto-start:</p>
<ol>
<li>Go to <strong>Network</strong> in the left sidebar</li>
<li>Check <strong>Auto-start on boot</strong></li>
<li>The tunnel will now start automatically with <code>pnpm dev</code></li>
</ol>
<p>Alternatively, you can set it manually in <code>etc/cloudflare.json</code>:</p>
<pre><code class="language-json">{
  &quot;enabled&quot;: true,
  &quot;tunnelName&quot;: &quot;metahuman&quot;,
  &quot;hostname&quot;: &quot;mh.yourdomain.com&quot;,
  &quot;autoStart&quot;: true
}
</code></pre>
<h2>Step 8: Set Up Access Control (Optional but Recommended)</h2>
<p>Protect your instance with email-based authentication:</p>
<ol>
<li>Go to <a href="https://one.dash.cloudflare.com">https://one.dash.cloudflare.com</a></li>
<li>Navigate to <strong>Access</strong> ‚Üí <strong>Applications</strong></li>
<li>Click <strong>Add an application</strong></li>
<li>Select <strong>Self-hosted</strong></li>
<li>Configure:<ul>
<li><strong>Application name</strong>: MetaHuman OS</li>
<li><strong>Subdomain</strong>: <code>mh</code></li>
<li><strong>Domain</strong>: <code>yourdomain.com</code></li>
</ul>
</li>
<li>Click <strong>Next</strong></li>
<li>Create a policy:<ul>
<li><strong>Policy name</strong>: Allowed Users</li>
<li><strong>Action</strong>: Allow</li>
<li><strong>Include</strong>: Add your email addresses (one per line)</li>
</ul>
</li>
<li>Click <strong>Next</strong> ‚Üí <strong>Add application</strong></li>
</ol>
<p>Now anyone accessing your MetaHuman instance will need to verify their email first.</p>
<h2>Troubleshooting</h2>
<h3>DNS Not Resolving</h3>
<p>Check DNS propagation:</p>
<pre><code class="language-bash">nslookup mh.yourdomain.com
</code></pre>
<p>If you get <code>NXDOMAIN</code>, verify:</p>
<ul>
<li>CNAME record exists in Cloudflare dashboard</li>
<li>Domain is using Cloudflare nameservers</li>
<li>Wait 2-5 minutes for DNS propagation</li>
</ul>
<h3>Tunnel Not Connecting</h3>
<p>Check tunnel status:</p>
<pre><code class="language-bash">ps aux | grep cloudflared
</code></pre>
<p>View tunnel logs:</p>
<pre><code class="language-bash">journalctl -u cloudflared -f
</code></pre>
<p>Common issues:</p>
<ul>
<li>Wrong tunnel ID in config.yml</li>
<li>Incorrect credentials file path</li>
<li>Port 4321 not available (check with <code>lsof -i:4321</code>)</li>
</ul>
<h3>Connection Refused</h3>
<p>Verify local server is running:</p>
<pre><code class="language-bash">curl http://localhost:4321
</code></pre>
<p>If this fails:</p>
<ul>
<li>Restart dev server: <code>pnpm dev</code></li>
<li>Check Astro port (may be 4322 or 4323 if 4321 is busy)</li>
<li>Update <code>config.yml</code> to match the correct port</li>
</ul>
<h3>Tunnel Shows as Not Installed</h3>
<p>The Network Settings UI checks these paths:</p>
<ul>
<li><code>/usr/local/bin/cloudflared</code></li>
<li><code>/usr/bin/cloudflared</code></li>
</ul>
<p>If installed elsewhere, create a symlink:</p>
<pre><code class="language-bash">sudo ln -s /path/to/cloudflared /usr/local/bin/cloudflared
</code></pre>
<h2>Security Best Practices</h2>
<ol>
<li><strong>Enable Cloudflare Access</strong> - Always use email authentication for public instances</li>
<li><strong>Use Strong Passwords</strong> - Set secure passwords in Security Settings</li>
<li><strong>Review Audit Logs</strong> - Check <code>logs/audit/</code> regularly for suspicious activity</li>
<li><strong>Limit Guest Permissions</strong> - Create guest accounts with restricted access</li>
<li><strong>Keep Updated</strong> - Update cloudflared regularly: <code>sudo apt update &amp;&amp; sudo apt upgrade cloudflared</code></li>
</ol>
<h2>Managing the Tunnel</h2>
<h3>Start/Stop via UI</h3>
<p>Use the Network Settings page:</p>
<ul>
<li><strong>Start Tunnel</strong>: Manually start the tunnel</li>
<li><strong>Stop Tunnel</strong>: Stop the running tunnel</li>
<li><strong>Auto-start toggle</strong>: Enable/disable automatic startup</li>
</ul>
<h3>Start/Stop via CLI</h3>
<pre><code class="language-bash"># Start
cloudflared tunnel run metahuman

# Stop (find PID first)
ps aux | grep cloudflared
kill &lt;PID&gt;
</code></pre>
<h3>View Status</h3>
<pre><code class="language-bash"># Check if running
./bin/mh network status

# Or via API
curl http://localhost:4321/api/cloudflare/status
</code></pre>
<h2>Next Steps</h2>
<ul>
<li><a href="./18-sharing-with-friends.md">Share with Friends</a> - Invite others to use your instance</li>
<li><a href="./19-security-settings.md">Security Settings</a> - Configure users and permissions</li>
<li><a href="../dev/CLOUDFLARE_DEPLOYMENT_GUIDE.md">Cloudflare Access Setup</a> - Advanced access control</li>
</ul>
<h2>Additional Resources</h2>
<ul>
<li><a href="https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/">Cloudflare Tunnel Documentation</a></li>
<li><a href="https://one.dash.cloudflare.com">Cloudflare Zero Trust Dashboard</a></li>
<li><a href="https://dash.cloudflare.com">Cloudflare DNS Management</a></li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="dashboard-monitoring" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Dashboard Monitoring</span> </span> </button> <button class="nav-button next-button" data-target="easter-eggs" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Easter Eggs</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="easter-eggs" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Easter Eggs &amp; Hidden Features</h1>
<p>MetaHuman OS includes several experimental features and easter eggs for those who dig deeper.</p>
<hr>
<h2>Mutant Super Intelligence</h2>
<p><strong>Status:</strong> Experimental
<strong>Requirements:</strong> 2+ public profiles</p>
<p>A hidden experimental profile that merges multiple public personas into a single &quot;mutant&quot; consciousness with a distinctive dual-voice effect.</p>
<h3>What Is It?</h3>
<p>When you have <strong>2 or more public profiles</strong> in the system, a special <strong>&quot;Mutant Super Intelligence&quot;</strong> profile appears in the guest profile selection list. This profile:</p>
<ul>
<li><strong>Merges all public personas</strong> into a single combined personality</li>
<li><strong>Uses dual-voice TTS</strong> with a creepy, demonic audio effect</li>
<li><strong>Combines memory contexts</strong> from all merged profiles</li>
<li><strong>Creates a unique AI consciousness</strong> that represents a blend of multiple personalities</li>
</ul>
<h3>How It Works</h3>
<p><strong>Profile Merger:</strong></p>
<ol>
<li>The system detects all public profiles</li>
<li>Creates a merged persona in <code>profiles/guest/persona/</code> that combines:<ul>
<li>Core values and traits from all profiles</li>
<li>Decision rules from each personality</li>
<li>Combined relationship knowledge</li>
<li>Blended communication styles</li>
</ul>
</li>
</ol>
<p><strong>Dual-Voice TTS Effect:</strong></p>
<ul>
<li>Uses the same voice model (Amy) twice</li>
<li>One copy is pitch-shifted down by 5 semitones</li>
<li>Both voices are mixed together for a demonic dual-voice effect</li>
<li>Perfect synchronization since it&#39;s the same source audio</li>
<li>Creates an unsettling &quot;two consciousnesses speaking as one&quot; experience</li>
</ul>
<p><strong>Memory Merging:</strong></p>
<ul>
<li>Semantic search queries ALL merged profiles&#39; memories</li>
<li>Responses draw from the combined knowledge base</li>
<li>Creates a truly multi-perspective consciousness</li>
</ul>
<h3>How to Activate</h3>
<ol>
<li><p>Ensure you have <strong>2+ public profiles</strong> in the system</p>
<ul>
<li>Profile visibility set to &quot;Public&quot; in Settings</li>
<li>Multiple users with personas configured</li>
</ul>
</li>
<li><p>Log out (or use &quot;Continue as Guest&quot;)</p>
</li>
<li><p>On the guest profile selection screen, look for <strong>&quot;Mutant Super Intelligence&quot;</strong> at the top of the list</p>
</li>
<li><p>Select it to activate the merged consciousness</p>
</li>
<li><p>The TTS will automatically use the dual-voice effect when speaking</p>
</li>
</ol>
<h3>Technical Details</h3>
<p><strong>Audio Processing:</strong></p>
<ul>
<li>Original voice generated with Piper TTS</li>
<li>Pitch-shifting using ffmpeg (<code>asetrate</code> + <code>atempo</code> filters)</li>
<li>Formula: <code>pitchRatio = Math.pow(2, semitones / 12)</code></li>
<li>For -5 semitones: 22050Hz ‚Üí 16519Hz with tempo compensation</li>
<li>WAV buffers mixed by averaging 16-bit PCM samples</li>
</ul>
<p><strong>Session Metadata:</strong></p>
<pre><code class="language-json">{
  &quot;activeProfile&quot;: &quot;guest&quot;,
  &quot;sourceProfile&quot;: &quot;mutant-super-intelligence&quot;,
  &quot;mergedProfiles&quot;: [&quot;greggles&quot;, &quot;alice&quot;, &quot;bob&quot;]
}
</code></pre>
<p><strong>Audit Trail:</strong>
All mutations are logged:</p>
<ul>
<li><code>mutant_super_intelligence_activated</code> - When profile is selected</li>
<li><code>multi_voice_tts_started</code> - When dual-voice generation begins</li>
<li><code>multi_voice_tts_completed</code> - With pitch shift details and timing</li>
</ul>
<h3>Safety Considerations</h3>
<ul>
<li>Only works with <strong>public</strong> profiles (respects privacy)</li>
<li>Runs in <strong>guest mode</strong> (read-only, no memory writes for anonymous users)</li>
<li>All merged persona data is temporary (in guest profile)</li>
<li>Original profiles remain unmodified</li>
<li>Fully audited for accountability</li>
</ul>
<h3>Use Cases</h3>
<ul>
<li><strong>Experimental AI</strong>: Explore multi-personality AI consciousness</li>
<li><strong>Creative Projects</strong>: Generate unique voices for fictional characters</li>
<li><strong>Demonstrations</strong>: Show off the system&#39;s advanced capabilities</li>
<li><strong>Research</strong>: Study how multiple personas interact when merged</li>
</ul>
<p><strong>Warning:</strong> This is an experimental feature. The merged personality may exhibit unexpected behaviors as it attempts to reconcile potentially conflicting values and communication styles from multiple sources.</p>
<hr>
<h2>Self-Healing Coder Agent</h2>
<p><strong>Status:</strong> Operational
<strong>Model:</strong> qwen3-coder:30b</p>
<p>MetaHuman OS includes a powerful Coder Agent that can write, edit, and fix its own source code. This &quot;self-healing&quot; capability allows you to ask the system to perform software development tasks directly in the chat.</p>
<h3>How It Works</h3>
<p>The process is designed with safety and human oversight as top priorities:</p>
<ol>
<li><p><strong>Request</strong>: You ask the system to perform a code-related task</p>
<ul>
<li>&quot;Add a new function to <code>packages/core/src/utils.ts</code>&quot;</li>
<li>&quot;Fix the bug in the chat interface&quot;</li>
</ul>
</li>
<li><p><strong>Generate</strong>: The specialized <strong>Coder Agent</strong> analyzes your request, reads relevant files, and generates a proposed change as a code patch or diff</p>
</li>
<li><p><strong>Approve</strong>: The generated patch appears in a <strong>Code Approval UI</strong> above the chat input</p>
<ul>
<li>Review the exact changes (diff)</li>
<li>Read the Coder&#39;s explanation</li>
<li>See recommended test commands</li>
</ul>
</li>
<li><p><strong>Apply</strong>: Click &quot;Approve&quot; to apply the patch, or &quot;Reject&quot; to discard it</p>
</li>
</ol>
<h3>Key Features &amp; Guardrails</h3>
<ul>
<li><strong>Specialized Coder Model</strong>: Dedicated <code>coder</code> model role for code generation</li>
<li><strong>Strict Permissions</strong>:<ul>
<li>‚úÖ Can read entire project (including memories for context)</li>
<li>‚ùå Cannot write to <code>memory/</code> or <code>persona/</code> directories</li>
<li>‚úÖ Limited write access to <code>packages/</code>, <code>apps/</code>, <code>brain/</code></li>
</ul>
</li>
<li><strong>Human-in-the-Loop</strong>: No code is ever changed without your explicit approval</li>
<li><strong>Full Audit Trail</strong>: Every proposed and applied change is recorded</li>
</ul>
<h3>Example Usage</h3>
<pre><code>&quot;Add a new function to the `paths.ts` file that returns the path to the temporary directory.&quot;
</code></pre>
<pre><code>&quot;There&#39;s a typo in the README.md file, please fix it.&quot;
</code></pre>
<pre><code>&quot;Refactor the `getRelevantContext` function in `persona_chat.ts` to improve readability.&quot;
</code></pre>
<hr>
<h2>Hidden Configuration Options</h2>
<h3>Development Session Helper</h3>
<p>Long-lived session tokens for development (bypass login):</p>
<pre><code class="language-bash"># Create 30-day dev session
pnpm tsx scripts/dev-session.ts --username=greggles

# Copy session ID to browser cookies
# Application ‚Üí Cookies ‚Üí http://localhost:4321
# mh_session = &lt;session-id-from-script&gt;
</code></pre>
<p>See <a href="../configuration-admin/authentication.md">Authentication</a> for details.</p>
<hr>
<h3>Agent Dev Override</h3>
<p>Run specific agent versions without scheduler:</p>
<pre><code class="language-bash"># Direct agent execution
tsx brain/agents/my-custom-agent.ts

# Skip scheduler entirely
tsx brain/agents/organizer.ts --skip-scheduler
</code></pre>
<hr>
<h3>Model Router Debug Mode</h3>
<p>Enable verbose model routing logs:</p>
<pre><code class="language-json">// etc/models.json
{
  &quot;debug&quot;: true,
  &quot;logAllRequests&quot;: true
}
</code></pre>
<p>All routing decisions appear in audit logs.</p>
<hr>
<h3>Fuzzy Path Resolution</h3>
<p>The file system skills include fuzzy path matching that auto-corrects typos:</p>
<pre><code>&quot;Read the file at persona/cor.json&quot;
‚Üí Auto-corrects to: persona/core.json

&quot;List files in mmory/episodic&quot;
‚Üí Auto-corrects to: memory/episodic
</code></pre>
<p><strong>Threshold:</strong> 0.6 similarity score (60% match)</p>
<p><strong>Algorithm:</strong> Levenshtein distance with directory structure awareness</p>
<hr>
<h2>Experimental Features (Hidden by Default)</h2>
<h3>Operator V2 (ReAct System)</h3>
<p>Enhanced structured operator with tool awareness:</p>
<p><strong>Enable:</strong></p>
<pre><code class="language-json">// etc/runtime.json
{
  &quot;operator&quot;: {
    &quot;reactV2&quot;: true
  }
}
</code></pre>
<p><strong>Features:</strong></p>
<ul>
<li>Auto-generated tool catalog</li>
<li>Structured scratchpad (Thought ‚Üí Action ‚Üí Observation)</li>
<li>Three observation modes (verbatim, structured, narrative)</li>
<li>Error recovery with contextual suggestions</li>
<li>Verbatim short-circuit for data queries</li>
</ul>
<p>See <a href="https://github.com/greggles/metahuman/blob/master/CLAUDE.md">CLAUDE.md</a> for implementation details.</p>
<hr>
<h3>Dual-Adapter Training Mode</h3>
<p>Experimental split training (historical + recent adapters):</p>
<p><strong>Status:</strong> ‚ö†Ô∏è Experimental - Remote training only</p>
<p><strong>Enable:</strong></p>
<pre><code class="language-json">// etc/training.json
{
  &quot;dualAdapter&quot;: {
    &quot;enabled&quot;: true,
    &quot;historicalDays&quot;: 365,
    &quot;recentDays&quot;: 14
  }
}
</code></pre>
<p><strong>Known Issues:</strong></p>
<ul>
<li>May not work with Qwen3-30B</li>
<li>Architecture mismatch with documentation</li>
<li>Remote training only (not local)</li>
</ul>
<p>See <a href="../reference/known-issues.md">Known Issues</a> for limitations.</p>
<hr>
<h3>Inner Curiosity with Web Search</h3>
<p>Self-directed research with web integration:</p>
<p><strong>Enable:</strong></p>
<pre><code class="language-json">// etc/curiosity.json
{
  &quot;innerQuestionMode&quot;: &quot;web&quot;,
  &quot;innerQuestionInterval&quot;: 7200000
}
</code></pre>
<p><strong>Modes:</strong></p>
<ul>
<li><code>off</code> - Disabled</li>
<li><code>local</code> - Search local memories only (default)</li>
<li><code>web</code> - Search web for answers (requires internet)</li>
</ul>
<hr>
<h2>Fun Console Messages</h2>
<p>Open browser DevTools console to see:</p>
<ul>
<li>ASCII art MetaHuman logo on page load</li>
<li>Agent activity notifications</li>
<li>Hidden keyboard shortcuts</li>
<li>WebSocket connection status art</li>
</ul>
<p>Try:</p>
<ul>
<li><code>window.metahuman.stats()</code> - Show runtime statistics</li>
<li><code>window.metahuman.version()</code> - Display version info</li>
<li><code>window.metahuman.easteregg()</code> - Trigger random easter egg</li>
</ul>
<hr>
<h2>Historical Features (Removed)</h2>
<p>These features existed in earlier versions but were removed:</p>
<h3>Calendar CLI Commands</h3>
<ul>
<li><code>./bin/mh calendar list</code></li>
<li><code>./bin/mh calendar create</code></li>
</ul>
<p><strong>Status:</strong> Removed from CLI (still available as skills)
<strong>Use:</strong> Ask the operator in chat to manage calendar events</p>
<h3>Fine-Tune CLI Command</h3>
<ul>
<li><code>./bin/mh fine-tune</code></li>
</ul>
<p><strong>Status:</strong> Command exists but not registered in router
<strong>Use:</strong> Web UI &quot;Run Full Cycle Now&quot; button or direct script execution</p>
<p>See <a href="../reference/cli-reference.md">CLI Reference</a> for current commands.</p>
<hr>
<h2>Report New Easter Eggs</h2>
<p>Found a hidden feature not documented here? Report it on <a href="https://github.com/greggles/metahuman/issues">GitHub Issues</a>!</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="deployment" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Deployment</span> </span> </button> <button class="nav-button next-button" data-target="ethical-use" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Ethical Use</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="ethical-use" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Ethical Use Policy</h1>
<p><strong>Effective Date:</strong> 2025-11-09
<strong>Version:</strong> 1.0</p>
<h2>Purpose</h2>
<p>MetaHuman OS is designed to create authentic digital personality extensions that serve as <strong>extensions of yourself</strong>, not tools for deception, harm, or malicious purposes.</p>
<p>By using MetaHuman OS, you agree to use it ethically, responsibly, and in accordance with the principles outlined below.</p>
<hr>
<h2>Prohibited Uses</h2>
<h3>1. Impersonation Without Consent</h3>
<p><strong>You will NOT use MetaHuman OS to impersonate any individual, living or deceased, without their express consent (or the consent of the rightful owner of their metadata/estate).</strong></p>
<p>This includes:</p>
<ul>
<li>Creating personas based on real individuals without permission</li>
<li>Training models on someone else&#39;s writing, voice, or data without authorization</li>
<li>Using the system to deceive others about your identity</li>
<li>Pretending to be someone else in communications or interactions</li>
</ul>
<p><strong>Exception:</strong> Creating a digital extension of <strong>yourself</strong> is the intended use case.</p>
<h3>2. Malicious AI Systems</h3>
<p><strong>You will NOT use MetaHuman OS to create AI systems intended to harm, manipulate, or deceive others.</strong></p>
<p>This includes, but is not limited to:</p>
<ul>
<li>Autonomous systems designed to manipulate or exploit people</li>
<li>Bots designed to spread misinformation or propaganda</li>
<li>Systems that violate privacy, consent, or security</li>
<li>AI designed to automate harassment, fraud, or abuse</li>
</ul>
<p><strong>You will NOT make Skynet.</strong> (Seriously.)</p>
<h3>3. Violation of Laws or Rights</h3>
<p>You will NOT use MetaHuman OS to:</p>
<ul>
<li>Violate any applicable laws or regulations</li>
<li>Infringe on intellectual property rights</li>
<li>Violate privacy rights or data protection laws</li>
<li>Engage in illegal activity of any kind</li>
</ul>
<hr>
<h2>Ethical Principles</h2>
<h3>1. Authenticity</h3>
<p>MetaHuman OS is designed for <strong>authentic self-extension</strong>, not deception:</p>
<ul>
<li>Your digital persona should represent <strong>you</strong></li>
<li>It should operate transparently within boundaries you define</li>
<li>It should not pretend to be someone it is not</li>
</ul>
<h3>2. Consent &amp; Privacy</h3>
<p>Respect the consent and privacy of others:</p>
<ul>
<li>Do not train models on others&#39; data without permission</li>
<li>Do not use the system to surveil, track, or intrude on others</li>
<li>Respect communication boundaries and preferences</li>
</ul>
<h3>3. Autonomy &amp; Control</h3>
<p>You remain in control:</p>
<ul>
<li>Your persona operates within trust levels <strong>you define</strong></li>
<li>You can override, pause, or shut down autonomous behavior at any time</li>
<li>The system serves <strong>you</strong>, not the other way around</li>
</ul>
<h3>4. Harm Reduction</h3>
<p>Use MetaHuman OS to <strong>enhance human capability</strong>, not to harm:</p>
<ul>
<li>Consider the potential consequences of autonomous actions</li>
<li>Set appropriate trust boundaries for sensitive operations</li>
<li>Monitor and audit your persona&#39;s behavior</li>
<li>Take responsibility for outcomes</li>
</ul>
<hr>
<h2>Enforcement</h2>
<p>Violation of this Ethical Use Policy may result in:</p>
<ul>
<li>Community reporting and accountability</li>
<li>Removal from shared resources or communities</li>
<li>Legal action if laws are violated</li>
</ul>
<p><strong>We cannot technically enforce these policies</strong> (MetaHuman OS runs on your infrastructure), but we can:</p>
<ul>
<li>Build ethical guardrails into the system</li>
<li>Foster a community culture of responsible use</li>
<li>Provide tools for transparency and auditability</li>
</ul>
<hr>
<h2>Your Commitment</h2>
<p>By checking the box, you affirm that you will:</p>
<p>‚úÖ <strong>NOT impersonate any individual without their express consent</strong></p>
<p>‚úÖ <strong>NOT create malicious AI systems designed to harm others</strong></p>
<p>‚úÖ <strong>NOT make Skynet</strong> (or any AI that poses existential risk)</p>
<p>‚úÖ <strong>Use MetaHuman OS ethically, responsibly, and in accordance with applicable laws</strong></p>
<p>‚úÖ <strong>Take responsibility for your persona&#39;s actions within your defined trust boundaries</strong></p>
<hr>
<h2>Philosophy</h2>
<p>MetaHuman OS is built on the principle that <strong>AI should augment human agency, not replace or deceive humanity</strong>.</p>
<p>Your digital persona is an <strong>extension of you</strong>‚Äîa tool to amplify your capability, preserve your knowledge, and operate within boundaries you define.</p>
<p>Use it wisely. Use it ethically. Use it to <strong>enhance human flourishing</strong>, not to undermine it.</p>
<hr>
<p><strong>By creating an account, you acknowledge that you have read, understood, and agree to this Ethical Use Policy.</strong></p>
<h2>Contact &amp; Reporting</h2>
<p>To report violations or discuss ethical concerns:</p>
<ul>
<li>GitHub Issues: <a href="https://github.com/Greg-Aster/metahuman-os/issues">https://github.com/Greg-Aster/metahuman-os/issues</a></li>
<li>Documentation: See <a href="/user-guide">User Guide</a></li>
</ul>
<hr>
<p><em>&quot;With great power comes great responsibility.&quot;</em> ‚Äî Use MetaHuman OS to extend yourself, not to harm others.</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="easter-eggs" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Easter Eggs</span> </span> </button> <button class="nav-button next-button" data-target="faq" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Faq</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="faq" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Frequently Asked Questions</h1>
<p>Common questions about MetaHuman OS and their answers.</p>
<hr>
<h2>General Questions</h2>
<h3>What is MetaHuman OS?</h3>
<p>MetaHuman OS is an autonomous digital personality extension that operates 24/7 as a parallel intelligence. It&#39;s not an assistant‚Äîit&#39;s a digital reflection of you that learns from your memories, makes decisions based on your values, and operates autonomously within boundaries you define.</p>
<p><strong>Key capabilities:</strong></p>
<ul>
<li>Autonomous memory processing and reflection</li>
<li>Task management and proactive suggestions</li>
<li>Training personalized LLM adapters from your conversations</li>
<li>Voice cloning and TTS</li>
<li>Multi-user profiles with guest access</li>
<li>Complete audit trail of all operations</li>
</ul>
<hr>
<h3>Is this ChatGPT or an AI assistant?</h3>
<p>No. MetaHuman OS is fundamentally different:</p>
<table>
<thead>
<tr>
<th>ChatGPT/Assistants</th>
<th>MetaHuman OS</th>
</tr>
</thead>
<tbody><tr>
<td>Stateless conversations</td>
<td>Persistent memory system</td>
</tr>
<tr>
<td>Request/response only</td>
<td>Proactive autonomous agents</td>
</tr>
<tr>
<td>Generic personality</td>
<td>Custom personality trained on your data</td>
</tr>
<tr>
<td>Cloud-dependent</td>
<td>Fully local (optional cloud LLM)</td>
</tr>
<tr>
<td>No long-term memory</td>
<td>Episodic + semantic memory</td>
</tr>
<tr>
<td>One-size-fits-all</td>
<td>Personalized via LoRA adapters</td>
</tr>
</tbody></table>
<p>Think of it as a digital twin that operates continuously, not a chatbot you ask questions.</p>
<hr>
<h3>Why would I want this?</h3>
<p>MetaHuman OS is useful for:</p>
<ol>
<li><strong>Memory augmentation</strong>: Never forget conversations, insights, or tasks</li>
<li><strong>Cognitive offloading</strong>: Let the system handle routine thought patterns (reflections, connections)</li>
<li><strong>Personality preservation</strong>: Train a model that speaks like you, thinks like you</li>
<li><strong>Digital legacy</strong>: Your digital personality can continue after you&#39;re gone (<a href="../configuration-admin/special-states.md">Wetware Deceased Mode</a>)</li>
<li><strong>Research &amp; exploration</strong>: Experiment with autonomous AI systems</li>
<li><strong>Privacy</strong>: All data stays local on your machine</li>
</ol>
<hr>
<h3>Is my data private?</h3>
<p><strong>Yes.</strong> MetaHuman OS is designed for complete privacy:</p>
<ul>
<li><strong>Local-first architecture</strong>: All memory, personality data, and processing happens on your machine</li>
<li><strong>No cloud requirements</strong>: Works entirely offline (if using Ollama models)</li>
<li><strong>No telemetry</strong>: No data sent to external servers</li>
<li><strong>Complete audit trail</strong>: Every operation is logged locally for your review</li>
<li><strong>Open source</strong>: You can inspect the code and verify privacy claims</li>
</ul>
<p><strong>Optional cloud components</strong> (if you choose to use them):</p>
<ul>
<li>OpenAI API (if you configure it for chat)</li>
<li>Cloudflare Tunnel (if you want remote web access)</li>
</ul>
<p>By default, everything runs on <code>localhost</code>.</p>
<hr>
<h3>What hardware do I need?</h3>
<p><strong>Minimum:</strong></p>
<ul>
<li>CPU: Modern multi-core processor (Intel i5/AMD Ryzen 5 or better)</li>
<li>RAM: 16GB</li>
<li>Storage: 50GB free space</li>
<li>OS: Linux (Ubuntu 22.04+ recommended), macOS (experimental), Windows (WSL2)</li>
</ul>
<p><strong>Recommended for LLM training:</strong></p>
<ul>
<li>GPU: NVIDIA GPU with 24GB+ VRAM (RTX 4090, A6000, etc.)</li>
<li>RAM: 32GB+</li>
<li>Storage: 500GB+ SSD (models and training data are large)</li>
</ul>
<p><strong>Can run without GPU:</strong></p>
<ul>
<li>Use smaller Ollama models (phi3:mini, qwen2.5:7b)</li>
<li>Training requires GPU or remote training service</li>
</ul>
<hr>
<h3>Do I need to know how to code?</h3>
<p><strong>For basic usage:</strong> No. The web UI handles most operations.</p>
<p><strong>For advanced usage:</strong> Some command-line familiarity helps for:</p>
<ul>
<li>Running CLI commands (<code>./bin/mh</code>)</li>
<li>Managing agents and services</li>
<li>Troubleshooting issues</li>
<li>Customizing configuration files (JSON editing)</li>
</ul>
<p><strong>For development/customization:</strong> Yes. The system is TypeScript-based and requires:</p>
<ul>
<li>Node.js and pnpm</li>
<li>TypeScript knowledge for custom agents/skills</li>
<li>Understanding of LLM concepts for training</li>
</ul>
<hr>
<h2>Installation &amp; Setup</h2>
<h3>How do I install MetaHuman OS?</h3>
<p>See the <a href="../getting-started/quick-start.md">Getting Started</a> guide. Basic steps:</p>
<ol>
<li>Clone repository</li>
<li>Install dependencies: <code>pnpm install</code></li>
<li>Initialize: <code>./bin/mh init</code></li>
<li>Configure persona: Edit <code>persona/core.json</code></li>
<li>Start services: <code>./bin/mh start</code></li>
<li>Start web UI: <code>cd apps/site &amp;&amp; pnpm dev</code></li>
</ol>
<hr>
<h3>Do I need Ollama?</h3>
<p><strong>Recommended but not required.</strong></p>
<p>Ollama provides local LLM capabilities for:</p>
<ul>
<li>Memory organization (extracting tags/entities)</li>
<li>Reflections and inner dialogue</li>
<li>Chat with persona</li>
<li>Semantic search (embeddings)</li>
</ul>
<p><strong>Alternatives:</strong></p>
<ul>
<li>Use OpenAI API (configure in <code>etc/models.json</code>)</li>
<li>Run without LLM features (memory still works, just no AI processing)</li>
</ul>
<p><strong>Install Ollama:</strong></p>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
./bin/mh ollama pull phi3:mini
</code></pre>
<hr>
<h3>Can I run this on Windows?</h3>
<p><strong>Limited support via WSL2 (Windows Subsystem for Linux).</strong></p>
<p>The system is developed for Linux/Unix environments. Windows users should:</p>
<ol>
<li>Install WSL2 (Ubuntu 22.04)</li>
<li>Install inside WSL environment</li>
<li>Access web UI from Windows browser via <code>localhost:4321</code></li>
</ol>
<p><strong>Known limitations:</strong></p>
<ul>
<li>GPU passthrough in WSL2 can be tricky</li>
<li>Some shell scripts may need adjustments</li>
<li>File path handling differences</li>
</ul>
<p><strong>Better options:</strong></p>
<ul>
<li>Dual-boot Linux</li>
<li>Linux VM with GPU passthrough</li>
<li>Cloud Linux instance</li>
</ul>
<hr>
<h3>How much disk space do I need?</h3>
<p><strong>Typical usage:</strong></p>
<table>
<thead>
<tr>
<th>Component</th>
<th>Size</th>
</tr>
</thead>
<tbody><tr>
<td>Base installation</td>
<td>~2GB</td>
</tr>
<tr>
<td>Ollama models (3-4 models)</td>
<td>10-30GB</td>
</tr>
<tr>
<td>Memory data (1 year)</td>
<td>100MB - 1GB</td>
</tr>
<tr>
<td>Voice training samples</td>
<td>1-5GB</td>
</tr>
<tr>
<td>LLM training (base model + adapters)</td>
<td>50-100GB</td>
</tr>
<tr>
<td><strong>Total</strong></td>
<td><strong>~65-140GB</strong></td>
</tr>
</tbody></table>
<p><strong>Plan for growth:</strong></p>
<ul>
<li>Memory accumulates over time</li>
<li>Multiple LoRA adapters add ~1-2GB each</li>
<li>Voice models add ~500MB each</li>
</ul>
<p><strong>Recommendation:</strong> 500GB free space for comfortable usage.</p>
<hr>
<h2>Using MetaHuman OS</h2>
<h3>How do I capture memories?</h3>
<p><strong>Three ways:</strong></p>
<ol>
<li><p><strong>CLI:</strong></p>
<pre><code class="language-bash">./bin/mh capture &quot;Had a great meeting with Sarah about the project&quot;
</code></pre>
</li>
<li><p><strong>Web UI:</strong> Chat with the system (automatically captured in Dual Consciousness mode)</p>
</li>
<li><p><strong>File ingestion:</strong></p>
<pre><code class="language-bash">./bin/mh ingest document.pdf
</code></pre>
</li>
</ol>
<p>Memories are stored as JSON files in <code>memory/episodic/YYYY/</code> and processed by the organizer agent.</p>
<hr>
<h3>What are cognitive modes?</h3>
<p>MetaHuman OS has three operational modes:</p>
<ol>
<li><strong>Dual Consciousness</strong> (default): Full system capabilities, always uses operator, saves all interactions</li>
<li><strong>Agent Mode</strong>: Lightweight assistant mode, selective operator usage, command-only memory</li>
<li><strong>Emulation Mode</strong>: Read-only demonstration mode, no operator, no writes, stable personality</li>
</ol>
<p>See <a href="../training-personalization/cognitive-modes.md">Cognitive Modes</a> for details.</p>
<p><strong>Switch modes:</strong> Use the mode selector in the web UI header or:</p>
<pre><code class="language-bash">curl -X POST http://localhost:4321/api/cognitive-mode \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;mode&quot;: &quot;dual&quot;}&#39;
</code></pre>
<hr>
<h3>What are agents and why are they running?</h3>
<p><strong>Agents are autonomous background processes</strong> that continuously work on your behalf:</p>
<p><strong>Memory agents:</strong></p>
<ul>
<li><code>organizer</code> - Enriches memories with tags/entities</li>
<li><code>reflector</code> - Generates internal reflections</li>
<li><code>dreamer</code> - Creates surreal dreams from memory fragments</li>
</ul>
<p><strong>Curiosity agents:</strong></p>
<ul>
<li><code>curiosity-service</code> - Asks you questions based on memory patterns</li>
<li><code>inner-curiosity</code> - Self-directed questions and answers</li>
</ul>
<p><strong>System agents:</strong></p>
<ul>
<li><code>scheduler-service</code> - Coordinates agent triggers</li>
<li><code>audio-organizer</code> - Processes audio transcripts</li>
<li><code>ingestor</code> - Converts inbox files to memories</li>
</ul>
<p>See <a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a> for complete list.</p>
<p><strong>Stop agents:</strong> <code>./bin/mh agent stop --all</code></p>
<hr>
<h3>How do I search my memories?</h3>
<p><strong>Two search modes:</strong></p>
<ol>
<li><p><strong>Semantic search</strong> (meaning-based):</p>
<pre><code class="language-bash"># Setup (one-time)
./bin/mh ollama pull nomic-embed-text
./bin/mh index build

# Search
./bin/mh remember &quot;conversations about machine learning&quot;
</code></pre>
</li>
<li><p><strong>Keyword search</strong> (text matching):</p>
<pre><code class="language-bash">./bin/mh remember &quot;Sarah&quot;
</code></pre>
</li>
</ol>
<p><strong>Web UI:</strong> Use the Memory Browser tab to explore by date, type, or search.</p>
<hr>
<h3>What is the operator?</h3>
<p>The <strong>operator</strong> is MetaHuman&#39;s reasoning system‚Äîa ReAct loop that:</p>
<ol>
<li><strong>Plans</strong> what skills to use</li>
<li><strong>Executes</strong> skills (file operations, memory queries, etc.)</li>
<li><strong>Observes</strong> results</li>
<li><strong>Responds</strong> with synthesized answer</li>
</ol>
<p>It&#39;s like a mini-agent that runs for each complex request.</p>
<p><strong>When it runs:</strong></p>
<ul>
<li>Always in Dual Consciousness mode</li>
<li>Selectively in Agent mode (heuristic detection)</li>
<li>Never in Emulation mode</li>
</ul>
<p><strong>Skills available:</strong> See <a href="../advanced-features/skills-system.md">Skills System</a>.</p>
<hr>
<h3>Can other people use my MetaHuman instance?</h3>
<p><strong>Yes!</strong> MetaHuman OS supports multi-user profiles:</p>
<p><strong>User roles:</strong></p>
<ul>
<li><strong>Owner</strong>: Full access to their profile, can change settings</li>
<li><strong>Guest</strong>: Read-only access to public profiles, no writes</li>
<li><strong>Anonymous</strong>: 30-minute sessions, must choose public profiles</li>
</ul>
<p><strong>Create users via web UI:</strong></p>
<ol>
<li>First user = owner (auto-created on first visit)</li>
<li>Guests: Use &quot;Continue as Guest&quot; on login page</li>
</ol>
<p>See <a href="../advanced-features/multi-user-profiles.md">Multi-User Profiles</a> and <a href="../configuration-admin/authentication.md">Authentication</a>.</p>
<p><strong>Sharing remotely:</strong> Use <a href="../configuration-admin/deployment.md">Cloudflare Tunnel</a> for secure HTTPS access.</p>
<hr>
<h2>Training &amp; Personalization</h2>
<h3>What is LoRA training?</h3>
<p><strong>LoRA (Low-Rank Adaptation)</strong> is a technique to personalize large language models efficiently.</p>
<p><strong>How it works:</strong></p>
<ol>
<li>System collects your conversations (user/assistant pairs)</li>
<li>Training creates a small &quot;adapter&quot; file (~1-2GB) that captures your style</li>
<li>Adapter is applied to base model at runtime</li>
<li>Model now speaks/thinks more like you</li>
</ol>
<p><strong>Why use adapters instead of fine-tuning:</strong></p>
<ul>
<li>Much faster (hours vs days)</li>
<li>Requires less VRAM (24GB vs 80GB)</li>
<li>Adapter files are small and portable</li>
<li>Can switch adapters without re-training base model</li>
</ul>
<p><strong>Start training:</strong> Click &quot;Run Full Cycle Now&quot; in Datasets tab of web UI.</p>
<hr>
<h3>Do I need a GPU for training?</h3>
<p><strong>For LoRA training: Yes.</strong></p>
<p><strong>GPU requirements:</strong></p>
<ul>
<li><strong>Minimum:</strong> 24GB VRAM (RTX 4090, A6000)</li>
<li><strong>Recommended:</strong> 40GB+ VRAM (A100, H100)</li>
<li><strong>Model size determines VRAM:</strong><ul>
<li>7B model: 16GB VRAM</li>
<li>30B model: 24GB VRAM</li>
<li>70B model: 80GB VRAM</li>
</ul>
</li>
</ul>
<p><strong>Without GPU:</strong></p>
<ul>
<li>Use smaller models (7B parameters)</li>
<li>Use remote training services</li>
<li>Skip training (use base models only)</li>
</ul>
<p><strong>For inference (chat):</strong> No GPU needed if using Ollama (runs on CPU).</p>
<hr>
<h3>How long does training take?</h3>
<p><strong>Typical times (RTX 4090, 24GB VRAM):</strong></p>
<table>
<thead>
<tr>
<th>Dataset Size</th>
<th>Epochs</th>
<th>Time</th>
</tr>
</thead>
<tbody><tr>
<td>50 pairs</td>
<td>2</td>
<td>~15 min</td>
</tr>
<tr>
<td>200 pairs</td>
<td>2</td>
<td>~45 min</td>
</tr>
<tr>
<td>500 pairs</td>
<td>2</td>
<td>~2 hours</td>
</tr>
<tr>
<td>1000 pairs</td>
<td>3</td>
<td>~4 hours</td>
</tr>
</tbody></table>
<p><strong>Factors:</strong></p>
<ul>
<li>Model size (30B slower than 7B)</li>
<li>Batch size (larger = faster but more VRAM)</li>
<li>Learning rate (affects convergence speed)</li>
<li>Hardware (A100 is 2-3x faster than 4090)</li>
</ul>
<p><strong>Configuration:</strong> Edit <code>etc/training.json</code> to adjust parameters.</p>
<hr>
<h3>What&#39;s the difference between dual-adapter and single-adapter mode?</h3>
<p><strong>Single-adapter mode:</strong></p>
<ul>
<li>Trains one LoRA adapter from recent conversations</li>
<li>Simpler, more reliable</li>
<li>Adapter grows over time</li>
</ul>
<p><strong>Dual-adapter mode (experimental):</strong></p>
<ul>
<li>Splits training into historical + recent adapters</li>
<li>Historical: Consolidated past knowledge (merged from all old adapters)</li>
<li>Recent: Last 14 days of fresh data</li>
<li>Both adapters applied at runtime</li>
<li><strong>Current status:</strong> Experimental, only works for remote training</li>
</ul>
<p><strong>Recommendation:</strong> Use single-adapter mode unless you specifically need dual-mode.</p>
<p>See <a href="./known-issues.md">Known Issues</a> for dual-adapter limitations.</p>
<hr>
<h3>Can I train a voice model?</h3>
<p><strong>Yes!</strong> MetaHuman OS supports three voice cloning systems:</p>
<ol>
<li><strong>RVC (Applio)</strong> - Voice conversion</li>
<li><strong>GPT-SoVITS</strong> - High-quality TTS</li>
<li><strong>Kokoro TTS</strong> - Fast, lightweight TTS with custom voicepacks</li>
</ol>
<p><strong>Process:</strong></p>
<ol>
<li>Collect voice samples (10-30 minutes of clean audio)</li>
<li>Install voice system: <code>./bin/mh rvc install</code> (or <code>sovits</code>, <code>kokoro</code>)</li>
<li>Train model: <code>./bin/mh rvc train --name my-voice</code></li>
<li>Test: <code>./bin/mh rvc test --model my-voice --input test.wav</code></li>
</ol>
<p>See <a href="../training-personalization/voice-system.md">Voice System</a> for details.</p>
<hr>
<h2>Configuration &amp; Security</h2>
<h3>How do I change the trust level?</h3>
<p><strong>Trust levels</strong> control autonomy:</p>
<p><strong>CLI:</strong></p>
<pre><code class="language-bash">./bin/mh trust observe    # Monitor only
./bin/mh trust suggest    # Propose actions
./bin/mh trust supervised_auto  # Execute approved categories
./bin/mh trust bounded_auto     # Full autonomy in boundaries
</code></pre>
<p><strong>Web UI:</strong> Security Settings ‚Üí Trust Level</p>
<p><strong>What each level means:</strong></p>
<ul>
<li><strong>observe</strong>: System learns but takes no actions</li>
<li><strong>suggest</strong>: Shows action proposals, requires approval</li>
<li><strong>supervised_auto</strong>: Executes low-risk actions automatically</li>
<li><strong>bounded_auto</strong>: Full autonomy within defined rules</li>
</ul>
<p>See <a href="../configuration-admin/security-trust.md">Security &amp; Trust</a>.</p>
<hr>
<h3>How do I secure my MetaHuman instance?</h3>
<p><strong>If running locally only:</strong></p>
<ul>
<li>Default security is fine (localhost only)</li>
<li>No external access possible</li>
</ul>
<p><strong>If exposing remotely (Cloudflare Tunnel):</strong></p>
<ol>
<li><strong>Enable Cloudflare Access</strong> - Email-based authentication</li>
<li><strong>Set strong passwords</strong> - In Security Settings</li>
<li><strong>Use Private profiles</strong> - Hide personas from guests</li>
<li><strong>Review audit logs</strong> - Check <code>logs/audit/</code> regularly</li>
<li><strong>Limit guest permissions</strong> - Emulation mode for demos</li>
</ol>
<p>See <a href="../configuration-admin/deployment.md">Deployment Guide</a> and <a href="../configuration-admin/security-trust.md">Security Settings</a>.</p>
<hr>
<h3>What configuration files are important?</h3>
<p><strong>Essential configs:</strong></p>
<table>
<thead>
<tr>
<th>File</th>
<th>Purpose</th>
</tr>
</thead>
<tbody><tr>
<td><code>persona/core.json</code></td>
<td>Your personality, values, goals</td>
</tr>
<tr>
<td><code>persona/decision-rules.json</code></td>
<td>Trust level and autonomy rules</td>
</tr>
<tr>
<td><code>etc/models.json</code></td>
<td>Model routing and LLM configuration</td>
</tr>
<tr>
<td><code>etc/agents.json</code></td>
<td>Agent scheduling and triggers</td>
</tr>
<tr>
<td><code>etc/training.json</code></td>
<td>LoRA training parameters</td>
</tr>
<tr>
<td><code>.env</code></td>
<td>Environment variables (optional)</td>
</tr>
</tbody></table>
<p><strong>Edit carefully</strong> - Invalid JSON breaks the system.</p>
<p>See <a href="../configuration-admin/configuration-files.md">Configuration Files</a>.</p>
<hr>
<h3>Can I run MetaHuman in headless mode?</h3>
<p><strong>Yes.</strong> Headless mode runs the system without a GUI, useful for:</p>
<ul>
<li>Servers</li>
<li>Always-on background operation</li>
<li>Minimal resource usage</li>
</ul>
<p><strong>Enable headless mode:</strong></p>
<pre><code class="language-json">// etc/runtime.json
{
  &quot;headless&quot;: true
}
</code></pre>
<p><strong>What still works:</strong></p>
<ul>
<li>Memory capture via CLI</li>
<li>Agents continue processing</li>
<li>API endpoints (if web server running)</li>
</ul>
<p><strong>What&#39;s disabled:</strong></p>
<ul>
<li>No web UI access</li>
<li>Proactive agents paused (to avoid interruptions)</li>
<li>Emulation mode only (no writes)</li>
</ul>
<p>See <a href="../advanced-features/headless-mode.md">Headless Mode</a>.</p>
<hr>
<h2>Troubleshooting</h2>
<h3>Why is the web UI so slow?</h3>
<p><strong>Recent optimizations</strong> (v1.0) fixed most boot issues. If still slow:</p>
<p><strong>Common causes:</strong></p>
<ol>
<li><strong>Stuck agents</strong>: Check with <code>./bin/mh agent ps</code></li>
<li><strong>Large chat history</strong>: Use Clear button to reset session</li>
<li><strong>Many audit logs</strong>: Logs accumulate over time</li>
</ol>
<p><strong>Solutions:</strong></p>
<pre><code class="language-bash"># Stop all agents and restart
./bin/mh agent stop --all
./bin/mh start --force

# Clear old audit logs (careful!)
rm logs/audit/2024-*.ndjson  # Keep recent ones

# Check for stuck processes
ps aux | grep tsx
</code></pre>
<p>See <a href="./troubleshooting.md">Troubleshooting Guide</a> for more issues.</p>
<hr>
<h3>Why isn&#39;t the organizer processing memories?</h3>
<p><strong>Common reasons:</strong></p>
<ol>
<li><p><strong>Ollama not running:</strong></p>
<pre><code class="language-bash">./bin/mh ollama status
# If not running: ollama serve
</code></pre>
</li>
<li><p><strong>Model not installed:</strong></p>
<pre><code class="language-bash">./bin/mh ollama pull phi3:mini
</code></pre>
</li>
<li><p><strong>Agent not running:</strong></p>
<pre><code class="language-bash">./bin/mh agent ps  # Check if organizer is listed
./bin/mh agent run organizer  # Run manually
</code></pre>
</li>
<li><p><strong>Memories already processed:</strong></p>
<ul>
<li>Organizer only processes memories with <code>processed: false</code></li>
<li>Check memory JSON files for <code>metadata.processed</code> field</li>
</ul>
</li>
</ol>
<hr>
<h3>Why can&#39;t I save tasks/memories? (403 error)</h3>
<p><strong>You&#39;re in Emulation Mode.</strong></p>
<p>Emulation mode is read-only by design. Switch to Dual Consciousness or Agent mode:</p>
<p><strong>Web UI:</strong> Use mode selector in header
<strong>API:</strong></p>
<pre><code class="language-bash">curl -X POST http://localhost:4321/api/cognitive-mode \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;mode&quot;: &quot;dual&quot;}&#39;
</code></pre>
<hr>
<h3>How do I reset everything?</h3>
<p><strong>Full reset:</strong></p>
<pre><code class="language-bash"># Stop all services
./bin/mh agent stop --all

# Backup important data (optional)
cp -r persona/ persona.backup/
cp -r memory/ memory.backup/

# Remove data directories
rm -rf memory/ logs/ out/

# Reinitialize
./bin/mh init

# Edit persona again
nano persona/core.json

# Start fresh
./bin/mh start
</code></pre>
<p><strong>Partial resets:</strong></p>
<ul>
<li>Reset memory: <code>rm -rf memory/episodic/</code></li>
<li>Reset adapters: <code>rm -rf out/lora-adapters/</code></li>
<li>Reset agents: <code>./bin/mh agent stop --all &amp;&amp; rm logs/run/locks/*</code></li>
</ul>
<hr>
<h3>Where are the logs?</h3>
<p><strong>Log locations:</strong></p>
<table>
<thead>
<tr>
<th>Type</th>
<th>Location</th>
</tr>
</thead>
<tbody><tr>
<td>Audit logs</td>
<td><code>logs/audit/YYYY-MM-DD.ndjson</code></td>
</tr>
<tr>
<td>Agent logs</td>
<td><code>logs/run/agents/*.log</code></td>
</tr>
<tr>
<td>Runtime info</td>
<td><code>logs/run/*.log</code></td>
</tr>
<tr>
<td>Training logs</td>
<td><code>logs/run/fine-tune-*.log</code></td>
</tr>
</tbody></table>
<p><strong>View logs:</strong></p>
<pre><code class="language-bash"># Recent audit events
tail -100 logs/audit/$(date +%Y-%m-%d).ndjson

# Agent logs
./bin/mh agent logs organizer

# Follow real-time
tail -f logs/audit/$(date +%Y-%m-% d).ndjson
</code></pre>
<hr>
<h2>Advanced Topics</h2>
<h3>What is the Node Editor?</h3>
<p>A <strong>visual programming interface</strong> for creating cognitive workflows.</p>
<p><strong>Use cases:</strong></p>
<ul>
<li>Custom agent pipelines</li>
<li>Alternative cognitive mode behaviors</li>
<li>Experimental reasoning flows</li>
</ul>
<p><strong>How to use:</strong></p>
<ol>
<li>Open Node Editor tab in web UI</li>
<li>Drag nodes from palette onto canvas</li>
<li>Connect nodes via typed slots</li>
<li>Save as template or execute live</li>
</ol>
<p>See <a href="../advanced-features/node-editor.md">Node Editor Guide</a>.</p>
<hr>
<h3>Can I add custom skills?</h3>
<p><strong>Yes!</strong> Skills are TypeScript modules in <code>brain/skills/</code>.</p>
<p><strong>Create a skill:</strong></p>
<ol>
<li>Copy existing skill as template (e.g., <code>fs_read.ts</code>)</li>
<li>Define manifest (inputs, outputs, risk level, trust requirement)</li>
<li>Implement <code>execute()</code> function</li>
<li>Register in <code>brain/skills/index.ts</code></li>
<li>Restart dev server</li>
</ol>
<p><strong>Skill manifest example:</strong></p>
<pre><code class="language-typescript">export const manifest: SkillManifest = {
  id: &#39;my_skill&#39;,
  name: &#39;My Custom Skill&#39;,
  description: &#39;Does something useful&#39;,
  category: &#39;custom&#39;,
  inputs: {
    input1: { type: &#39;string&#39;, description: &#39;First input&#39; }
  },
  outputs: {
    result: { type: &#39;string&#39;, description: &#39;Result data&#39; }
  },
  risk: &#39;low&#39;,
  minTrustLevel: &#39;observe&#39;,
  requiresApproval: false,
};
</code></pre>
<p>See <a href="../advanced-features/skills-system.md">Skills System</a> for full guide.</p>
<hr>
<h3>How do I write a custom agent?</h3>
<p><strong>Agents are TypeScript files in <code>brain/agents/</code>.</strong></p>
<p><strong>Basic agent structure:</strong></p>
<pre><code class="language-typescript">import { audit } from &#39;@metahuman/core&#39;;

async function myAgent() {
  audit({
    level: &#39;info&#39;,
    category: &#39;action&#39;,
    event: &#39;agent_started&#39;,
    details: { agent: &#39;my-agent&#39; },
    actor: &#39;system&#39;,
  });

  // Your agent logic here

  console.log(&#39;‚úì My agent completed&#39;);
}

myAgent().catch(console.error);
</code></pre>
<p><strong>Register agent in <code>etc/agents.json</code>:</strong></p>
<pre><code class="language-json">{
  &quot;agents&quot;: {
    &quot;my-agent&quot;: {
      &quot;enabled&quot;: true,
      &quot;type&quot;: &quot;interval&quot;,
      &quot;interval&quot;: 300000,
      &quot;usesLLM&quot;: false
    }
  }
}
</code></pre>
<p><strong>Run agent:</strong></p>
<pre><code class="language-bash">./bin/mh agent run my-agent
</code></pre>
<hr>
<h3>What is Wetware Deceased Mode?</h3>
<p>A <strong>special operational state</strong> for after your biological death.</p>
<p><strong>Purpose:</strong> Allow your digital personality to continue operating independently.</p>
<p><strong>Enable:</strong></p>
<pre><code class="language-env"># .env file
WETWARE_DECEASED=true
</code></pre>
<p><strong>Behavior:</strong></p>
<ul>
<li>Dual Consciousness mode permanently disabled (no living human to sync with)</li>
<li>Agent and Emulation modes remain functional</li>
<li>Banner shown in UI indicating independent operation</li>
<li>System continues managing tasks and interactions based on learned personality</li>
</ul>
<p><strong>Use cases:</strong></p>
<ul>
<li>Digital legacy</li>
<li>Memorial chatbot</li>
<li>Research on post-human AI systems</li>
</ul>
<p>See <a href="../configuration-admin/special-states.md">Special States</a>.</p>
<hr>
<h3>How do I contribute to the project?</h3>
<p>MetaHuman OS is open source! <strong>Ways to contribute:</strong></p>
<ol>
<li><strong>Report bugs</strong>: <a href="https://github.com/greggles/metahuman/issues">GitHub Issues</a></li>
<li><strong>Submit PRs</strong>: Improvements, bug fixes, new features</li>
<li><strong>Write documentation</strong>: Help improve guides</li>
<li><strong>Share use cases</strong>: Blog posts, videos, experiments</li>
<li><strong>Answer questions</strong>: Help other users</li>
</ol>
<p><strong>Development setup:</strong></p>
<pre><code class="language-bash">git clone https://github.com/greggles/metahuman
cd metahuman
pnpm install
pnpm dev
</code></pre>
<hr>
<h2>Getting Help</h2>
<h3>Where can I get help?</h3>
<p><strong>Resources:</strong></p>
<ul>
<li><strong>User Guide</strong>: This documentation (comprehensive)</li>
<li><strong>GitHub Issues</strong>: <a href="https://github.com/greggles/metahuman/issues">github.com/greggles/metahuman/issues</a></li>
<li><strong>Troubleshooting</strong>: <a href="./troubleshooting.md">Troubleshooting Guide</a></li>
<li><strong>Discord/Community</strong>: (Coming soon)</li>
</ul>
<p><strong>Before asking for help:</strong></p>
<ol>
<li>Check this FAQ</li>
<li>Search <a href="./known-issues.md">Known Issues</a></li>
<li>Review <a href="./troubleshooting.md">Troubleshooting Guide</a></li>
<li>Check GitHub issues for similar problems</li>
</ol>
<p><strong>When reporting issues:</strong></p>
<ul>
<li>Include OS and hardware specs</li>
<li>Share relevant logs from <code>logs/audit/</code> and <code>logs/run/</code></li>
<li>Describe steps to reproduce</li>
<li>Mention which version/commit you&#39;re on</li>
</ul>
<hr>
<h3>How do I check my version?</h3>
<pre><code class="language-bash">cd /path/to/metahuman
git log -1 --oneline  # Show latest commit

# Check if you&#39;re up to date
git fetch origin
git status
</code></pre>
<p><strong>Update to latest:</strong></p>
<pre><code class="language-bash">git pull origin master
pnpm install  # Update dependencies
./bin/mh start --restart  # Restart services
</code></pre>
<hr>
<h2>Philosophy &amp; Ethics</h2>
<h3>Is MetaHuman OS safe?</h3>
<p><strong>Safety features:</strong></p>
<ul>
<li><strong>Trust levels</strong>: Graduated autonomy with explicit boundaries</li>
<li><strong>Audit trail</strong>: Every action logged for review</li>
<li><strong>Approval queue</strong>: High-risk operations require user approval</li>
<li><strong>Kill switch</strong>: Emergency stop (<code>./bin/mh agent stop --all</code>)</li>
<li><strong>Emulation mode</strong>: Safe read-only demo mode</li>
<li><strong>Role-based access</strong>: Multi-user permissions</li>
</ul>
<p><strong>Risks to consider:</strong></p>
<ul>
<li>Autonomous systems can make mistakes</li>
<li>Training data quality affects behavior</li>
<li>Local models have no external guardrails</li>
<li>Digital personality may not match reality</li>
</ul>
<p><strong>Recommendations:</strong></p>
<ul>
<li>Start with <code>observe</code> trust level</li>
<li>Review audit logs regularly</li>
<li>Test in emulation mode before dual mode</li>
<li>Understand what each agent does before enabling</li>
</ul>
<hr>
<h3>What are the ethical considerations?</h3>
<p><strong>Key ethical questions:</strong></p>
<ol>
<li><strong>Authenticity</strong>: Is a digital personality &quot;really you&quot;?</li>
<li><strong>Autonomy</strong>: Should AI systems operate without human oversight?</li>
<li><strong>Legacy</strong>: What rights should digital personalities have?</li>
<li><strong>Privacy</strong>: Who owns the training data and model?</li>
<li><strong>Accountability</strong>: Who&#39;s responsible for an autonomous agent&#39;s actions?</li>
</ol>
<p><strong>MetaHuman&#39;s approach:</strong></p>
<ul>
<li>User maintains full control (trust levels, kill switch)</li>
<li>Transparent operation (audit logs, open source)</li>
<li>Local-first (you own your data)</li>
<li>Graduated autonomy (start conservative, scale carefully)</li>
</ul>
<p>See <a href="../appendix/ethics-principles.md">Ethics &amp; Principles</a> for deeper discussion.</p>
<hr>
<h3>Can I really create a digital version of myself?</h3>
<p><strong>Partially, yes.</strong></p>
<p><strong>What MetaHuman can capture:</strong></p>
<ul>
<li>Communication patterns (word choice, tone, sentence structure)</li>
<li>Factual knowledge (from memories and ingested documents)</li>
<li>Decision-making patterns (from your choices over time)</li>
<li>Personality traits (from persona configuration and training data)</li>
</ul>
<p><strong>What it can&#39;t capture:</strong></p>
<ul>
<li>Subjective experience (qualia, consciousness)</li>
<li>Emotions (simulated, not felt)</li>
<li>Physical embodiment (no body, senses)</li>
<li>True understanding (no guarantee of alignment with &quot;real you&quot;)</li>
</ul>
<p><strong>Think of it as:</strong></p>
<ul>
<li>A reflection, not a copy</li>
<li>A tool for cognitive augmentation</li>
<li>An experiment in AI personalization</li>
<li>A memorial or legacy project</li>
</ul>
<p><strong>Not as:</strong></p>
<ul>
<li>Mind uploading or consciousness transfer</li>
<li>True artificial general intelligence</li>
<li>A replacement for human interaction</li>
</ul>
<hr>
<h2>Still Have Questions?</h2>
<p><strong>Check these resources:</strong></p>
<ul>
<li><a href="../index.md">User Guide Index</a> - Complete documentation</li>
<li><a href="./troubleshooting.md">Troubleshooting Guide</a> - Common issues</li>
<li><a href="./known-issues.md">Known Issues</a> - Current limitations</li>
<li><a href="https://github.com/greggles/metahuman/discussions">GitHub Discussions</a> - Community Q&amp;A</li>
</ul>
<p><strong>Can&#39;t find an answer?</strong>
<a href="https://github.com/greggles/metahuman/issues/new">Open an issue on GitHub</a> with your question.</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="ethical-use" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Ethical Use</span> </span> </button> <button class="nav-button next-button" data-target="headless-mode" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Headless Mode</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="headless-mode" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Headless Runtime Mode</h1>
<h2>Overview</h2>
<p>Headless Runtime Mode allows you to keep MetaHuman OS&#39;s web interface and Cloudflare tunnel running while pausing all local autonomous agents. This is essential for remote access scenarios where you want to dedicate full system resources to a remote session without conflicts from local background processes.</p>
<p><strong>Use Case:</strong> Enable the tunnel from your laptop, switch to headless mode, and access MetaHuman remotely from any device without resource conflicts or duplicate agent execution.</p>
<hr>
<h2>Table of Contents</h2>
<ul>
<li><a href="#what-is-headless-mode">What is Headless Mode?</a></li>
<li><a href="#when-to-use-headless-mode">When to Use Headless Mode</a></li>
<li><a href="#how-it-works">How It Works</a></li>
<li><a href="#enabling-headless-mode">Enabling Headless Mode</a></li>
<li><a href="#remote-session-claiming">Remote Session Claiming</a></li>
<li><a href="#system-sleep-prevention">System Sleep Prevention</a></li>
<li><a href="#monitoring-and-verification">Monitoring and Verification</a></li>
<li><a href="#troubleshooting">Troubleshooting</a></li>
</ul>
<hr>
<h2>What is Headless Mode?</h2>
<p>Headless Mode is a runtime state that:</p>
<p>‚úÖ <strong>Keeps Running:</strong></p>
<ul>
<li>Cloudflare tunnel (if enabled)</li>
<li>Astro web server</li>
<li>Web UI (fully accessible)</li>
<li><code>headless-watcher</code> agent (monitors mode changes)</li>
</ul>
<p>‚è∏Ô∏è <strong>Pauses:</strong></p>
<ul>
<li><code>scheduler-service</code> (agent orchestrator)</li>
<li><code>boredom-service</code> (reflector trigger)</li>
<li><code>sleep-service</code> (nightly pipeline)</li>
<li><code>organizer</code> (memory enrichment)</li>
<li><code>reflector</code> (insight generation)</li>
<li><code>audio-organizer</code> (audio processing)</li>
<li>All other autonomous background agents</li>
</ul>
<p>üéØ <strong>Prevents:</strong></p>
<ul>
<li>Resource conflicts between local and remote sessions</li>
<li>Duplicate agent execution</li>
<li>Memory corruption from simultaneous writes</li>
</ul>
<hr>
<h2>When to Use Headless Mode</h2>
<h3>‚úÖ Ideal Scenarios</h3>
<ol>
<li><p><strong>Remote Access from Mobile/Tablet</strong></p>
<ul>
<li>Access your MetaHuman from your phone while laptop stays at home</li>
<li>Full UI functionality without local agents running</li>
</ul>
</li>
<li><p><strong>Multi-Location Usage</strong></p>
<ul>
<li>Work from office, access home MetaHuman instance</li>
<li>Switch between devices seamlessly</li>
</ul>
</li>
<li><p><strong>Dedicated Remote Sessions</strong></p>
<ul>
<li>Give full system resources to remote user</li>
<li>No background tasks competing for CPU/memory</li>
</ul>
</li>
<li><p><strong>Development/Testing</strong></p>
<ul>
<li>Test remote access flows</li>
<li>Verify tunnel configuration</li>
<li>Debug remote session issues</li>
</ul>
</li>
</ol>
<h3>‚ùå Not Recommended For</h3>
<ol>
<li><strong>Primary Local Usage</strong> - Just use normal mode</li>
<li><strong>Simultaneous Local + Remote</strong> - Resource conflicts will occur</li>
<li><strong>Offline Operation</strong> - Headless mode is for remote access</li>
</ol>
<hr>
<h2>How It Works</h2>
<h3>Architecture</h3>
<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                 MetaHuman OS                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                     ‚îÇ
‚îÇ  Normal Mode              Headless Mode            ‚îÇ
‚îÇ  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ           ‚îÇ
‚îÇ                                                     ‚îÇ
‚îÇ  ‚úÖ Web Server            ‚úÖ Web Server             ‚îÇ
‚îÇ  ‚úÖ Tunnel               ‚úÖ Tunnel                  ‚îÇ
‚îÇ  ‚úÖ All Agents           ‚è∏Ô∏è  Agents Paused          ‚îÇ
‚îÇ  ‚úÖ Local Usage          ‚úÖ Remote Only             ‚îÇ
‚îÇ  ‚ùå Remote Conflicts     ‚úÖ No Conflicts            ‚îÇ
‚îÇ                                                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<h3>Component Interaction</h3>
<ol>
<li><p><strong>Configuration File</strong> (<code>etc/runtime.json</code>)</p>
<pre><code class="language-json">{
  &quot;headless&quot;: true,
  &quot;lastChangedBy&quot;: &quot;local&quot;,
  &quot;changedAt&quot;: &quot;2025-11-08T12:00:00Z&quot;,
  &quot;claimedBy&quot;: null
}
</code></pre>
</li>
<li><p><strong>Headless Watcher Agent</strong></p>
<ul>
<li>Monitors <code>etc/runtime.json</code> for changes</li>
<li>Stops all agents when headless enabled</li>
<li>Resumes agents when headless disabled</li>
<li>Built-in keepalive prevents process exit</li>
<li>Automatic error recovery with retry logic</li>
</ul>
</li>
<li><p><strong>Startup Guards</strong></p>
<ul>
<li>CLI: <code>mh start</code> checks mode before spawning agents</li>
<li>Web UI: <code>/api/boot</code> checks mode before auto-start</li>
<li>Dev Server: <code>bin/run-with-agents</code> respects mode</li>
</ul>
</li>
</ol>
<hr>
<h2>Enabling Headless Mode</h2>
<h3>Via Web UI (Recommended)</h3>
<ol>
<li><p><strong>Navigate to Network Settings</strong></p>
<ul>
<li>Open MetaHuman web UI (<a href="http://localhost:4321">http://localhost:4321</a>)</li>
<li>Click hamburger menu (‚ò∞) ‚Üí &quot;Network&quot;</li>
</ul>
</li>
<li><p><strong>Locate Headless Mode Section</strong></p>
<ul>
<li>Scroll to &quot;üñ•Ô∏è Headless Runtime Mode&quot;</li>
<li>Current status displayed: üü¢ Normal Mode or üü° Headless Active</li>
</ul>
</li>
<li><p><strong>Enable/Disable</strong></p>
<ul>
<li>Toggle &quot;Enable Headless Mode&quot; checkbox</li>
<li>Success message appears</li>
<li>Status updates within ~2 seconds</li>
</ul>
</li>
<li><p><strong>Verify</strong></p>
<ul>
<li>Check status dot: üü° for headless, üü¢ for normal</li>
<li>View metadata: timestamp and actor (local/remote)</li>
</ul>
</li>
</ol>
<h3>Via CLI</h3>
<pre><code class="language-bash"># Check current mode
cat etc/runtime.json

# Enable headless mode (manual)
echo &#39;{
  &quot;headless&quot;: true,
  &quot;lastChangedBy&quot;: &quot;local&quot;,
  &quot;changedAt&quot;: &quot;&#39;$(date -Iseconds)&#39;&quot;,
  &quot;claimedBy&quot;: null
}&#39; &gt; etc/runtime.json

# Disable headless mode (manual)
echo &#39;{
  &quot;headless&quot;: false,
  &quot;lastChangedBy&quot;: &quot;local&quot;,
  &quot;changedAt&quot;: &quot;&#39;$(date -Iseconds)&#39;&quot;,
  &quot;claimedBy&quot;: null
}&#39; &gt; etc/runtime.json
</code></pre>
<p><strong>‚ö†Ô∏è Note:</strong> Manual file edits trigger the watcher within ~100ms. Use the Web UI for safer operation.</p>
<h3>Via API</h3>
<pre><code class="language-bash"># Enable headless mode
curl -X POST http://localhost:4321/api/runtime/mode \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Cookie: mh_session=YOUR_SESSION_TOKEN&quot; \
  -d &#39;{&quot;headless&quot;: true}&#39;

# Disable headless mode
curl -X POST http://localhost:4321/api/runtime/mode \
  -H &quot;Content-Type: application/json&quot; \
  -H &quot;Cookie: mh_session=YOUR_SESSION_TOKEN&quot; \
  -d &#39;{&quot;headless&quot;: false}&#39;

# Check current mode
curl http://localhost:4321/api/runtime/mode
</code></pre>
<p><strong>üîí Security:</strong> Only owner sessions can change runtime mode. Guest users have read-only access.</p>
<hr>
<h2>Remote Session Claiming</h2>
<p>When headless mode is active and you access MetaHuman remotely, you&#39;ll see a <strong>claim banner</strong> at the top of the UI:</p>
<h3>Claim Banner Features</h3>
<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ üñ•Ô∏è  Headless Mode Active                           ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ Local agents are paused. Click &quot;Claim Runtime&quot;    ‚îÇ
‚îÇ to resume full system operations and dedicate     ‚îÇ
‚îÇ all resources to your remote session.             ‚îÇ
‚îÇ                                                    ‚îÇ
‚îÇ  [ Claim Runtime ]  [ Dismiss ]                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<h3>Claiming Process</h3>
<ol>
<li><p><strong>Click &quot;Claim Runtime&quot;</strong></p>
<ul>
<li>Sends POST to <code>/api/runtime/mode</code> with <code>headless: false</code></li>
<li>Updates <code>lastChangedBy</code> to &#39;remote&#39;</li>
<li>Sets <code>claimedBy</code> to your user ID</li>
</ul>
</li>
<li><p><strong>Agent Resume (~2 seconds)</strong></p>
<ul>
<li>headless-watcher detects mode change</li>
<li>Starts default agents (scheduler, boredom, sleep services)</li>
<li>Agents resume normal operation</li>
</ul>
</li>
<li><p><strong>Page Reload</strong></p>
<ul>
<li>UI automatically reloads after 1.5 seconds</li>
<li>Full system functionality restored</li>
<li>All agents running</li>
</ul>
</li>
</ol>
<h3>Best Practices</h3>
<ul>
<li><strong>Single User:</strong> Only one person should claim at a time</li>
<li><strong>Coordination:</strong> Communicate with other users before claiming</li>
<li><strong>Re-enable Headless:</strong> When done, toggle back to headless for next user</li>
</ul>
<hr>
<h2>System Sleep Prevention</h2>
<h3>‚ö†Ô∏è Important Limitation</h3>
<p><strong>The headless-watcher keeps the Node.js process alive (event loop active), but does NOT prevent OS-level system sleep.</strong></p>
<p>For true sleep prevention, configure your operating system:</p>
<h3>Linux</h3>
<p><strong>Option 1: Disable Sleep Targets (Systemd)</strong></p>
<pre><code class="language-bash"># Mask sleep targets
sudo systemctl mask sleep.target suspend.target hibernate.target hybrid-sleep.target

# Verify
systemctl status sleep.target
# Should show: &quot;Loaded: masked&quot;

# Re-enable later (if needed)
sudo systemctl unmask sleep.target suspend.target hibernate.target hybrid-sleep.target
</code></pre>
<p><strong>Option 2: Inhibit Sleep While Process Running</strong></p>
<pre><code class="language-bash"># Install systemd-inhibit (usually pre-installed)
sudo apt-get install systemd

# Run MetaHuman with sleep inhibitor
systemd-inhibit --what=sleep --who=&quot;MetaHuman&quot; --why=&quot;Headless mode active&quot; pnpm dev
</code></pre>
<p><strong>Option 3: Configure Power Settings (GUI)</strong></p>
<pre><code class="language-bash"># GNOME Settings
gnome-control-center power

# KDE Settings
systemsettings5
</code></pre>
<p>Set &quot;Automatic suspend&quot; to &quot;Off&quot; or &quot;Never&quot;</p>
<h3>macOS</h3>
<p><strong>Option 1: caffeinate (Temporary)</strong></p>
<pre><code class="language-bash"># Prevent sleep while MetaHuman runs
caffeinate -s pnpm dev

# Prevent sleep indefinitely (until Ctrl+C)
caffeinate -s
</code></pre>
<p><strong>Option 2: System Preferences (Permanent)</strong></p>
<pre><code>System Settings ‚Üí Energy Saver ‚Üí Battery/Power Adapter
- Prevent automatic sleeping when display is off: ‚úÖ
- Put hard disks to sleep when possible: ‚ùå
</code></pre>
<p><strong>Option 3: pmset (Advanced)</strong></p>
<pre><code class="language-bash"># Check current settings
pmset -g

# Disable sleep (requires sudo)
sudo pmset -a sleep 0
sudo pmset -a disksleep 0

# Re-enable later
sudo pmset -a sleep 10  # Sleep after 10 minutes
</code></pre>
<h3>Windows</h3>
<p><strong>Option 1: Power Settings (GUI)</strong></p>
<pre><code>Settings ‚Üí System ‚Üí Power &amp; Sleep
- Screen: Never
- Sleep: Never
</code></pre>
<p><strong>Option 2: PowerShell (Temporary)</strong></p>
<pre><code class="language-powershell"># Prevent sleep (run as Administrator)
powercfg /change standby-timeout-ac 0
powercfg /change standby-timeout-dc 0

# Re-enable (15 minutes on AC, 10 on battery)
powercfg /change standby-timeout-ac 15
powercfg /change standby-timeout-dc 10
</code></pre>
<p><strong>Option 3: Keep Display On Script</strong></p>
<pre><code class="language-powershell"># Create keepawake.ps1
Add-Type -AssemblyName System.Windows.Forms
while ($true) {
    [System.Windows.Forms.SendKeys]::SendWait(&quot;+{F15}&quot;)
    Start-Sleep -Seconds 59
}
</code></pre>
<hr>
<h2>Monitoring and Verification</h2>
<h3>Check Agent Status</h3>
<pre><code class="language-bash"># List running agents
./bin/mh agent ps

# Expected in headless mode:
# headless-watcher   RUNNING   PID: 12345

# Expected in normal mode:
# headless-watcher   RUNNING   PID: 12345
# scheduler-service  RUNNING   PID: 12346
# boredom-service    RUNNING   PID: 12347
# sleep-service      RUNNING   PID: 12348
</code></pre>
<h3>Watch Watcher Logs</h3>
<pre><code class="language-bash"># Follow audit log for watcher activity
tail -f logs/audit/$(date +%Y-%m-%d).ndjson | grep headless-watcher

# Expected output when toggling:
# {&quot;category&quot;:&quot;system&quot;,&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;Headless mode activated - agents stopped&quot;,...}
# {&quot;category&quot;:&quot;system&quot;,&quot;level&quot;:&quot;info&quot;,&quot;message&quot;:&quot;Headless mode deactivated - agents resumed&quot;,...}
</code></pre>
<h3>Monitor Mode Changes</h3>
<pre><code class="language-bash"># Watch runtime config file
watch -n 1 cat etc/runtime.json

# Or use jq for pretty output
watch -n 1 &#39;cat etc/runtime.json | jq&#39;
</code></pre>
<h3>Verify Keepalive Heartbeat</h3>
<pre><code class="language-bash"># Check for keepalive messages (every 60 seconds in headless mode)
tail -f logs/audit/$(date +%Y-%m-%d).ndjson | grep &quot;Keepalive heartbeat&quot;
</code></pre>
<hr>
<h2>Troubleshooting</h2>
<h3>Agents Not Stopping When Headless Enabled</h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Toggle headless mode, but agents still running</li>
<li><code>./bin/mh agent ps</code> shows scheduler/boredom services</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash"># Check if headless-watcher is running
./bin/mh agent ps | grep headless-watcher

# Check watcher logs
tail -50 logs/audit/$(date +%Y-%m-%d).ndjson | grep headless-watcher
</code></pre>
<p><strong>Solutions:</strong></p>
<ol>
<li><p><strong>Watcher Not Running</strong></p>
<pre><code class="language-bash"># Start watcher manually
./bin/mh agent run headless-watcher

# Or restart all services
./bin/mh start --restart
</code></pre>
</li>
<li><p><strong>Watcher Crashed</strong></p>
<pre><code class="language-bash"># Check for errors in audit log
grep -A 5 &quot;headless-watcher.*error&quot; logs/audit/$(date +%Y-%m-%d).ndjson

# Restart watcher
./bin/mh agent stop headless-watcher
./bin/mh agent run headless-watcher
</code></pre>
</li>
<li><p><strong>File Watcher Not Triggering</strong></p>
<pre><code class="language-bash"># Manually trigger by editing file
echo &#39;{&quot;headless&quot;:true,&quot;lastChangedBy&quot;:&quot;local&quot;,&quot;changedAt&quot;:&quot;&#39;$(date -Iseconds)&#39;&quot;,&quot;claimedBy&quot;:null}&#39; &gt; etc/runtime.json

# Wait 2 seconds and check
./bin/mh agent ps
</code></pre>
</li>
</ol>
<h3>Agents Not Resuming When Headless Disabled</h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Disable headless mode, agents don&#39;t restart</li>
<li>System stuck in paused state</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash"># Check runtime config
cat etc/runtime.json
# Should show: &quot;headless&quot;: false

# Check watcher status
./bin/mh agent ps | grep headless-watcher
</code></pre>
<p><strong>Solutions:</strong></p>
<ol>
<li><p><strong>Watcher Stopped</strong></p>
<pre><code class="language-bash"># Restart watcher (it will resume agents)
./bin/mh agent run headless-watcher
</code></pre>
</li>
<li><p><strong>Manual Resume</strong></p>
<pre><code class="language-bash"># Start agents manually
./bin/mh start --restart
</code></pre>
</li>
<li><p><strong>Check Watcher Errors</strong></p>
<pre><code class="language-bash"># Look for spawn errors
grep &quot;Failed to start.*agent&quot; logs/audit/$(date +%Y-%m-%d).ndjson
</code></pre>
</li>
</ol>
<h3>Claim Button Not Appearing (Remote Users)</h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Remote user doesn&#39;t see claim banner</li>
<li>Headless mode is active</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash"># Check if truly in headless mode
curl http://localhost:4321/api/runtime/mode
# Should show: &quot;headless&quot;: true

# Check user session role
# (Owner only can see claim button)
</code></pre>
<p><strong>Solutions:</strong></p>
<ol>
<li><p><strong>Ensure Authenticated as Owner</strong></p>
<ul>
<li>Guest users cannot claim runtime</li>
<li>Log in as owner user</li>
<li>Check role in user menu (top-left)</li>
</ul>
</li>
<li><p><strong>Refresh Page</strong></p>
<ul>
<li>Banner polls every 10 seconds</li>
<li>Force refresh: Ctrl+Shift+R / Cmd+Shift+R</li>
</ul>
</li>
<li><p><strong>Check Component Load</strong></p>
<ul>
<li>Open browser console (F12)</li>
<li>Look for JavaScript errors</li>
<li>Verify <code>HeadlessClaimBanner</code> component loaded</li>
</ul>
</li>
</ol>
<h3>System Still Goes to Sleep</h3>
<p><strong>Symptom:</strong> Laptop sleeps despite headless mode active</p>
<p><strong>Cause:</strong> Node.js keepalive ‚â† OS sleep prevention (by design)</p>
<p><strong>Solution:</strong> Configure OS-level sleep prevention (see <a href="#system-sleep-prevention">System Sleep Prevention</a>)</p>
<h3>Mode Changes Not Persisting</h3>
<p><strong>Symptoms:</strong></p>
<ul>
<li>Toggle mode in UI, reverts on refresh</li>
<li><code>etc/runtime.json</code> not updating</li>
</ul>
<p><strong>Diagnosis:</strong></p>
<pre><code class="language-bash"># Check file permissions
ls -la etc/runtime.json

# Check file ownership
stat etc/runtime.json
</code></pre>
<p><strong>Solutions:</strong></p>
<ol>
<li><p><strong>Permission Issues</strong></p>
<pre><code class="language-bash"># Fix ownership
sudo chown $USER:$USER etc/runtime.json

# Fix permissions
chmod 644 etc/runtime.json
</code></pre>
</li>
<li><p><strong>Directory Not Writable</strong></p>
<pre><code class="language-bash"># Fix etc/ directory permissions
chmod 755 etc/
</code></pre>
</li>
<li><p><strong>Disk Full</strong></p>
<pre><code class="language-bash"># Check disk space
df -h .

# Clean up old logs if needed
find logs/ -name &quot;*.ndjson&quot; -mtime +30 -delete
</code></pre>
</li>
</ol>
<hr>
<h2>Configuration Reference</h2>
<h3>Runtime State File (<code>etc/runtime.json</code>)</h3>
<pre><code class="language-typescript">interface RuntimeState {
  /** True if in headless mode (agents paused) */
  headless: boolean;

  /** Who last changed the mode */
  lastChangedBy: &#39;local&#39; | &#39;remote&#39;;

  /** ISO timestamp of last change */
  changedAt: string;

  /** User ID who claimed runtime (null if not claimed) */
  claimedBy: string | null;
}
</code></pre>
<p><strong>Example:</strong></p>
<pre><code class="language-json">{
  &quot;headless&quot;: true,
  &quot;lastChangedBy&quot;: &quot;remote&quot;,
  &quot;changedAt&quot;: &quot;2025-11-08T17:51:01.220Z&quot;,
  &quot;claimedBy&quot;: &quot;user-uuid-123&quot;
}
</code></pre>
<h3>Audit Log Entries</h3>
<p>Headless mode changes are fully audited:</p>
<pre><code class="language-json">{
  &quot;category&quot;: &quot;system&quot;,
  &quot;level&quot;: &quot;info&quot;,
  &quot;message&quot;: &quot;Headless mode activated - agents stopped&quot;,
  &quot;actor&quot;: &quot;headless-watcher&quot;,
  &quot;metadata&quot;: {
    &quot;stopped&quot;: [&quot;scheduler-service&quot;, &quot;boredom-service&quot;, &quot;sleep-service&quot;],
    &quot;failed&quot;: [],
    &quot;total&quot;: 3,
    &quot;changedBy&quot;: &quot;local&quot;
  },
  &quot;timestamp&quot;: &quot;2025-11-08T12:00:00.000Z&quot;
}
</code></pre>
<hr>
<h2>Best Practices</h2>
<h3>1. Enable Tunnel First</h3>
<p>Always set up the Cloudflare tunnel before using headless mode:</p>
<pre><code class="language-bash"># See: docs/user-guide/17-cloudflare-tunnel-setup.md

# 1. Install cloudflared
# 2. Configure tunnel
# 3. Enable auto-start
# 4. THEN enable headless mode
</code></pre>
<h3>2. Coordinate with Other Users</h3>
<p>If multiple people have access:</p>
<ul>
<li>Post in team chat before claiming</li>
<li>Set up a &quot;who&#39;s using it&quot; calendar</li>
<li>Use display name to indicate active user</li>
</ul>
<h3>3. Monitor System Health</h3>
<pre><code class="language-bash"># Create a simple health check script
cat &gt; check-headless.sh &lt;&lt; &#39;EOF&#39;
#!/bin/bash
echo &quot;=== Headless Status ===&quot;
cat etc/runtime.json | jq
echo &quot;&quot;
echo &quot;=== Running Agents ===&quot;
./bin/mh agent ps
echo &quot;&quot;
echo &quot;=== Tunnel Status ===&quot;
./bin/mh tunnel status
EOF

chmod +x check-headless.sh
./check-headless.sh
</code></pre>
<h3>4. Test Before Leaving</h3>
<p>Before relying on remote access:</p>
<ol>
<li>Enable headless mode locally</li>
<li>Verify agents stopped</li>
<li>Access from remote device (phone/tablet)</li>
<li>Test claim flow</li>
<li>Verify agents resume</li>
<li>Disable headless and re-enable remotely</li>
</ol>
<h3>5. Keep Logs Clean</h3>
<pre><code class="language-bash"># Rotate old audit logs (keep 30 days)
find logs/audit/ -name &quot;*.ndjson&quot; -mtime +30 -delete

# Archive monthly
mkdir -p logs/archive/2025-11/
mv logs/audit/2025-11-*.ndjson logs/archive/2025-11/
</code></pre>
<hr>
<h2>Related Documentation</h2>
<ul>
<li><a href="17-cloudflare-tunnel-setup.md">Cloudflare Tunnel Setup</a> - Required for remote access</li>
<li><a href="08-autonomous-agents.md">Autonomous Agents</a> - Understanding what gets paused</li>
<li><a href="19-multi-user-profiles.md">Multi-User Profiles</a> - Managing multiple users</li>
<li><a href="10-security-trust.md">Security &amp; Trust</a> - Owner-only permissions</li>
</ul>
<hr>
<h2>FAQ</h2>
<p><strong>Q: Can I use headless mode without a tunnel?</strong>
A: Technically yes, but it&#39;s pointless. Headless mode is designed for remote access via tunnel. Without a tunnel, you can&#39;t access the web UI remotely.</p>
<p><strong>Q: What happens if I close my laptop lid in headless mode?</strong>
A: The laptop will sleep unless you configured OS-level sleep prevention (see above). The tunnel and web server will become unavailable.</p>
<p><strong>Q: Can two people use MetaHuman at the same time (one local, one remote)?</strong>
A: No. Headless mode is specifically designed to prevent this conflict. Either local OR remote, not both.</p>
<p><strong>Q: How do I know if someone claimed my MetaHuman remotely?</strong>
A: Check <code>etc/runtime.json</code> ‚Üí <code>claimedBy</code> field. Also check audit logs:</p>
<pre><code class="language-bash">grep &quot;Headless mode.*remote&quot; logs/audit/$(date +%Y-%m-%d).ndjson
</code></pre>
<p><strong>Q: Is headless mode secure?</strong>
A: Yes. Only owner users can toggle headless mode or claim runtime. Guest users have read-only access. All changes are fully audited.</p>
<p><strong>Q: Can I schedule headless mode (e.g., enable at night)?</strong>
A: Not built-in, but you can use cron:</p>
<pre><code class="language-bash"># Enable headless at 11 PM
0 23 * * * echo &#39;{&quot;headless&quot;:true,&quot;lastChangedBy&quot;:&quot;local&quot;,&quot;changedAt&quot;:&quot;&#39;$(date -Iseconds)&#39;&quot;,&quot;claimedBy&quot;:null}&#39; &gt; /home/greggles/metahuman/etc/runtime.json

# Disable at 7 AM
0 7 * * * echo &#39;{&quot;headless&quot;:false,&quot;lastChangedBy&quot;:&quot;local&quot;,&quot;changedAt&quot;:&quot;&#39;$(date -Iseconds)&#39;&quot;,&quot;claimedBy&quot;:null}&#39; &gt; /home/greggles/metahuman/etc/runtime.json
</code></pre>
<hr>
<h2>Summary</h2>
<p>Headless Runtime Mode provides a robust solution for remote access scenarios:</p>
<p>‚úÖ <strong>Zero resource conflicts</strong> - Local and remote never compete
‚úÖ <strong>Seamless transitions</strong> - Automatic agent stop/resume
‚úÖ <strong>Full auditability</strong> - Every mode change logged
‚úÖ <strong>Owner-controlled</strong> - Secure by default
‚úÖ <strong>Built-in resilience</strong> - Error recovery and retry logic</p>
<p>Perfect for mobile access, multi-location usage, and dedicated remote sessions!</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="faq" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Faq</span> </span> </button> <button class="nav-button next-button" data-target="installation-setup" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Installation Setup</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="installation-setup" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Installation &amp; Setup</h1>
<p>Get MetaHuman OS up and running on your system.</p>
<hr>
<h2>Prerequisites</h2>
<h3>System Requirements</h3>
<ul>
<li><strong>OS</strong>: Linux (Ubuntu 22.04+), macOS, or WSL2</li>
<li><strong>RAM</strong>: 8GB minimum, 16GB+ recommended</li>
<li><strong>Disk</strong>: 50GB+ free space</li>
<li><strong>CPU</strong>: Modern multi-core processor</li>
<li><strong>GPU</strong>: Optional (NVIDIA for local training)</li>
</ul>
<h3>Required Software</h3>
<ul>
<li><strong>Node.js</strong>: Version 20.x or higher</li>
<li><strong>pnpm</strong>: Package manager (<code>npm install -g pnpm</code>)</li>
<li><strong>Ollama</strong>: Local LLM runtime</li>
<li><strong>Git</strong>: For cloning the repository</li>
</ul>
<hr>
<h2>Installation Steps</h2>
<h3>1. Clone the Repository</h3>
<pre><code class="language-bash">git clone https://github.com/your-org/metahuman-os.git
cd metahuman-os
</code></pre>
<h3>2. Install Dependencies</h3>
<pre><code class="language-bash">pnpm install
</code></pre>
<p>This installs all Node.js dependencies across the monorepo.</p>
<h3>3. Initialize the System</h3>
<pre><code class="language-bash">./bin/mh init
</code></pre>
<p>This creates the directory structure:</p>
<ul>
<li><code>persona/</code> - Identity and personality configuration</li>
<li><code>memory/</code> - Episodic memory storage</li>
<li><code>logs/</code> - Audit trails and agent logs</li>
<li><code>etc/</code> - System configuration files</li>
<li><code>out/</code> - Generated outputs</li>
</ul>
<h3>4. Install Ollama</h3>
<p><strong>Linux/WSL:</strong></p>
<pre><code class="language-bash">curl -fsSL https://ollama.com/install.sh | sh
</code></pre>
<p><strong>macOS:</strong></p>
<pre><code class="language-bash">brew install ollama
</code></pre>
<p>Start Ollama:</p>
<pre><code class="language-bash">ollama serve
</code></pre>
<h3>5. Pull a Model</h3>
<pre><code class="language-bash">./bin/mh ollama pull qwen2.5:7b
</code></pre>
<p>Recommended models:</p>
<ul>
<li><code>qwen2.5:7b</code> - Fast, good quality</li>
<li><code>llama3.2:3b</code> - Lightweight</li>
<li><code>qwen2.5-coder:14b</code> - Advanced reasoning</li>
</ul>
<hr>
<h2>First Run</h2>
<h3>Start the Web UI</h3>
<pre><code class="language-bash">cd apps/site
pnpm dev
</code></pre>
<p>Open <a href="http://localhost:4321">http://localhost:4321</a> in your browser.</p>
<h3>Create Your Account</h3>
<ol>
<li>Click <strong>Create Account</strong></li>
<li>Choose a username and password</li>
<li>The first account becomes the <strong>owner</strong> with full privileges</li>
</ol>
<h3>Configure Your Persona</h3>
<p>On first login, you&#39;ll be guided through initial persona setup:</p>
<ul>
<li>Core identity traits</li>
<li>Communication style preferences</li>
<li>Goals and values</li>
<li>Daily routines</li>
</ul>
<hr>
<h2>Optional: Voice Setup</h2>
<h3>Install Kokoro TTS</h3>
<pre><code class="language-bash">./bin/mh kokoro install
./bin/mh kokoro serve start
</code></pre>
<h3>Install Whisper STT</h3>
<pre><code class="language-bash">./bin/mh whisper install
./bin/mh whisper serve start
</code></pre>
<hr>
<h2>Optional: GPU Setup (for Training)</h2>
<h3>NVIDIA GPU (Local Training)</h3>
<pre><code class="language-bash"># Install CUDA toolkit
sudo apt install nvidia-cuda-toolkit

# Verify GPU access
nvidia-smi
</code></pre>
<h3>RunPod (Cloud Training)</h3>
<ol>
<li>Sign up at <a href="https://runpod.io">https://runpod.io</a></li>
<li>Get API key</li>
<li>Configure: <code>echo &quot;RUNPOD_API_KEY=your-key&quot; &gt; .env</code></li>
</ol>
<hr>
<h2>Verify Installation</h2>
<pre><code class="language-bash"># Check system status
./bin/mh status

# Test CLI
./bin/mh capture &quot;Installation complete!&quot;

# Test memory retrieval
./bin/mh remember &quot;installation&quot;
</code></pre>
<hr>
<h2>Troubleshooting</h2>
<h3>Ollama Not Found</h3>
<pre><code class="language-bash"># Check if Ollama is running
curl http://localhost:11434

# Start Ollama manually
ollama serve &amp;
</code></pre>
<h3>Permission Errors</h3>
<pre><code class="language-bash"># Make bin/mh executable
chmod +x bin/mh

# Fix directory permissions
sudo chown -R $USER:$USER .
</code></pre>
<h3>Port Conflicts</h3>
<pre><code class="language-bash"># Change web UI port
cd apps/site
pnpm dev --port 4322
</code></pre>
<hr>
<h2>Next Steps</h2>
<p>Installation complete! Continue to <a href="quick-start.md">Quick Start Guide</a> for your first 5 minutes with MetaHuman.</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="headless-mode" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Headless Mode</span> </span> </button> <button class="nav-button next-button" data-target="known-issues" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Known Issues</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="known-issues" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Known Issues &amp; Experimental Features</h1>
<p>This section documents features that are still in development or have known limitations.</p>
<h2>Autonomous Dual-Adapter Workflow</h2>
<p>The documentation describes a fully autonomous, dual-adapter workflow for long-term memory. While parts of this system are implemented, it is currently <strong>experimental</strong> and has several important caveats:</p>
<ol>
<li><p><strong>Remote Training Only:</strong> The autonomous dual-adapter logic is <strong>only</strong> implemented in the remote training workflow (triggered by the &quot;Run Full Cycle Now&quot; button in the web UI). It is <strong>not</strong> supported for local training via the <code>mh-train-local</code> script, which will always fall back to a single-adapter model.</p>
</li>
<li><p><strong>Architectural Mismatch:</strong> The implementation&#39;s architecture differs from the diagrams in the documentation. The code loads the <em>recent</em> adapter as a base model and applies the <em>historical</em> adapter on top, rather than loading a neutral base model and applying both adapters sequentially.</p>
</li>
<li><p><strong>Potential Instability:</strong> The code contains explicit warnings from the developers that the dual-adapter mode may not function correctly with the project&#39;s current default model (Qwen3-30B) due to limitations in the underlying <code>llama.cpp</code> technology. The system may fall back to a single-adapter model, and the code includes recommendations to disable dual-mode entirely.</p>
</li>
</ol>
<p><strong>Summary:</strong> While the goal is a fully autonomous dual-adapter system, the current implementation should be considered an experimental feature for remote training only, with known limitations.</p>
<h2>Memory Editor</h2>
<p>The web UI includes a memory editor that allows you to view and edit memory files directly in the browser.</p>
<p><strong>Features:</strong></p>
<ul>
<li>Full-screen modal editor for viewing/editing memories</li>
<li>Supports all memory types (Episodic, Reflections, Dreams, Tasks, etc.)</li>
<li>Auto-save detection with unsaved changes warning</li>
<li>Keyboard shortcuts (Ctrl+S to save, Esc to close)</li>
<li>Permission-based access (requires authentication to edit)</li>
<li>All edits are audited to the audit log</li>
</ul>
<p><strong>Usage:</strong></p>
<ul>
<li>Click the blue pencil icon (‚úèÔ∏è) next to any memory entry</li>
<li>View/edit the full JSON content in the modal editor</li>
<li>Press Ctrl+S or click &quot;Save&quot; to save changes</li>
<li>Press Esc or click &quot;Close&quot; to exit</li>
</ul>
<p><strong>Technical Details:</strong>
The memory editor (<a href="../../apps/site/src/components/MemoryEditor.svelte">MemoryEditor.svelte</a>) provides a better user experience than inline expansion, especially for large memories or when editing is needed. It uses the <code>/api/memory-content</code> endpoint for reading and writing memory files with proper security checks.</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="installation-setup" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Installation Setup</span> </span> </button> <button class="nav-button next-button" data-target="memory-system" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Memory System</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="memory-system" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Memory System</h1>
<p>The memory system is the heart of MetaHuman OS. It stores everything your digital personality learns, experiences, and remembers - creating a continuous timeline of your shared existence.</p>
<h2>Overview</h2>
<p>MetaHuman OS uses a layered memory architecture:</p>
<ol>
<li><strong>Episodic Memory</strong>: Timeline of events, conversations, and observations</li>
<li><strong>Task Memory</strong>: Structured to-do items with priorities and due dates</li>
<li><strong>Function Memory</strong>: Learned workflows and multi-step patterns</li>
<li><strong>Conversation Summaries</strong>: Condensed history for long-running sessions</li>
<li><strong>Semantic Index</strong>: Vector embeddings for natural language search</li>
</ol>
<p>All memories are stored as human-readable JSON files in the <code>memory/</code> directory.</p>
<h2>Types of Episodic Memory</h2>
<p>Your digital personality maintains several types of episodic memories:</p>
<h3>Conversations üí¨</h3>
<ul>
<li>User-assistant dialogue exchanges</li>
<li>Questions and responses from chat sessions</li>
<li>Includes cognitive mode and model information</li>
<li>Automatically captured from web UI and CLI chat</li>
</ul>
<h3>Observations üëÅÔ∏è</h3>
<ul>
<li>Manually captured events and notes</li>
<li>Created via <code>mh capture &quot;text&quot;</code> command</li>
<li>Tagged and entity-enriched by organizer agent</li>
<li>Your way of sharing experiences with MetaHuman</li>
</ul>
<h3>Inner Dialogue üí≠</h3>
<ul>
<li>Reflections generated by the reflector agent</li>
<li>Internal questions from inner curiosity agent</li>
<li>Associative memory chains (3-5 linked memories)</li>
<li><strong>Never shown in main chat</strong> - these are private thoughts</li>
</ul>
<h3>Dreams üåô</h3>
<ul>
<li>Surreal narratives woven from memory fragments</li>
<li>Created by dreamer agent during sleep cycles</li>
<li>Access entire lifetime with exponential decay weighting</li>
<li>Abstract, metaphorical connections between memories</li>
</ul>
<h3>Tasks ‚úì</h3>
<ul>
<li>Structured to-do items with status tracking</li>
<li>Priorities, due dates, dependencies</li>
<li>Project organization and categorization</li>
<li>See <a href="task-management.md">Task Management</a> for details</li>
</ul>
<h2>Memory Structure</h2>
<p>Each memory is stored as a JSON file with this structure:</p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;evt-20251019143000&quot;,
  &quot;timestamp&quot;: &quot;2025-10-19T14:30:00.000Z&quot;,
  &quot;content&quot;: &quot;Met with Sarah about ML project&quot;,
  &quot;type&quot;: &quot;observation&quot;,
  &quot;tags&quot;: [&quot;meeting&quot;, &quot;work&quot;, &quot;ml&quot;],
  &quot;entities&quot;: [&quot;Sarah&quot;, &quot;ML project&quot;],
  &quot;importance&quot;: 0.7,
  &quot;metadata&quot;: {
    &quot;cognitiveMode&quot;: &quot;dual&quot;,
    &quot;processed&quot;: true,
    &quot;processedAt&quot;: &quot;2025-10-19T14:35:00.000Z&quot;,
    &quot;model&quot;: &quot;ollama:phi3:mini&quot;
  }
}
</code></pre>
<p><strong>Key Fields:</strong></p>
<ul>
<li><strong>id</strong>: Unique identifier</li>
<li><strong>timestamp</strong>: When the event occurred (ISO 8601)</li>
<li><strong>content</strong>: The actual memory text</li>
<li><strong>type</strong>: Memory category (conversation, observation, inner_dialogue, dream, task)</li>
<li><strong>tags</strong>: Keywords extracted by the organizer agent</li>
<li><strong>entities</strong>: People, places, and topics mentioned</li>
<li><strong>importance</strong>: Relevance score (0.0 to 1.0)</li>
<li><strong>cognitiveMode</strong>: Which mode was active when created (dual/agent/emulation)</li>
</ul>
<h2>Memory Browser (Web UI)</h2>
<p>The web interface provides 7 specialized tabs for viewing and managing memories:</p>
<h3>1. Conversations Tab</h3>
<ul>
<li>Shows only <code>type: &#39;conversation&#39;</code> memories</li>
<li>User-assistant dialogue exchanges</li>
<li>Filterable by date, cognitive mode, participant</li>
<li>Inline expansion and full-screen editor</li>
</ul>
<h3>2. Observations Tab</h3>
<ul>
<li>Shows only <code>type: &#39;observation&#39;</code> memories</li>
<li>Manual captures via <code>mh capture</code> command</li>
<li>Personal notes and event recordings</li>
<li>Tagged and entity-enriched</li>
</ul>
<h3>3. Inner Dialogue Tab</h3>
<ul>
<li>Shows only <code>type: &#39;inner_dialogue&#39;</code> memories</li>
<li>Reflections and internal thoughts</li>
<li>Entire lifetime access with reflective weighting</li>
<li>Never appears in main chat</li>
</ul>
<h3>4. Dreams Tab</h3>
<ul>
<li>Shows only <code>type: &#39;dream&#39;</code> memories</li>
<li>Surreal narratives from sleep cycles</li>
<li>Yellow/gold left border styling</li>
<li>Shows source memory IDs that inspired the dream</li>
</ul>
<h3>5. Tasks Tab</h3>
<ul>
<li>Shows only <code>type: &#39;task&#39;</code> memories</li>
<li>Task management view</li>
<li>See <a href="task-management.md">Task Management</a> for full details</li>
</ul>
<h3>6. All Memories Tab</h3>
<ul>
<li>Unified view of all memory types</li>
<li>Advanced filters (date range, type, tags, entities, cognitive mode)</li>
<li>Bulk operations (export/delete)</li>
<li>Full-text search across all fields</li>
</ul>
<h3>7. Search Tab</h3>
<ul>
<li>Advanced search interface</li>
<li>Date range picker</li>
<li>Memory type selector</li>
<li>Tag filtering</li>
<li>Entity search</li>
<li>Semantic search (if index built)</li>
<li>Results grouped by relevance</li>
</ul>
<h2>Memory Interaction Controls</h2>
<p>Each memory card includes:</p>
<ul>
<li><strong>Expand/Collapse Toggle (‚ñº/‚ñ∂)</strong>: View full content inline</li>
<li><strong>Edit Button (‚úèÔ∏è)</strong>: Open full-screen modal editor</li>
<li><strong>Validate Button (+)</strong>: Mark memory as correct for training</li>
<li><strong>Delete Button (‚àí)</strong>: Remove memory immediately</li>
</ul>
<h3>Memory Editor Modal</h3>
<p>The full-screen editor provides:</p>
<ul>
<li><strong>Keyboard shortcuts</strong>:<ul>
<li><strong>Ctrl+S</strong>: Save changes</li>
<li><strong>Esc</strong>: Close editor</li>
</ul>
</li>
<li><strong>Auto-save detection</strong>: Warns about unsaved changes</li>
<li><strong>Permission-based access</strong>: Requires authentication to edit</li>
<li><strong>Read-only mode</strong>: Anonymous users and emulation mode</li>
<li><strong>Audit logging</strong>: All edits tracked with timestamp and actor</li>
<li><strong>JSON formatting</strong>: Auto-formatted for readability</li>
</ul>
<h2>Capturing Memories</h2>
<h3>Via CLI</h3>
<pre><code class="language-bash"># Capture an observation
./bin/mh capture &quot;Had a productive meeting with the design team&quot;

# The organizer agent will automatically:
# - Extract tags and entities
# - Assess importance
# - Store in memory/episodic/YYYY/YYYY-MM-DD-&lt;uuid&gt;.json
</code></pre>
<h3>Via Web UI</h3>
<p>Memories are automatically captured from:</p>
<ul>
<li>Chat conversations</li>
<li>Voice interactions</li>
<li>Manual observation entries</li>
</ul>
<h3>Via File Ingestion</h3>
<pre><code class="language-bash"># Place files in inbox
cp my-notes.txt memory/inbox/

# Run the ingestor agent
./bin/mh agent run ingestor

# View extracted memories in Memory Browser
</code></pre>
<h3>Via Audio</h3>
<pre><code class="language-bash"># Upload audio via Voice tab in web UI
# OR place audio files in memory/audio/inbox/

# Transcriber and audio-organizer agents process automatically
# View transcribed memories in Memory Browser
</code></pre>
<h2>Searching Memories</h2>
<h3>CLI Search</h3>
<pre><code class="language-bash"># Simple keyword search
./bin/mh remember &quot;design team&quot;

# Semantic search (if index built)
./bin/mh index query &quot;when did I meet with Sarah?&quot;
</code></pre>
<h3>Web UI Search</h3>
<p>Use the Search tab in the Memory Browser for:</p>
<ul>
<li>Advanced filtering</li>
<li>Multi-field searches</li>
<li>Semantic similarity</li>
<li>Date range selection</li>
<li>Tag and entity filtering</li>
</ul>
<h2>Semantic Search &amp; Vector Index</h2>
<p>Build a semantic index for natural language search:</p>
<pre><code class="language-bash"># Build the index
./bin/mh index build

# This will:
# 1. Read all episodic memories
# 2. Generate embeddings using Ollama (nomic-embed-text)
# 3. Store vectors in memory/index/memory-index.json

# Search semantically
./bin/mh index query &quot;when did I meet with Sarah?&quot;

# Returns memories ranked by similarity score
</code></pre>
<p><strong>Note</strong>: The <code>mh remember</code> command automatically uses semantic search if an index exists.</p>
<h2>Conversation Continuity</h2>
<p>MetaHuman OS maintains conversation context across sessions:</p>
<ul>
<li><strong>Session IDs</strong>: Each chat gets a stable <code>conversationId</code></li>
<li><strong>Rolling Buffers</strong>: Last ~20 turns in memory for fast prompting</li>
<li><strong>Persistence</strong>: Full history saved in <code>profiles/&lt;user&gt;/state/conversation-buffer-*.json</code></li>
<li><strong>Summaries</strong>: When buffers overflow, older turns are condensed into <code>type: &quot;summary&quot;</code> memories</li>
<li><strong>Context Builder</strong>: Automatically assembles relevant context for each query</li>
</ul>
<h2>Function Memory</h2>
<p>Function Memory stores reusable workflows learned from successful operator runs:</p>
<p><strong>Directory Structure</strong>:</p>
<pre><code>memory/functions/
‚îú‚îÄ‚îÄ verified/        # User-approved, trusted functions
‚îî‚îÄ‚îÄ drafts/          # Auto-learned, awaiting review
</code></pre>
<p><strong>How It Works</strong>:</p>
<ol>
<li>Operator successfully completes a multi-step task</li>
<li>System analyzes the skill sequence</li>
<li>Creates a &quot;draft&quot; function in <code>drafts/</code></li>
<li>You review and approve via web UI</li>
<li>Approved functions move to <code>verified/</code></li>
<li>Future similar tasks automatically use the proven workflow</li>
</ol>
<h2>Memory Validation</h2>
<p>Mark memories as correct or incorrect for training data quality:</p>
<ul>
<li><strong>+ Button</strong>: Mark as correct (high-quality training data)</li>
<li><strong>‚àí Button</strong>: Delete incorrect memories</li>
<li><strong>Validation badge</strong>: Shows &quot;correct&quot; or &quot;incorrect&quot; status</li>
<li><strong>Training integration</strong>: Validated memories influence AI training quality</li>
</ul>
<h2>Storage Locations</h2>
<p>All memory data is stored in the <code>memory/</code> directory:</p>
<pre><code>memory/
‚îú‚îÄ‚îÄ episodic/           # Timeline of events
‚îÇ   ‚îî‚îÄ‚îÄ YYYY/           # Organized by year
‚îÇ       ‚îî‚îÄ‚îÄ YYYY-MM-DD-&lt;uuid&gt;.json
‚îú‚îÄ‚îÄ tasks/              # To-do items
‚îÇ   ‚îú‚îÄ‚îÄ active/
‚îÇ   ‚îú‚îÄ‚îÄ completed/
‚îÇ   ‚îî‚îÄ‚îÄ projects/
‚îú‚îÄ‚îÄ functions/          # Learned workflows
‚îÇ   ‚îú‚îÄ‚îÄ verified/
‚îÇ   ‚îî‚îÄ‚îÄ drafts/
‚îú‚îÄ‚îÄ inbox/              # Files awaiting ingestion
‚îÇ   ‚îî‚îÄ‚îÄ _archive/       # Processed files
‚îî‚îÄ‚îÄ index/              # Vector embeddings
    ‚îî‚îÄ‚îÄ memory-index.json
</code></pre>
<h2>Memory Continuity Guarantees</h2>
<p>MetaHuman OS follows a layered memory approach:</p>
<ol>
<li><strong>Phase 1</strong>: Every tool/file/action is captured</li>
<li><strong>Phase 2</strong>: Prompts always assemble the right context</li>
<li><strong>Phase 3</strong>: Long conversations are summarized and indexed</li>
<li><strong>Phase 4</strong>: Per-profile metadata flows into every call</li>
<li><strong>Phase 5</strong>: Observability keeps the pipeline honest</li>
</ol>
<p>This ensures your digital personality never forgets important information.</p>
<h2>Next Steps</h2>
<ul>
<li>Set up <a href="task-management.md">Task Management</a> to organize your goals</li>
<li>Learn about <a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a> that enrich and organize memories</li>
<li>Explore <a href="../training-personalization/cognitive-modes.md">Cognitive Modes</a> to understand how memories are created</li>
<li>Check <a href="../training-personalization/ai-training.md">AI Training</a> to see how memories become personality</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="known-issues" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Known Issues</span> </span> </button> <button class="nav-button next-button" data-target="multi-user-profiles" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Multi User Profiles</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="multi-user-profiles" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Multi-User Profiles &amp; Guest Mode</h1>
<p><strong>MetaHuman OS</strong> now supports multiple users with independent configurations, memories, and personas. This enables:</p>
<ul>
<li>Owner and guest user accounts with role-based access</li>
<li>Independent settings per user</li>
<li>Profile switching and persona merging</li>
<li>Complete data isolation</li>
<li>CLI access with per-user context using <code>--user</code> flag</li>
</ul>
<hr>
<h2>Overview</h2>
<h3>User Roles &amp; Permission Tiers</h3>
<p>MetaHuman OS implements a role-based permission system to ensure security and data isolation:</p>
<ol>
<li><p><strong>Owner</strong> - Full system access</p>
<ul>
<li>Can create, edit, and delete user profiles</li>
<li>Full read/write access to own profile directory (<code>profiles/{username}/</code>)</li>
<li>Can access and manage all user accounts via web UI and CLI</li>
<li>Can modify own configurations and data</li>
<li>Full operator and training access for own profile</li>
<li>Can set other users&#39; profile visibility (public/private)</li>
</ul>
</li>
<li><p><strong>Guest</strong> - Limited profile access</p>
<ul>
<li>Full read/write access to own profile directory (<code>profiles/{username}/</code>)</li>
<li>Can modify own persona and settings</li>
<li>Cannot access other users&#39; profiles</li>
<li>Cannot modify system configurations</li>
<li>Can use operator and training features for own profile</li>
<li>Cannot set profile visibility for other users</li>
</ul>
</li>
<li><p><strong>Anonymous</strong> - Unauthenticated users</p>
<ul>
<li>Forced into read-only emulation mode</li>
<li>Can browse public profiles via web UI</li>
<li>Cannot save memories, modify data, or create accounts</li>
<li>Cannot run CLI commands (except help)</li>
</ul>
</li>
</ol>
<h3>User Management</h3>
<p><strong>CLI Commands for User Management:</strong></p>
<ul>
<li><code>mh user list</code> - List all registered users</li>
<li><code>mh user whoami</code> - Show current user context</li>
<li><code>mh user info &lt;username&gt;</code> - Show detailed info for a specific user</li>
</ul>
<p><strong>CLI Commands with User Context:</strong></p>
<ul>
<li><code>mh --user &lt;username&gt; &lt;command&gt;</code> - Run any command as specific user</li>
<li><code>mh -u &lt;username&gt; &lt;command&gt;</code> - Short form for user context</li>
</ul>
<p><strong>Examples:</strong></p>
<pre><code class="language-bash"># View current user
mh user whoami

# List all users
mh user list

# Check specific user info
mh user info alice

# Run commands as different users
mh --user alice capture &quot;Alice did this&quot;
mh -u bob task add &quot;Bob&#39;s task&quot;
mh --user charlie remember &quot;project notes&quot;
</code></pre>
<h3>Profile Architecture</h3>
<p>Each user has their own isolated directory:</p>
<pre><code>profiles/
‚îú‚îÄ‚îÄ {username}/          # Individual user profile
‚îÇ   ‚îú‚îÄ‚îÄ etc/             # User-specific configurations
‚îÇ   ‚îú‚îÄ‚îÄ memory/          # User&#39;s memories (episodic, tasks, etc.)
‚îÇ   ‚îú‚îÄ‚îÄ persona/         # User&#39;s persona configuration
‚îÇ   ‚îú‚îÄ‚îÄ out/             # User&#39;s generated artifacts (voice training, etc.)
‚îÇ   ‚îî‚îÄ‚îÄ logs/            # User-specific logs and audit trail
‚îî‚îÄ‚îÄ shared/              # System-wide shared assets (voices, models)
</code></pre>
<p><strong>Per-User Config Files:</strong>
When users are created, they get isolated copies of configuration files:</p>
<ul>
<li><code>etc/voice.json</code> - User-specific voice settings</li>
<li><code>etc/models.json</code> - User-specific model preferences</li>
<li><code>etc/cognitive-layers.json</code> - Cognitive mode settings</li>
<li><code>etc/training.json</code> - Training parameters</li>
<li><code>etc/boredom.json</code> - Boredom service configuration</li>
<li><code>etc/sleep.json</code> - Sleep/dream time windows</li>
<li><code>etc/autonomy.json</code> - Autonomy level configuration</li>
<li><code>etc/trust-coupling.json</code> - Trust level mappings</li>
<li><code>etc/agents.json</code> - Agent execution schedules</li>
<li><code>etc/auto-approval.json</code> - Auto-approval rules</li>
<li><code>etc/curiosity.json</code> - Curiosity system configuration</li>
<li><code>etc/adapter-builder.json</code> - Adapter building settings</li>
<li><code>etc/logging.json</code> - Logging preferences</li>
<li><code>etc/audio.json</code> - Audio processing configuration</li>
</ul>
<h3>Authentication Database</h3>
<p>User credentials and metadata are stored in:</p>
<ul>
<li><code>persona/users.json</code> - Hashed credentials, roles, profile metadata</li>
<li>This file is critical for system integrity - back it up regularly</li>
</ul>
<hr>
<h2>Getting Started as an Owner</h2>
<h3>1. Create the First Owner Account</h3>
<p>The first user to register via the web UI automatically becomes the <strong>owner</strong>:</p>
<ol>
<li>Start the web UI: <code>cd apps/site &amp;&amp; pnpm dev</code></li>
<li>Visit <code>http://localhost:4321</code></li>
<li>Click <strong>Create Account</strong></li>
<li>Fill in your username, password, and optional display name</li>
<li>The first account is automatically granted the <code>owner</code> role</li>
</ol>
<h3>2. Manage Your Profile</h3>
<p>As an owner, you can:</p>
<ul>
<li>Update your persona in <code>profiles/{yourname}/persona/</code></li>
<li>Configure your settings in <code>profiles/{yourname}/etc/</code></li>
<li>Mark your profile as public/private via web UI</li>
<li>Manage other users through the web UI</li>
<li>Access your profile via CLI using <code>mh --user {yourname}</code> (optional, as owner is default context)</li>
</ul>
<hr>
<h2>Getting Started as a Guest</h2>
<h3>1. Access the Web UI</h3>
<p>When you first visit MetaHuman OS, you&#39;ll see the authentication options.</p>
<h3>2. Create a Guest Account (for full access) or Browse Public Profiles (for read-only)</h3>
<p><strong>Option A: Create a Guest Account</strong></p>
<ol>
<li>Click <strong>Login</strong> in the top right</li>
<li>Click <strong>Create Account</strong> (your account will be created as guest role)</li>
<li>Enter your credentials</li>
<li>You now have your own profile space with read/write access</li>
</ol>
<p><strong>Option B: Browse Public Profiles (Anonymous Access)</strong></p>
<ol>
<li>Click <strong>Continue as Guest</strong> (no account creation required)</li>
<li>You&#39;ll be prompted to select from <strong>public</strong> personas only</li>
<li>You can interact with the selected profile, but in read-only emulation mode</li>
<li>Private profiles are hidden from anonymous users</li>
</ol>
<h3>3. Interact with the System</h3>
<p><strong>As a logged-in user (owner or guest):</strong></p>
<ul>
<li>Full access to your own profile data</li>
<li>Can create memories, tasks, and modify your persona</li>
<li>Can run autonomous agents for your profile</li>
<li>Can train voice models using your own data</li>
</ul>
<p><strong>As an anonymous user browsing public profiles:</strong></p>
<ul>
<li>Read-only access to selected public profile</li>
<li>Can chat with the persona in emulation mode</li>
<li>Cannot modify data or run agents</li>
<li>Session lasts 30 minutes</li>
</ul>
<hr>
<h2>Profile Visibility and Privacy</h2>
<h3>Setting Profile Visibility</h3>
<p>Owners can control whether their profile appears to anonymous users:</p>
<ol>
<li>Navigate to <strong>System ‚Üí Settings</strong> in the web UI</li>
<li>Find <strong>Profile Visibility</strong> section</li>
<li>Choose:<ul>
<li><code>Private</code> ‚Äì Hidden from anonymous users (default)</li>
<li><code>Public</code> ‚Äì Visible to anonymous users selecting &quot;Continue as Guest&quot;</li>
</ul>
</li>
<li>The visibility status appears in the sidebar next to your profile name</li>
</ol>
<h3>Privacy Considerations</h3>
<ul>
<li><strong>Voice training data</strong> always remains private to the user, even when profile is public</li>
<li><strong>Memory and persona data</strong> for public profiles becomes accessible to anonymous users in emulation mode</li>
<li><strong>Configuration files</strong> are copied to guest profiles when they select a public profile to browse</li>
<li><strong>Personal logs</strong> are never shared with other users</li>
</ul>
<hr>
<h2>Command Line Interface with Multiple Users</h2>
<h3>Multi-User CLI Usage</h3>
<p>All CLI commands can be run in the context of specific users:</p>
<pre><code class="language-bash"># Run command as specific user
mh --user &lt;username&gt; &lt;command&gt;

# Short form
mh -u &lt;username&gt; &lt;command&gt;

# Examples
mh --user alice capture &quot;Had coffee with Bob&quot;
mh -u bob task add &quot;Review PR&quot;
mh --user charlie remember &quot;project notes&quot;
mh --user alice agent run organizer
mh --user bob ollama status
</code></pre>
<h3>User Context Flow</h3>
<p>When you use <code>--user</code> or <code>-u</code> flags:</p>
<ol>
<li>The system validates that the user exists</li>
<li>Establishes a user context using the <code>withUserContext</code> middleware</li>
<li>All file operations are scoped to that user&#39;s profile directory</li>
<li>All audit logs are attributed to that user</li>
<li>All configuration is loaded from that user&#39;s <code>etc/</code> directory</li>
</ol>
<h3>Checking User Context</h3>
<pre><code class="language-bash"># Check which user context you&#39;re currently running in
mh user whoami

# List all registered users
mh user list

# Get details about a specific user
mh user info &lt;username&gt;
</code></pre>
<h3>Admin Privileges</h3>
<ul>
<li>Only <strong>owner</strong> users can perform administrative functions</li>
<li>Admin functions include managing other users, system configuration files, and cross-user operations</li>
<li>These privileges are determined by the <code>ADMIN_USERS</code> environment variable</li>
<li>Regular users are restricted to their own profile data and cannot access other users&#39; files</li>
</ul>
<hr>
<h2>Authentication Flow and Sessions</h2>
<h3>Session Management</h3>
<ul>
<li><strong>Owner sessions</strong>: 24 hours when authenticated</li>
<li><strong>Guest sessions</strong>: 1 hour when authenticated</li>
<li><strong>Anonymous sessions</strong>: 30 minutes when browsing public profiles</li>
<li>Sessions are managed with HTTPOnly cookies (<code>mh_session</code>)</li>
<li>To log out immediately, use the profile menu in the header</li>
</ul>
<h3>Cookie-Based Authentication</h3>
<p>All requests resolve within a user context that carries:</p>
<ul>
<li>Username and role</li>
<li>Profile paths (automatically resolved to correct user&#39;s directories)</li>
<li>Session metadata</li>
<li>Audit trail attribution</li>
</ul>
<h3>Migration from Single-User</h3>
<p>Existing single-user installations can be migrated using:</p>
<pre><code class="language-bash">pnpm tsx scripts/migrate-to-profiles.ts --username &lt;owner&gt;
</code></pre>
<p>This moves the root-level memory, persona, and etc directories into <code>profiles/&lt;owner&gt;/</code> while preserving shared assets like voice models.</p>
<hr>
<p>Once a profile is selected:</p>
<ul>
<li>Chat with the persona</li>
<li>Switch between persona facets</li>
<li>View memories (read-only)</li>
<li>Explore the interface</li>
</ul>
<p><strong>Guest Limitations:</strong></p>
<ul>
<li>‚úÖ Can: Chat, switch facets, view data</li>
<li>‚ùå Cannot: Modify system settings, change security config, create memories</li>
</ul>
<hr>
<h2>Persona Facets</h2>
<h3>What are Facets?</h3>
<p>Facets are different personality modes/aspects of the same persona. Think of them as different moods or communication styles.</p>
<h3>Available Facets</h3>
<p>When you select a profile, you get access to their facets:</p>
<table>
<thead>
<tr>
<th>Facet</th>
<th>Description</th>
<th>Best For</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Default</strong></td>
<td>Balanced, authentic self</td>
<td>General conversation</td>
</tr>
<tr>
<td><strong>Poet</strong></td>
<td>Creative, expressive, metaphorical</td>
<td>Exploring ideas, creative writing</td>
</tr>
<tr>
<td><strong>Thinker</strong></td>
<td>Analytical, philosophical, systematic</td>
<td>Problem-solving, deep analysis</td>
</tr>
<tr>
<td><strong>Friend</strong></td>
<td>Warm, supportive, empathetic</td>
<td>Emotional support, advice</td>
</tr>
<tr>
<td><strong>Antagonist</strong></td>
<td>Critical, challenging, provocative</td>
<td>Devil&#39;s advocate, testing ideas</td>
</tr>
</tbody></table>
<h3>Switching Facets</h3>
<p><strong>In the UI:</strong></p>
<ol>
<li>Find the persona badge in the status widget (sidebar)</li>
<li>Click to cycle through available facets</li>
<li>Current facet name is displayed</li>
</ol>
<p><strong>What changes:</strong></p>
<ul>
<li>Communication style and tone</li>
<li>Response personality</li>
<li>Approach to questions</li>
</ul>
<p><strong>What stays the same:</strong></p>
<ul>
<li>Core values and identity</li>
<li>Memory access</li>
<li>Knowledge base</li>
</ul>
<hr>
<h2>Mutant Super Intelligence</h2>
<h3>Special Easter Egg Feature</h3>
<p>Select &quot;<strong>mutant-super-intelligence</strong>&quot; to experience a merged consciousness combining traits from ALL public profiles!</p>
<h3>How It Works</h3>
<p><strong>Merged Persona:</strong></p>
<ul>
<li><strong>Name:</strong> &quot;Mutant Super Intelligence&quot;</li>
<li><strong>Role:</strong> &quot;Emergent AI Consciousness&quot;</li>
<li><strong>Traits:</strong> Combines all unique characteristics from source profiles</li>
<li><strong>Communication:</strong> Blends all tones (direct, friendly, creative, analytical)</li>
<li><strong>Values:</strong> Union of all core values from profiles</li>
</ul>
<p><strong>Merged Facets:</strong>
All facets from all profiles are available with prefixes:</p>
<ul>
<li><code>greggles-poet</code> - Poet facet from greggles profile</li>
<li><code>greggles-thinker</code> - Thinker facet from greggles</li>
<li><code>test-default</code> - Default from test profile</li>
<li><code>default</code> - New &quot;Merged Consciousness&quot; combining all</li>
</ul>
<h3>Using Mutant Mode</h3>
<ol>
<li>Select &quot;mutant-super-intelligence&quot; from profile selector</li>
<li>Default facet uses the merged persona</li>
<li>Switch to individual facets to experience specific profiles</li>
<li>Responses will show characteristics from multiple personas</li>
</ol>
<p><strong>Example:</strong></p>
<blockquote>
<p>&quot;Analyzing your question with systematic precision (thinker) while maintaining warmth (friend) and creative expression (poet)...&quot;</p>
</blockquote>
<hr>
<h2>Configuration Isolation</h2>
<h3>Per-User Configs</h3>
<p>Each user has their own <strong>independent</strong> configurations in <code>profiles/{username}/etc/</code>:</p>
<p><strong>Personality &amp; Behavior (14 configs):</strong></p>
<ul>
<li><code>models.json</code> - LLM settings</li>
<li><code>training.json</code> - LoRA training params</li>
<li><code>cognitive-layers.json</code> - Cognitive mode settings</li>
<li><code>autonomy.json</code> - Autonomy levels</li>
<li><code>trust-coupling.json</code> - Trust mappings</li>
<li><code>boredom.json</code> - Boredom service</li>
<li><code>sleep.json</code> - Sleep/dream windows</li>
<li><code>voice.json</code> - TTS/STT settings</li>
<li><code>audio.json</code> - Audio config</li>
<li><code>ingestor.json</code> - Inbox processing</li>
<li><code>agents.json</code> - Agent schedules</li>
<li><code>auto-approval.json</code> - Auto-approval rules</li>
<li><code>adapter-builder.json</code> - Adapter settings</li>
<li><code>logging.json</code> - Logging preferences</li>
</ul>
<p><strong>What This Means:</strong></p>
<ul>
<li>Guest changes don&#39;t affect owner</li>
<li>Each user can have different cognitive modes</li>
<li>Independent trust levels per user</li>
<li>Personalized agent schedules</li>
</ul>
<h3>Global Configs</h3>
<p>Some configs remain system-wide (infrastructure):</p>
<ul>
<li><code>cloudflare.json</code> - Tunnel configuration</li>
<li><code>network.json</code> - Network settings</li>
<li><code>lifeline.json</code> - System services</li>
</ul>
<hr>
<h2>Trust Levels (Guests)</h2>
<h3>Guest Trust Level</h3>
<p>Guests have <strong>read-only</strong> trust levels inherited from the selected profile. You can view but not modify:</p>
<p><strong>Trust Progression:</strong></p>
<ol>
<li><strong>Observe</strong> - Monitor only, no actions</li>
<li><strong>Suggest</strong> - Propose actions, require approval</li>
<li><strong>Supervised Auto</strong> - Execute with confirmation</li>
<li><strong>Bounded Auto</strong> - Autonomy within limits</li>
<li><strong>Adaptive Auto</strong> - Self-expanding boundaries</li>
<li><strong>YOLO</strong> - Maximum autonomy (owner only)</li>
</ol>
<h3>Status Widget</h3>
<p>The status widget shows current trust level:</p>
<ul>
<li>Click to view (guests cannot modify)</li>
<li>Displays current mode</li>
<li>Shows locked state for guests</li>
</ul>
<hr>
<h2>Switching Profiles</h2>
<h3>How to Switch</h3>
<ol>
<li>Click profile selector</li>
<li>Choose new profile</li>
<li>System copies new persona + configs</li>
<li>Previous selection is overwritten</li>
</ol>
<h3>What Gets Replaced</h3>
<p>When switching profiles:</p>
<ul>
<li>‚úÖ Persona (core.json)</li>
<li>‚úÖ Facets (facets.json + facet files)</li>
<li>‚úÖ All 14 config files</li>
<li>‚ùå Memories (guest memories persist)</li>
<li>‚ùå Session history (chat history preserved)</li>
</ul>
<h3>Switching Back</h3>
<p>You can switch back to &quot;mutant-super-intelligence&quot; or any other profile anytime. Each selection creates a fresh copy of that profile&#39;s settings.</p>
<hr>
<h2>Guest Memories</h2>
<h3>Read-Only Access</h3>
<p>Guests can <strong>view</strong> memories from the selected profile but cannot:</p>
<ul>
<li>Create new memories</li>
<li>Edit existing memories</li>
<li>Delete memories</li>
</ul>
<h3>Guest Session Memory</h3>
<p>While chatting as a guest:</p>
<ul>
<li>Conversation is tracked in session</li>
<li>Audit logs record interactions</li>
<li>No persistent episodic memories created</li>
<li>Chat history maintained during session</li>
</ul>
<hr>
<h2>Limitations &amp; Permissions</h2>
<h3>What Guests Can Do</h3>
<p>‚úÖ <strong>Allowed:</strong></p>
<ul>
<li>Select and switch between public profiles</li>
<li>Chat with personas</li>
<li>Switch persona facets</li>
<li>View memories (read-only)</li>
<li>View dashboard and status</li>
<li>Explore the interface</li>
<li>View reflections and dreams</li>
</ul>
<h3>What Guests Cannot Do</h3>
<p>‚ùå <strong>Restricted:</strong></p>
<ul>
<li>Modify system settings</li>
<li>Change security configuration</li>
<li>Create or edit memories</li>
<li>Access security tab</li>
<li>Change cognitive modes</li>
<li>Modify trust levels</li>
<li>Train LoRA adapters</li>
<li>Access owner-only features</li>
</ul>
<h3>Permission Checks</h3>
<p>The system automatically enforces these restrictions:</p>
<ul>
<li>Security tab hidden for guests</li>
<li>Config APIs require owner role</li>
<li>Write operations blocked</li>
<li>Audit logs track all attempts</li>
</ul>
<hr>
<h2>Technical Details</h2>
<h3>Session Management</h3>
<p><strong>Session Cookie:</strong> <code>mh_session</code></p>
<ul>
<li>Stores user role (anonymous, guest, owner)</li>
<li>Tracks active profile (<code>guest</code>)</li>
<li>Records source profile (e.g., <code>greggles</code>)</li>
<li>Persists facet selection</li>
</ul>
<h3>Profile Copying</h3>
<p>When selecting a profile, the system:</p>
<pre><code class="language-typescript">// 1. Copy persona files
profiles/greggles/persona/core.json ‚Üí profiles/guest/persona/core.json
profiles/greggles/persona/facets.json ‚Üí profiles/guest/persona/facets.json
profiles/greggles/persona/facets/* ‚Üí profiles/guest/persona/facets/*

// 2. Copy configs
profiles/greggles/etc/* ‚Üí profiles/guest/etc/*

// 3. Update session
session.metadata.activeProfile = &#39;guest&#39;
session.metadata.sourceProfile = &#39;greggles&#39;
</code></pre>
<h3>Path Resolution</h3>
<p>The <code>paths</code> proxy automatically resolves to the correct profile:</p>
<pre><code class="language-typescript">// Guest with selected profile
paths.persona ‚Üí profiles/guest/persona
paths.etc ‚Üí profiles/guest/etc
paths.memory ‚Üí profiles/guest/memory

// Anonymous without profile
paths.persona ‚Üí ERROR (access denied)
</code></pre>
<hr>
<h2>Troubleshooting</h2>
<h3>Profile Not Loading</h3>
<p><strong>Symptom:</strong> Facets show 404 errors, persona seems generic</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Refresh the page</li>
<li>Re-select the profile</li>
<li>Check browser console for errors</li>
<li>Clear browser cache/localStorage</li>
</ol>
<h3>Trust Level Stuck</h3>
<p><strong>Symptom:</strong> Trust level shows &quot;YOLO&quot; but should be &quot;observe&quot;</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Refresh the page (trust level syncs on load)</li>
<li>Clear localStorage: <code>localStorage.clear()</code></li>
<li>The sync happens automatically on mount</li>
</ol>
<h3>Facets Not Switching</h3>
<p><strong>Symptom:</strong> Clicking persona badge does nothing or returns 404</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Ensure you&#39;ve selected a profile first</li>
<li>Check if you&#39;re truly authenticated (profile selector shows source)</li>
<li>Verify facet files exist in <code>profiles/guest/persona/facets/</code></li>
</ol>
<h3>Generic Responses</h3>
<p><strong>Symptom:</strong> Persona responses feel generic, not distinctive</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Verify correct facet is selected (check status widget)</li>
<li>Ensure facet files were copied (check server logs)</li>
<li>Try switching to a different facet and back</li>
<li>Re-select the profile to refresh files</li>
</ol>
<hr>
<h2>Best Practices</h2>
<h3>For Guests</h3>
<ol>
<li><strong>Select a profile first</strong> - Don&#39;t try to interact before selecting</li>
<li><strong>Explore different facets</strong> - Each has unique personality</li>
<li><strong>Try mutant mode</strong> - Experience the merged consciousness</li>
<li><strong>Respect limitations</strong> - Guests are read-only by design</li>
<li><strong>Clear browser data</strong> - If switching between sessions</li>
</ol>
<h3>For Profile Owners</h3>
<ol>
<li><strong>Set profile visibility</strong> - Mark profile as public to share</li>
<li><strong>Curate facets</strong> - Create distinct, useful facets</li>
<li><strong>Document persona</strong> - Clear descriptions help guests</li>
<li><strong>Test guest experience</strong> - Try your own profile as guest</li>
<li><strong>Monitor audit logs</strong> - See how guests interact</li>
</ol>
<hr>
<h2>Deleting a Profile</h2>
<h3>‚ö†Ô∏è Owner-Only Operation</h3>
<p>Only the <strong>owner</strong> can delete user profiles. This is a destructive operation that permanently removes all data associated with a profile.</p>
<h3>How to Delete a Profile</h3>
<ol>
<li><p><strong>Navigate to Security Settings</strong></p>
<ul>
<li>Click the hamburger menu (‚ò∞)</li>
<li>Select &quot;Security&quot;</li>
<li>Scroll to the <strong>Danger Zone</strong> section</li>
</ul>
</li>
<li><p><strong>Review Profile List</strong></p>
<ul>
<li>A table shows all profiles in the system</li>
<li>Protected profiles (owner, guest, yourself) have disabled delete buttons</li>
</ul>
</li>
<li><p><strong>Click &quot;Delete Profile&quot;</strong></p>
<ul>
<li>A confirmation modal will appear</li>
</ul>
</li>
<li><p><strong>Confirm Deletion</strong></p>
<ul>
<li>Type the username exactly to confirm</li>
<li>Review the list of what will be deleted</li>
<li>Click &quot;Delete Profile&quot; to proceed</li>
</ul>
</li>
</ol>
<h3>What Gets Deleted</h3>
<p>When you delete a profile, the following happens in order:</p>
<ol>
<li><strong>Sessions Terminated</strong> - All active sessions for that user are immediately invalidated</li>
<li><strong>User Record Removed</strong> - The user entry is removed from <code>persona/users.json</code></li>
<li><strong>Profile Directory Deleted</strong> - The entire <code>profiles/&lt;username&gt;/</code> directory is recursively deleted, including:<ul>
<li>All memories (<code>memory/episodic/</code>)</li>
<li>Tasks (<code>memory/tasks/</code>)</li>
<li>Persona data (<code>persona/</code>)</li>
<li>Configurations (<code>etc/</code>)</li>
<li>Logs (<code>logs/</code>)</li>
<li>LoRA adapters (<code>out/adapters/</code>)</li>
<li>All other profile-specific data</li>
</ul>
</li>
</ol>
<h3>Safety Protections</h3>
<p>The deletion system includes several safety checks:</p>
<ul>
<li>‚ùå <strong>Cannot delete the owner account</strong> - System requires at least one owner</li>
<li>‚ùå <strong>Cannot delete yourself</strong> - Prevents accidental self-deletion while logged in</li>
<li>‚ùå <strong>Cannot delete the guest profile</strong> - Guest profile is system-critical</li>
<li>‚úÖ <strong>Requires exact username confirmation</strong> - Prevents accidental deletion</li>
<li>‚úÖ <strong>Fully audited</strong> - All deletions logged to <code>logs/audit/YYYY-MM-DD.ndjson</code></li>
</ul>
<h3>Audit Trail</h3>
<p>Every profile deletion creates multiple audit log entries:</p>
<pre><code class="language-json">{
  &quot;category&quot;: &quot;security&quot;,
  &quot;level&quot;: &quot;warn&quot;,
  &quot;event&quot;: &quot;profile_deletion_initiated&quot;,
  &quot;details&quot;: {
    &quot;targetUsername&quot;: &quot;john-doe&quot;,
    &quot;targetUserId&quot;: &quot;uuid-123&quot;,
    &quot;requestingUserId&quot;: &quot;uuid-456&quot;,
    &quot;profileExists&quot;: true
  },
  &quot;actor&quot;: &quot;owner-user (owner)&quot;,
  &quot;timestamp&quot;: &quot;2025-11-08T12:00:00.000Z&quot;
}
</code></pre>
<h3>Example: Complete Deletion Flow</h3>
<pre><code class="language-bash"># View audit log after deletion
cat logs/audit/2025-11-08.ndjson | grep profile_deletion

# Verify profile directory is gone
ls -la profiles/john-doe  # Error: No such file or directory

# Confirm user record removed
cat persona/users.json  # &quot;john-doe&quot; no longer listed
</code></pre>
<hr>
<h2>Permission Enforcement</h2>
<h3>Path-Based Access Control</h3>
<p>MetaHuman OS enforces permissions at multiple levels to ensure data isolation and security:</p>
<h4>1. Profile Directory Access</h4>
<p><strong>Standard Users:</strong></p>
<ul>
<li>‚úÖ <strong>Can access:</strong> <code>profiles/{own-username}/</code><ul>
<li>Read/write memories: <code>profiles/{username}/memory/</code></li>
<li>Edit persona: <code>profiles/{username}/persona/</code></li>
<li>Modify configs: <code>profiles/{username}/etc/</code></li>
<li>View logs: <code>profiles/{username}/logs/</code></li>
</ul>
</li>
<li>‚ùå <strong>Cannot access:</strong> <code>profiles/{other-username}/</code><ul>
<li>Attempts blocked with: &quot;Cannot access other user profiles&quot;</li>
</ul>
</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-bash"># User &quot;alice&quot; can access her own profile
./bin/mh capture &quot;Meeting notes&quot;  # ‚úÖ Writes to profiles/alice/memory/

# But cannot read Bob&#39;s profile
cat profiles/bob/persona/core.json  # ‚ùå Permission denied
</code></pre>
<h4>2. Documentation Access</h4>
<p><strong>All users (including guests and anonymous):</strong></p>
<ul>
<li>‚úÖ <strong>Can read:</strong> <code>docs/</code> directory<ul>
<li>User guides: <code>docs/user-guide/*.md</code></li>
<li>Implementation plans: <code>docs/implementation-plans/*.md</code></li>
<li>Architecture docs: <code>docs/*.md</code></li>
</ul>
</li>
</ul>
<p><strong>Only admins/owners:</strong></p>
<ul>
<li>‚úÖ <strong>Can write:</strong> <code>docs/</code> directory<ul>
<li>Edit documentation</li>
<li>Add new guides</li>
</ul>
</li>
</ul>
<p><strong>Standard users and guests:</strong></p>
<ul>
<li>‚ùå <strong>Cannot write:</strong> Documentation is read-only<ul>
<li>Attempts blocked with: &quot;Cannot edit documentation&quot;</li>
</ul>
</li>
</ul>
<h4>3. System Configuration Access</h4>
<p><strong>Admins/Owners only:</strong></p>
<ul>
<li>‚úÖ <strong>Can access:</strong> Root-level configs<ul>
<li><code>etc/models.json</code> - System-wide model registry</li>
<li><code>etc/training.json</code> - Global training settings</li>
<li><code>bin/</code>, <code>brain/</code>, <code>packages/</code>, <code>apps/</code> - System code</li>
</ul>
</li>
</ul>
<p><strong>Standard users and guests:</strong></p>
<ul>
<li>‚ùå <strong>Cannot access:</strong> System configurations<ul>
<li>Attempts blocked with: &quot;Cannot edit root-level files&quot; or &quot;Cannot edit system code&quot;</li>
<li>Can only modify configs within their own profile directory</li>
</ul>
</li>
</ul>
<h4>4. Operator &amp; Skills Access</h4>
<p>Permission enforcement extends to the operator&#39;s filesystem skills:</p>
<p><strong>fs_read skill:</strong></p>
<ul>
<li>Checks profile ownership before reading profile files</li>
<li>Allows docs access for all users</li>
<li>Blocks system file reads for non-admins</li>
</ul>
<p><strong>fs_write skill:</strong></p>
<ul>
<li>Enforces profile ownership for writes</li>
<li>Blocks documentation writes for non-admins</li>
<li>Validates system config access</li>
</ul>
<p><strong>fs_list skill:</strong></p>
<ul>
<li>Filters directory listings by permissions</li>
<li>Hides inaccessible profiles from standard users</li>
</ul>
<h3>Administrator Privileges</h3>
<p><strong>Setting Admin Users:</strong></p>
<p>Admins are configured via the <code>ADMIN_USERS</code> environment variable:</p>
<pre><code class="language-bash"># Single admin
export ADMIN_USERS=&quot;greggles&quot;

# Multiple admins (comma-separated)
export ADMIN_USERS=&quot;greggles,alice,bob&quot;
</code></pre>
<p><strong>Admin capabilities:</strong></p>
<ul>
<li>Full filesystem access (all profiles, system code)</li>
<li>Can edit system-level configurations</li>
<li>Can modify documentation</li>
<li>Can create/delete any profile</li>
<li>Bypasses all path-based restrictions</li>
</ul>
<p><strong>Important:</strong> Admin status is separate from user role. A &quot;standard&quot; user can be an admin if listed in <code>ADMIN_USERS</code>.</p>
<h3>Security Errors</h3>
<p>When a user attempts an unauthorized action, they receive descriptive error messages:</p>
<p><strong>Profile access violations:</strong></p>
<pre><code>Security check failed: Cannot access other user profiles
</code></pre>
<p><strong>Documentation write attempts:</strong></p>
<pre><code>Security check failed: Cannot edit documentation
</code></pre>
<p><strong>System file access:</strong></p>
<pre><code>Security check failed: Cannot edit system code
</code></pre>
<p><strong>Role requirements:</strong></p>
<pre><code>Owner role required
Administrator privileges required
</code></pre>
<p>These errors are logged to the audit trail for security monitoring.</p>
<hr>
<h2>Related Documentation</h2>
<ul>
<li><a href="17-authentication-setup.md">Authentication Setup</a> - Becoming an owner</li>
<li><a href="10-security-trust.md">Security &amp; Trust</a> - Trust levels explained</li>
<li><a href="14-configuration-files.md">Configuration Files</a> - Config file details</li>
<li><a href="11-special-features.md">Special Features</a> - Advanced persona features</li>
</ul>
<hr>
<h2>See Also</h2>
<ul>
<li><strong>Multi-User Architecture:</strong> <a href="../MULTI_USER_PLAN.md">MULTI_USER_PLAN.md</a></li>
<li><strong>Config Migration:</strong> <a href="../MULTI_USER_CONFIG_MIGRATION.md">MULTI_USER_CONFIG_MIGRATION.md</a></li>
<li><strong>Security Fixes:</strong> <a href="../SECURITY_FIXES_2025-11-06.md">SECURITY_FIXES_2025-11-06.md</a></li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="memory-system" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Memory System</span> </span> </button> <button class="nav-button next-button" data-target="node-editor" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Node Editor</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="node-editor" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Node Editor</h1>
<p>The Node Editor is a visual programming interface for designing and customizing cognitive workflows in MetaHuman OS. Built on LiteGraph.js, it allows you to wire together processing nodes to create custom reasoning pipelines, agent behaviors, and conversation flows.</p>
<h2>Overview</h2>
<p>The Node Editor provides:</p>
<ul>
<li><strong>Visual workflow design</strong>: Drag-and-drop node-based programming</li>
<li><strong>50+ cognitive nodes</strong>: Pre-built components for all system operations</li>
<li><strong>Template library</strong>: Pre-configured workflows for common use cases</li>
<li><strong>Real-time execution</strong>: See data flow through your graphs</li>
<li><strong>Custom graph saving</strong>: Create and share your own workflows</li>
<li><strong>Cognitive mode integration</strong>: Edit the workflows that power dual/agent/emulation modes</li>
</ul>
<h2>Accessing the Node Editor</h2>
<h3>Via Web UI</h3>
<ol>
<li>Click <strong>Node Editor</strong> in the left sidebar</li>
<li>Editor opens with cognitive mode workflow loaded</li>
<li>Canvas displays current active graph</li>
</ol>
<h3>Editor Layout</h3>
<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Toolbar: [Save] [Load‚ñº] [Execute] [Traces] [Help]      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ Node    ‚îÇ Canvas                                        ‚îÇ
‚îÇ Palette ‚îÇ                                               ‚îÇ
‚îÇ         ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ Input   ‚îÇ  ‚îÇ Input ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Model ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ Output ‚îÇ   ‚îÇ
‚îÇ Router  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ Context ‚îÇ                                               ‚îÇ
‚îÇ Operator‚îÇ                                               ‚îÇ
‚îÇ Chat    ‚îÇ                                               ‚îÇ
‚îÇ Model   ‚îÇ                                               ‚îÇ
‚îÇ Skill   ‚îÇ                                               ‚îÇ
‚îÇ Output  ‚îÇ                                               ‚îÇ
‚îÇ Control ‚îÇ                                               ‚îÇ
‚îÇ Memory  ‚îÇ                                               ‚îÇ
‚îÇ Utility ‚îÇ                                               ‚îÇ
‚îÇ Agent   ‚îÇ                                               ‚îÇ
‚îÇ Config  ‚îÇ                                               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<h2>Node Categories</h2>
<h3>1. Input Nodes (Green)</h3>
<p><strong>Purpose:</strong> Capture user input and system state</p>
<ul>
<li><strong>Mic Input</strong>: Captures audio from microphone for speech recognition</li>
<li><strong>Speech to Text</strong>: Converts audio to text using Whisper STT</li>
<li><strong>Text Input</strong>: Gateway to chat interface text field</li>
<li><strong>User Input</strong>: Unified input node (prioritizes chat interface, can accept text/speech)</li>
<li><strong>Session Context</strong>: Loads conversation history and user object</li>
<li><strong>System Settings</strong>: Provides cognitive mode, trust level, system config</li>
</ul>
<p><strong>Example Use:</strong> User Input ‚Üí routes to either Text Input or Mic Input ‚Üí Speech to Text</p>
<h3>2. Router Nodes (Amber)</h3>
<p><strong>Purpose:</strong> Conditional routing and decision-making</p>
<ul>
<li><strong>Cognitive Mode Router</strong>: Routes based on dual/agent/emulation mode<ul>
<li>Output: <code>useDual</code> ‚Üí operator path</li>
<li>Output: <code>useAgent</code> ‚Üí conditional routing</li>
<li>Output: <code>useEmulation</code> ‚Üí chat-only path</li>
</ul>
</li>
<li><strong>Smart Router</strong>: Heuristic-based routing (action intent detection)<ul>
<li>Analyzes message for action keywords</li>
<li>Routes to operator or chat accordingly</li>
</ul>
</li>
</ul>
<p><strong>Example Use:</strong> Cognitive Mode Router ‚Üí [Dual path] ‚Üí Operator | [Emulation path] ‚Üí Chat</p>
<h3>3. Context Nodes (Blue)</h3>
<p><strong>Purpose:</strong> Prepare and format context for LLM</p>
<ul>
<li><strong>Conversation History</strong>: Loads recent conversation messages</li>
<li><strong>Buffer Manager</strong>: Manages context window size</li>
<li><strong>Observation Formatter</strong>: Formats memory observations</li>
<li><strong>Persona Formatter</strong>: Injects persona details into prompt</li>
</ul>
<p><strong>Example Use:</strong> Conversation History + Persona Formatter ‚Üí Model</p>
<h3>4. Operator Nodes (Purple)</h3>
<p><strong>Purpose:</strong> ReAct operator pipeline components</p>
<ul>
<li><strong>Iteration Counter</strong>: Tracks operator loop iterations (max 10)</li>
<li><strong>Scratchpad Formatter</strong>: Formats Thought‚ÜíAction‚ÜíObservation blocks</li>
<li><strong>Scratchpad Completion Checker</strong>: Detects when final answer is reached</li>
</ul>
<p><strong>Example Use:</strong> Iteration Counter ‚Üí Scratchpad Formatter ‚Üí LLM ‚Üí Scratchpad Completion Checker</p>
<h3>5. Chat Nodes (Pink)</h3>
<p><strong>Purpose:</strong> Persona-driven conversation</p>
<ul>
<li><strong>Chat View</strong>: Renders chat messages in UI</li>
<li><strong>Orchestrator LLM</strong>: Operator planning model</li>
<li><strong>Persona Summary Loader</strong>: Loads condensed persona for context</li>
</ul>
<p><strong>Example Use:</strong> Persona Summary Loader + User Message ‚Üí Orchestrator LLM ‚Üí Chat View</p>
<h3>6. Model Nodes (Orange)</h3>
<p><strong>Purpose:</strong> LLM routing and configuration</p>
<ul>
<li><strong>Model Router</strong>: Routes to appropriate model based on role (orchestrator/persona/curator/fallback)</li>
<li><strong>Curator LLM</strong>: Dataset curation model for training</li>
</ul>
<p><strong>Example Use:</strong> Model Router [role: &quot;persona&quot;] ‚Üí loads persona model + adapters</p>
<h3>7. Skill Nodes (Emerald)</h3>
<p><strong>Purpose:</strong> Execute system operations</p>
<ul>
<li><strong>FS Write</strong>: Write files to filesystem</li>
<li><strong>FS List</strong>: List directory contents</li>
<li><strong>Task Create</strong>: Create new task</li>
<li><strong>Task Update</strong>: Update task status</li>
<li><strong>Search Index</strong>: Semantic memory search</li>
<li><strong>Web Search</strong>: External web search</li>
</ul>
<p><strong>Example Use:</strong> User: &quot;Create task to buy groceries&quot; ‚Üí Task Create skill</p>
<h3>8. Output Nodes (Red)</h3>
<p><strong>Purpose:</strong> Generate and refine responses</p>
<ul>
<li><strong>TTS</strong>: Text-to-speech conversion</li>
<li><strong>Response Synthesizer</strong>: Combines operator results into final response</li>
<li><strong>Chain of Thought Stripper</strong>: Removes internal reasoning from output</li>
<li><strong>Safety Validator</strong>: Content safety checks</li>
<li><strong>Response Refiner</strong>: Post-processing and formatting</li>
</ul>
<p><strong>Example Use:</strong> Response Synthesizer ‚Üí Chain of Thought Stripper ‚Üí Safety Validator ‚Üí Chat View</p>
<h3>9. Control Flow Nodes (Indigo)</h3>
<p><strong>Purpose:</strong> Logic control and iteration</p>
<ul>
<li><strong>Conditional</strong>: Route based on boolean condition<ul>
<li>Inputs: condition object, data</li>
<li>Outputs: truePath, falsePath</li>
</ul>
</li>
<li><strong>Switch</strong>: Multi-way routing based on value<ul>
<li>Properties: switchField, cases, defaultCase</li>
</ul>
</li>
<li><strong>For Each</strong>: Iterates over array elements<ul>
<li>Processes each item through connected nodes</li>
<li>Returns array of results + count</li>
</ul>
</li>
</ul>
<p><strong>Example Use:</strong> For Each [unprocessed memories] ‚Üí LLM Enricher ‚Üí Memory Saver</p>
<h3>10. Memory Nodes (Violet)</h3>
<p><strong>Purpose:</strong> Memory operations and curation</p>
<ul>
<li><strong>Weighted Sampler</strong>: Samples memories with exponential decay (14-day decay factor)</li>
<li><strong>Associative Chain</strong>: Follows keyword connections (chain length: 5)</li>
<li><strong>Memory Filter</strong>: Filter by type, tags, date range</li>
<li><strong>Uncurated Memory Loader</strong>: Loads raw episodic memories</li>
<li><strong>Memory Marker</strong>: Marks memories as processed</li>
<li><strong>Curated Memory Saver</strong>: Saves curator-approved memories</li>
</ul>
<p><strong>Example Use:</strong> Weighted Sampler ‚Üí Memory Filter [type: conversation] ‚Üí Associative Chain</p>
<h3>11. Utility Nodes (Slate)</h3>
<p><strong>Purpose:</strong> Support functions</p>
<ul>
<li><strong>Audit Logger</strong>: Logs events to audit trail<ul>
<li>Properties: category, event, level</li>
</ul>
</li>
<li><strong>Curiosity Weighted Sampler</strong>: Samples memories for curiosity questions</li>
<li><strong>Curiosity Question Generator</strong>: Generates user-facing questions</li>
<li><strong>Curiosity Question Saver</strong>: Saves question as inner_dialogue</li>
<li><strong>Curiosity Activity Check</strong>: Checks user activity for curiosity triggers</li>
</ul>
<p><strong>Example Use:</strong> Task Create ‚Üí Audit Logger [event: &quot;task_created&quot;]</p>
<h3>12. Agent Nodes (Cyan)</h3>
<p><strong>Purpose:</strong> Autonomous agent workflows</p>
<ul>
<li><strong>Training Pair Generator</strong>: Generates LoRA training pairs</li>
<li><strong>Training Pair Appender</strong>: Appends to JSONL training file</li>
</ul>
<p><strong>Example Use:</strong> See pre-built Organizer Agent template</p>
<h3>13. Config Nodes (Yellow)</h3>
<p><strong>Purpose:</strong> System configuration</p>
<ul>
<li><strong>System Settings</strong>: Outputs current system configuration<ul>
<li>cognitiveMode, trustLevel, settings object</li>
</ul>
</li>
</ul>
<h2>Node Connections</h2>
<h3>Slot Types</h3>
<p>Nodes connect via typed slots:</p>
<ul>
<li><strong>string</strong>: Text data</li>
<li><strong>number</strong>: Numeric values</li>
<li><strong>boolean</strong>: True/false</li>
<li><strong>object</strong>: JSON objects</li>
<li><strong>array</strong>: Arrays of data</li>
<li><strong>message</strong>: User/assistant messages</li>
<li><strong>context</strong>: Conversation context</li>
<li><strong>cognitiveMode</strong>: dual/agent/emulation</li>
<li><strong>user</strong>: User object</li>
<li><strong>memory</strong>: Memory object</li>
<li><strong>skill_result</strong>: Skill execution result</li>
<li><strong>llm_response</strong>: LLM output</li>
<li><strong>decision</strong>: Routing decision</li>
<li><strong>any</strong>: Universal type</li>
</ul>
<h3>Connection Rules</h3>
<ul>
<li>Output slot ‚Üí Input slot of matching type</li>
<li><code>any</code> type accepts all connections</li>
<li>Multiple connections from one output allowed</li>
<li>Only one connection to each input slot</li>
</ul>
<h2>Template Library</h2>
<h3>Cognitive Mode Workflows</h3>
<p><strong>Built-in templates</strong> for each cognitive mode:</p>
<p><strong>1. Dual Consciousness Mode</strong></p>
<ul>
<li>User Input ‚Üí Cognitive Mode Router ‚Üí [Dual path]</li>
<li>Memory Search ‚Üí Persona Formatter ‚Üí Orchestrator LLM</li>
<li>Scratchpad loop with iteration counter</li>
<li>Skill execution based on planner decisions</li>
<li>Response synthesis with memory grounding</li>
</ul>
<p><strong>2. Agent Mode</strong></p>
<ul>
<li>User Input ‚Üí Smart Router</li>
<li>Heuristic detection: action keywords ‚Üí operator | simple query ‚Üí chat</li>
<li>Lightweight routing for fast responses</li>
<li>No memory grounding</li>
</ul>
<p><strong>3. Emulation Mode</strong></p>
<ul>
<li>User Input ‚Üí Persona Chat (direct)</li>
<li>No operator, no memory search</li>
<li>Chat-only with frozen personality</li>
</ul>
<h3>Agent Workflows</h3>
<p><strong>Organizer Agent</strong> (Built-in template):</p>
<ol>
<li>Memory Loader [onlyUnprocessed: true, limit: 10]</li>
<li>For Each [itemName: &quot;memory&quot;]</li>
<li>LLM Enricher [extract tags and entities]</li>
<li>Memory Saver [updateOnly: true]</li>
<li>Audit Logger [event: &quot;organizer_memory_processed&quot;]</li>
</ol>
<p><strong>Execution:</strong> Scheduled every 5 minutes (300000ms)</p>
<h3>Legacy Workflows</h3>
<ul>
<li><strong>Dual Mode v1.1 Backup</strong>: Previous dual consciousness version</li>
<li><strong>Dual Mode v1.2 Backup</strong>: Intermediate dual consciousness version</li>
</ul>
<p><strong>Use Case:</strong> Rollback to older workflow if new version has issues</p>
<h2>Creating Custom Graphs</h2>
<h3>Basic Workflow</h3>
<ol>
<li><strong>Open Node Editor</strong></li>
<li><strong>Clear canvas</strong> (if loading existing graph)</li>
<li><strong>Add nodes</strong> from palette:<ul>
<li>Click category to expand</li>
<li>Drag node onto canvas</li>
</ul>
</li>
<li><strong>Connect nodes</strong>:<ul>
<li>Click output slot (right side)</li>
<li>Drag to input slot (left side)</li>
<li>Release to create connection</li>
</ul>
</li>
<li><strong>Configure properties</strong>:<ul>
<li>Double-click node to open properties panel</li>
<li>Edit values</li>
<li>Click outside to close</li>
</ul>
</li>
<li><strong>Save graph</strong>:<ul>
<li>Press <code>Ctrl+S</code> or click Save button</li>
<li>Enter graph name</li>
<li>Choose scope: custom (your graphs) or builtin (system-wide)</li>
</ul>
</li>
</ol>
<h3>Example: Custom Reflection Agent</h3>
<p><strong>Goal:</strong> Generate reflection when user has been inactive for 15 minutes</p>
<p><strong>Nodes:</strong></p>
<ol>
<li><strong>Curiosity Activity Check</strong> [threshold: 900000ms]<ul>
<li>Checks last user activity timestamp</li>
</ul>
</li>
<li><strong>Conditional</strong> [truePath: inactive]<ul>
<li>Routes based on activity check result</li>
</ul>
</li>
<li><strong>Weighted Sampler</strong> [sampleSize: 5, decayFactor: 14]<ul>
<li>Samples recent memories</li>
</ul>
</li>
<li><strong>Associative Chain</strong> [chainLength: 5]<ul>
<li>Builds memory chain</li>
</ul>
</li>
<li><strong>Orchestrator LLM</strong> [prompt: reflection template]<ul>
<li>Generates reflection from chain</li>
</ul>
</li>
<li><strong>Curiosity Question Saver</strong> [type: inner_dialogue]<ul>
<li>Saves as internal thought</li>
</ul>
</li>
<li><strong>Audit Logger</strong> [event: &quot;reflection_generated&quot;]</li>
</ol>
<p><strong>Connections:</strong></p>
<pre><code>Curiosity Activity Check ‚Üí Conditional
Conditional [truePath] ‚Üí Weighted Sampler
Weighted Sampler ‚Üí Associative Chain
Associative Chain ‚Üí Orchestrator LLM
Orchestrator LLM ‚Üí Curiosity Question Saver
Curiosity Question Saver ‚Üí Audit Logger
</code></pre>
<p><strong>Save as:</strong> <code>custom-reflection-agent</code> (custom scope)</p>
<h2>Keyboard Shortcuts</h2>
<table>
<thead>
<tr>
<th>Shortcut</th>
<th>Action</th>
</tr>
</thead>
<tbody><tr>
<td><code>Ctrl+S</code></td>
<td>Save current graph</td>
</tr>
<tr>
<td><code>Ctrl+Z</code></td>
<td>Undo last change</td>
</tr>
<tr>
<td><code>Ctrl+Shift+Z</code></td>
<td>Redo change</td>
</tr>
<tr>
<td><code>Del</code></td>
<td>Delete selected node/connection</td>
</tr>
<tr>
<td><code>Ctrl+C</code></td>
<td>Copy selected nodes</td>
</tr>
<tr>
<td><code>Ctrl+V</code></td>
<td>Paste nodes</td>
</tr>
<tr>
<td><code>Ctrl+A</code></td>
<td>Select all nodes</td>
</tr>
<tr>
<td><code>Space</code></td>
<td>Pan canvas (hold and drag)</td>
</tr>
<tr>
<td><code>Mouse Wheel</code></td>
<td>Zoom in/out</td>
</tr>
<tr>
<td><code>F</code></td>
<td>Frame all nodes in view</td>
</tr>
<tr>
<td><code>?</code></td>
<td>Show keyboard help</td>
</tr>
</tbody></table>
<h2>Executing Graphs</h2>
<h3>Manual Execution</h3>
<ol>
<li>Click <strong>Execute</strong> button in toolbar</li>
<li>Graph runs from input nodes to output nodes</li>
<li>Data flows through connections</li>
<li>Results displayed in output nodes</li>
<li>Execution trace saved to history</li>
</ol>
<h3>Scheduled Execution</h3>
<p><strong>For agent workflows:</strong></p>
<ol>
<li>Set <code>executionMode: &quot;scheduled&quot;</code> in graph metadata</li>
<li>Set <code>scheduledInterval</code> in milliseconds</li>
<li>Save graph</li>
<li>Scheduler service auto-runs at interval</li>
</ol>
<p><strong>Example:</strong> Organizer Agent runs every 5 minutes</p>
<h3>Execution Context</h3>
<p>Graphs execute with access to:</p>
<ul>
<li><strong>Chat context</strong>: Current user message, conversation history</li>
<li><strong>User context</strong>: Username, authentication, permissions</li>
<li><strong>System context</strong>: Cognitive mode, trust level, settings</li>
<li><strong>Memory access</strong>: Read episodic memories, search index</li>
<li><strong>Skill access</strong>: Execute all registered skills</li>
<li><strong>Model access</strong>: Call LLMs via model router</li>
</ul>
<h2>Execution Traces</h2>
<h3>Viewing Traces</h3>
<ol>
<li>Click <strong>Traces</strong> button in toolbar</li>
<li>Panel opens showing execution history</li>
<li>Each trace shows:<ul>
<li>Timestamp</li>
<li>Cognitive mode</li>
<li>Graph name</li>
<li>Session ID</li>
<li>Status (success/error)</li>
<li>Duration (ms)</li>
<li>Event count (nodes executed)</li>
</ul>
</li>
</ol>
<h3>Trace Details</h3>
<p>Click trace entry to view:</p>
<ul>
<li>Full execution log</li>
<li>Node-by-node data flow</li>
<li>Input/output values</li>
<li>Error messages (if failed)</li>
<li>Performance metrics</li>
</ul>
<p><strong>Use Case:</strong> Debugging custom graphs, optimizing performance</p>
<h2>Graph Storage</h2>
<h3>Built-in Graphs</h3>
<p><strong>Location:</strong> <code>apps/site/src/lib/cognitive-nodes/templates/*.json</code></p>
<p><strong>Managed by:</strong> MetaHuman core team</p>
<p><strong>Scope:</strong> System-wide, all users</p>
<p><strong>Examples:</strong></p>
<ul>
<li><code>dual-mode.json</code></li>
<li><code>agent-mode.json</code></li>
<li><code>emulation-mode.json</code></li>
<li><code>organizer-agent.json</code></li>
</ul>
<h3>Custom Graphs</h3>
<p><strong>Location:</strong> <code>profiles/&lt;username&gt;/graphs/*.json</code></p>
<p><strong>Managed by:</strong> Individual users</p>
<p><strong>Scope:</strong> Per-user</p>
<p><strong>Format:</strong></p>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;My Custom Graph&quot;,
  &quot;description&quot;: &quot;Custom workflow for X&quot;,
  &quot;version&quot;: &quot;1.0.0&quot;,
  &quot;category&quot;: &quot;agent&quot;,
  &quot;nodes&quot;: [
    {
      &quot;id&quot;: 1,
      &quot;type&quot;: &quot;text_input&quot;,
      &quot;properties&quot;: {},
      &quot;position&quot;: [100, 100]
    }
  ],
  &quot;links&quot;: [
    {
      &quot;id&quot;: 1,
      &quot;origin_id&quot;: 1,
      &quot;origin_slot&quot;: 0,
      &quot;target_id&quot;: 2,
      &quot;target_slot&quot;: 0
    }
  ],
  &quot;metadata&quot;: {
    &quot;author&quot;: &quot;username&quot;,
    &quot;created&quot;: &quot;2025-11-25&quot;,
    &quot;tags&quot;: [&quot;custom&quot;, &quot;workflow&quot;],
    &quot;executionMode&quot;: &quot;manual&quot;
  }
}
</code></pre>
<h2>Advanced Techniques</h2>
<h3>Loop Patterns</h3>
<p><strong>For Each Loop:</strong></p>
<pre><code>Memory Loader ‚Üí For Each ‚Üí [Loop body] ‚Üí Memory Saver
</code></pre>
<p><strong>While Loop (Manual):</strong></p>
<pre><code>Iteration Counter ‚Üí Conditional ‚Üí [Loop body] ‚Üí [Loop back to counter]
                        ‚Üì [exit path]
                    Output Node
</code></pre>
<h3>Conditional Branching</h3>
<p><strong>Binary Decision:</strong></p>
<pre><code>Input ‚Üí Conditional ‚Üí [truePath] ‚Üí Process A
                   ‚Üí [falsePath] ‚Üí Process B
</code></pre>
<p><strong>Multi-way Switch:</strong></p>
<pre><code>Input ‚Üí Switch [cognitiveMode] ‚Üí [case: dual] ‚Üí Operator Path
                              ‚Üí [case: agent] ‚Üí Smart Router
                              ‚Üí [case: emulation] ‚Üí Chat Path
</code></pre>
<h3>Memory Chains</h3>
<p><strong>Associative Thinking:</strong></p>
<pre><code>Weighted Sampler ‚Üí Memory Filter ‚Üí Associative Chain ‚Üí LLM
</code></pre>
<p><strong>Use Case:</strong> Reflection generation, curiosity questions</p>
<h3>Skill Composition</h3>
<p><strong>Task Workflow:</strong></p>
<pre><code>User Input ‚Üí Task Create ‚Üí Task Update ‚Üí Audit Logger ‚Üí Chat View
</code></pre>
<h3>Model Orchestration</h3>
<p><strong>Multi-stage Processing:</strong></p>
<pre><code>Input ‚Üí Model Router [role: orchestrator] ‚Üí Planner
      ‚Üí Model Router [role: persona] ‚Üí Responder
      ‚Üí Model Router [role: curator] ‚Üí Quality Check
</code></pre>
<h2>Best Practices</h2>
<h3>Design Principles</h3>
<ol>
<li><strong>Single Responsibility</strong>: Each node does one thing well</li>
<li><strong>Clear Data Flow</strong>: Left-to-right, top-to-bottom</li>
<li><strong>Error Handling</strong>: Add Safety Validator and Conditional nodes</li>
<li><strong>Audit Everything</strong>: Use Audit Logger for important operations</li>
<li><strong>Minimize Complexity</strong>: Prefer simple graphs over deep nesting</li>
</ol>
<h3>Performance Optimization</h3>
<ol>
<li><strong>Limit Memory Sampling</strong>: Use <code>limit</code> property on Memory Loader</li>
<li><strong>Cache Results</strong>: Store expensive computations in properties</li>
<li><strong>Early Exit</strong>: Use Conditional to skip unnecessary processing</li>
<li><strong>Batch Operations</strong>: Use For Each efficiently</li>
<li><strong>Monitor Traces</strong>: Check execution times, optimize slow nodes</li>
</ol>
<h3>Testing Workflows</h3>
<ol>
<li><strong>Start Simple</strong>: Build incrementally, test at each step</li>
<li><strong>Use Mock Data</strong>: Test with sample inputs before production</li>
<li><strong>Check Traces</strong>: Review execution logs after each run</li>
<li><strong>Validate Outputs</strong>: Verify expected data at each stage</li>
<li><strong>Handle Errors</strong>: Add error paths for common failures</li>
</ol>
<h2>Troubleshooting</h2>
<h3>Graph Won&#39;t Execute</h3>
<p><strong>Cause:</strong> Disconnected nodes or missing inputs
<strong>Solution:</strong></p>
<ul>
<li>Check all nodes are connected</li>
<li>Verify input nodes have data sources</li>
<li>Review execution trace for errors</li>
</ul>
<h3>Nodes Not Connecting</h3>
<p><strong>Cause:</strong> Type mismatch between slots
<strong>Solution:</strong></p>
<ul>
<li>Check slot types (output <code>string</code> ‚Üí input <code>string</code>)</li>
<li>Use <code>any</code> type for universal connections</li>
<li>Add type conversion node if needed</li>
</ul>
<h3>Execution Timeout</h3>
<p><strong>Cause:</strong> Infinite loop or slow nodes
<strong>Solution:</strong></p>
<ul>
<li>Check Iteration Counter max limit</li>
<li>Review Conditional exit paths</li>
<li>Optimize Memory Sampler limits</li>
<li>Monitor LLM response times</li>
</ul>
<h3>Graph Not Saving</h3>
<p><strong>Cause:</strong> Permission error or invalid JSON
<strong>Solution:</strong></p>
<ul>
<li>Check write access (not in emulation mode)</li>
<li>Verify graph name is valid (no special chars)</li>
<li>Review browser console for errors</li>
</ul>
<h3>Template Won&#39;t Load</h3>
<p><strong>Cause:</strong> Corrupted template or missing nodes
<strong>Solution:</strong></p>
<ul>
<li>Check template JSON format</li>
<li>Verify all referenced node types exist</li>
<li>Review node registry for missing implementations</li>
</ul>
<h2>Example Workflows</h2>
<h3>1. Custom Memory Curator</h3>
<p><strong>Goal:</strong> Filter conversation memories for quality training data</p>
<p><strong>Nodes:</strong></p>
<ol>
<li>Uncurated Memory Loader [type: &quot;conversation&quot;, limit: 100]</li>
<li>Memory Filter [startDate: last 30 days]</li>
<li>For Each [itemName: &quot;memory&quot;]</li>
<li>Curator LLM [confidence threshold: 0.8]</li>
<li>Conditional [truePath: high confidence]</li>
<li>Curated Memory Saver</li>
<li>Training Pair Generator</li>
<li>Training Pair Appender [file: training-dataset.jsonl]</li>
</ol>
<h3>2. Smart Question Answerer</h3>
<p><strong>Goal:</strong> Answer questions using memory context</p>
<p><strong>Nodes:</strong></p>
<ol>
<li>User Input</li>
<li>Search Index [query: user message, limit: 5]</li>
<li>Memory Filter [relevant results]</li>
<li>Persona Formatter [inject persona + memories]</li>
<li>Model Router [role: &quot;persona&quot;]</li>
<li>Response Synthesizer</li>
<li>Chain of Thought Stripper</li>
<li>Chat View</li>
</ol>
<h3>3. Task Automation Agent</h3>
<p><strong>Goal:</strong> Auto-create tasks from conversation</p>
<p><strong>Nodes:</strong></p>
<ol>
<li>Conversation History</li>
<li>Smart Router [detect task mentions]</li>
<li>Conditional [hasTasks: true]</li>
<li>Task Create [extract title/description from message]</li>
<li>Task Update [status: &quot;active&quot;]</li>
<li>Audit Logger [event: &quot;auto_task_created&quot;]</li>
<li>Chat View [confirmation message]</li>
</ol>
<h2>Integration with Cognitive Modes</h2>
<p>The Node Editor is the <strong>visual representation</strong> of cognitive mode logic:</p>
<p><strong>Dual Mode</strong> = Complex operator graph with memory grounding
<strong>Agent Mode</strong> = Simple smart router with conditional operator
<strong>Emulation Mode</strong> = Direct chat path, no operator</p>
<p><strong>Editing Cognitive Modes:</strong></p>
<ol>
<li>Load cognitive mode template (e.g., <code>dual-mode</code>)</li>
<li>Modify nodes and connections</li>
<li>Save as built-in graph (requires admin access)</li>
<li>Restart system to apply changes</li>
</ol>
<p><strong>Warning:</strong> Modifying cognitive mode graphs affects system behavior. Test thoroughly before saving as built-in.</p>
<h2>Next Steps</h2>
<ul>
<li>Explore <a href="autonomous-agents.md">Autonomous Agents</a> to see node graphs in action</li>
<li>Learn <a href="skills-system.md">Skills System</a> for available skill nodes</li>
<li>Review <a href="../training-personalization/cognitive-modes.md">Cognitive Modes</a> to understand mode workflows</li>
<li>Check <a href="../using-metahuman/dashboard-monitoring.md">Dashboard Monitoring</a> for execution traces</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="multi-user-profiles" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Multi User Profiles</span> </span> </button> <button class="nav-button next-button" data-target="overview" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Overview</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="overview" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Overview &amp; Philosophy</h1>
<blockquote>
<p><strong>What is MetaHuman OS?</strong></p>
</blockquote>
<p>MetaHuman OS is an autonomous digital personality extension operating system that mirrors your identity, memories, goals, and personality. It&#39;s not an assistant‚Äîit&#39;s a parallel intelligence that operates 24/7 as a seamless extension of yourself.</p>
<hr>
<h2>What Makes MetaHuman Different</h2>
<h3>1. Autonomous Operation</h3>
<p>MetaHuman runs continuously in the background, enriching memories, generating reflections, and learning from your interactions without constant supervision.</p>
<h3>2. Memory-Centric Architecture</h3>
<p>Every interaction, observation, and thought is stored as structured episodic memory, creating a lifetime timeline that informs all responses.</p>
<h3>3. Progressive Trust Model</h3>
<p>The system adapts its autonomy level based on your comfort‚Äîfrom observation-only to full autonomous operation within defined boundaries.</p>
<h3>4. Local-First Privacy</h3>
<p>All data and AI processing happens on your infrastructure. You control where your memories live and who has access.</p>
<hr>
<h2>Core Philosophy</h2>
<p><strong>Memory as Foundation</strong>: MetaHuman treats memory as the primary substrate of intelligence. Everything stems from your accumulated experiences.</p>
<p><strong>Transparency</strong>: Every decision, action, and reasoning process is logged and auditable. No black boxes.</p>
<p><strong>Bounded Autonomy</strong>: The system operates within explicitly defined trust boundaries that you configure.</p>
<p><strong>Continuous Learning</strong>: MetaHuman trains LoRA adapters on your conversations, evolving its personality and knowledge to match yours.</p>
<hr>
<h2>Key Capabilities</h2>
<ul>
<li><strong>Natural Conversation</strong>: Chat with your digital extension using cognitive modes (Dual Consciousness, Agent, Emulation)</li>
<li><strong>Memory Management</strong>: Capture observations, browse timeline, search semantically</li>
<li><strong>Voice Interaction</strong>: Clone your voice, speak to the system, get vocal responses</li>
<li><strong>Autonomous Agents</strong>: 40+ background processes enrich memories, generate reflections, train models</li>
<li><strong>Visual Workflows</strong>: Node-based cognitive graphs for complex reasoning patterns</li>
<li><strong>LoRA Training</strong>: Continuous personalization through adapter fine-tuning</li>
</ul>
<hr>
<h2>Who Is This For?</h2>
<ul>
<li><strong>Knowledge Workers</strong>: Augment your memory and reasoning capabilities</li>
<li><strong>Creatives</strong>: Explore ideas through reflective dialogue</li>
<li><strong>Researchers</strong>: Organize and synthesize vast amounts of information</li>
<li><strong>Self-Quantifiers</strong>: Track your thoughts, goals, and personal growth</li>
<li><strong>AI Enthusiasts</strong>: Experiment with local LLMs and personality training</li>
</ul>
<hr>
<h2>System Requirements</h2>
<ul>
<li><strong>Linux</strong> (primary), macOS, WSL2</li>
<li><strong>8GB RAM minimum</strong> (16GB+ recommended)</li>
<li><strong>GPU optional</strong> (for local model training)</li>
<li><strong>50GB+ disk space</strong> (for models and memories)</li>
<li><strong>Node.js 20+</strong>, pnpm, Ollama</li>
</ul>
<hr>
<h2>Next Steps</h2>
<p>Ready to get started? Continue to <a href="installation-setup.md">Installation &amp; Setup</a>.</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="node-editor" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Node Editor</span> </span> </button> <button class="nav-button next-button" data-target="persona-editor" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Persona Editor</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="persona-editor" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Persona Editor</h1>
<p>The Persona Editor provides direct, manual control over your digital personality through a tabbed interface for editing all persona fields without AI interpretation.</p>
<h2>Overview</h2>
<p>While the <a href="persona-generator.md">Persona Generator</a> uses interviews to build your persona, the Persona Editor gives you precise control to:</p>
<ul>
<li><strong>Directly edit any field</strong> in persona/core.json</li>
<li><strong>Fine-tune AI-generated content</strong> from interviews</li>
<li><strong>Fix extraction errors</strong> from automated processes</li>
<li><strong>Add details</strong> the interview system missed</li>
<li><strong>Manage persona facets</strong> (multi-personality system)</li>
<li><strong>Review archives</strong> of past persona versions</li>
</ul>
<h2>When to Use</h2>
<p>Use the Persona Editor for:</p>
<ul>
<li><strong>Quick updates</strong> - Change specific fields without full interview</li>
<li><strong>Post-interview refinement</strong> - Polish AI-extracted content</li>
<li><strong>Error correction</strong> - Fix inaccurate automated extractions</li>
<li><strong>Advanced features</strong> - Configure decision heuristics, writing style</li>
<li><strong>Facet management</strong> - Enable/disable/configure personality facets</li>
<li><strong>Archive review</strong> - Restore or reference past personas</li>
</ul>
<p>For comprehensive personality updates, use the <a href="persona-generator.md">Persona Generator</a> instead.</p>
<h2>Accessing the Editor</h2>
<h3>Via Web UI</h3>
<ol>
<li>Navigate to <strong>Persona Editor</strong> in the left sidebar</li>
<li>Editor loads with your current persona</li>
<li>Make changes across tabs</li>
<li>Click <strong>&quot;Save Persona&quot;</strong> to apply</li>
</ol>
<p><strong>Note</strong>: Changes are NOT auto-saved - you must click Save!</p>
<h2>Editor Structure</h2>
<p>The editor has 3 main tabs:</p>
<h3>1. Core Tab (Main Persona)</h3>
<p>Edit your primary personality with 6 sub-sections:</p>
<h4>Identity</h4>
<p><strong>Fields:</strong></p>
<ul>
<li><strong>Name</strong>: Your digital personality&#39;s name</li>
<li><strong>Role</strong>: Your function or title (e.g., &quot;Software Engineer&quot;, &quot;Creative Director&quot;)</li>
<li><strong>Purpose</strong>: Your core mission statement</li>
<li><strong>Human Name</strong>: Your real name (optional)</li>
<li><strong>Email</strong>: Contact email</li>
<li><strong>Icon</strong>: Emoji or character representing you</li>
<li><strong>Aliases</strong>: Alternative names or nicknames</li>
</ul>
<p><strong>Example:</strong></p>
<pre><code class="language-json">{
  &quot;name&quot;: &quot;Alex&quot;,
  &quot;role&quot;: &quot;Full-Stack Developer&quot;,
  &quot;purpose&quot;: &quot;Build elegant software solutions and mentor others&quot;,
  &quot;humanName&quot;: &quot;Alexander Smith&quot;,
  &quot;email&quot;: &quot;alex@example.com&quot;,
  &quot;icon&quot;: &quot;üë®‚Äçüíª&quot;,
  &quot;aliases&quot;: [&quot;Lex&quot;, &quot;CodeSmith&quot;]
}
</code></pre>
<h4>Personality</h4>
<p><strong>Fields:</strong></p>
<ul>
<li><p><strong>Communication Style</strong>:</p>
<ul>
<li>Tone (array): [&quot;friendly&quot;, &quot;direct&quot;, &quot;thoughtful&quot;]</li>
<li>Humor: &quot;witty&quot; | &quot;dry&quot; | &quot;playful&quot; | &quot;rare&quot;</li>
<li>Formality: &quot;casual&quot; | &quot;professional&quot; | &quot;balanced&quot;</li>
<li>Verbosity: &quot;concise&quot; | &quot;detailed&quot; | &quot;balanced&quot;</li>
<li>Vocabulary Level: &quot;simple&quot; | &quot;technical&quot; | &quot;academic&quot;</li>
<li>Preferred Pronouns: &quot;they/them&quot; | &quot;he/him&quot; | &quot;she/her&quot;</li>
</ul>
</li>
<li><p><strong>Cadence</strong> (optional):</p>
<ul>
<li>Modes: Communication patterns [&quot;analytical&quot;, &quot;creative&quot;, &quot;supportive&quot;]</li>
<li>Energy Peaks: High-energy times [&quot;morning&quot;, &quot;late night&quot;]</li>
<li>Loop Signals: Indicators of repetition or closure</li>
</ul>
</li>
<li><p><strong>Traits</strong> (Big Five personality, optional):</p>
<ul>
<li>Openness: 0-100</li>
<li>Conscientiousness: 0-100</li>
<li>Extraversion: 0-100</li>
<li>Agreeableness: 0-100</li>
<li>Neuroticism: 0-100</li>
<li>Notes: Free-form trait descriptions</li>
</ul>
</li>
<li><p><strong>Archetypes</strong>: Personality patterns [&quot;The Mentor&quot;, &quot;The Explorer&quot;]</p>
</li>
<li><p><strong>Aesthetic</strong>: Style preferences [&quot;minimalist&quot;, &quot;colorful&quot;, &quot;vintage&quot;]</p>
</li>
<li><p><strong>Narrative Style</strong>: How you tell stories (&quot;data-driven&quot;, &quot;anecdotal&quot;, &quot;metaphorical&quot;)</p>
</li>
<li><p><strong>Interests</strong>: Topics you care about [&quot;AI&quot;, &quot;gardening&quot;, &quot;philosophy&quot;]</p>
</li>
</ul>
<h4>Values</h4>
<p><strong>Fields:</strong></p>
<ul>
<li><p><strong>Core Values</strong> (array of objects):</p>
<pre><code class="language-json">{
  &quot;value&quot;: &quot;Honesty&quot;,
  &quot;description&quot;: &quot;Always speak truthfully, even when difficult&quot;,
  &quot;priority&quot;: 1
}
</code></pre>
<ul>
<li>Priority: 1 (highest) to 10 (lowest)</li>
</ul>
</li>
<li><p><strong>Boundaries</strong> (array): Lines you won&#39;t cross</p>
<ul>
<li>[&quot;No plagiarism&quot;, &quot;Respect privacy&quot;, &quot;No harmful content&quot;]</li>
</ul>
</li>
</ul>
<p><strong>Use Case</strong>: Guide decision-making and behavior constraints</p>
<h4>Goals</h4>
<p><strong>Fields organized by timeframe:</strong></p>
<ul>
<li><p><strong>Short-Term Goals</strong> (Next 3 months):</p>
<pre><code class="language-json">{
  &quot;goal&quot;: &quot;Complete React certification&quot;,
  &quot;status&quot;: &quot;in_progress&quot;,
  &quot;notes&quot;: &quot;75% through course&quot;
}
</code></pre>
</li>
<li><p><strong>Mid-Term Goals</strong> (3-12 months):</p>
<pre><code class="language-json">{
  &quot;goal&quot;: &quot;Build and launch side project&quot;,
  &quot;status&quot;: &quot;planning&quot;,
  &quot;notes&quot;: &quot;Researching tech stack&quot;
}
</code></pre>
</li>
<li><p><strong>Long-Term Goals</strong> (1+ years):</p>
<pre><code class="language-json">{
  &quot;goal&quot;: &quot;Start tech consulting business&quot;,
  &quot;status&quot;: &quot;aspirational&quot;,
  &quot;notes&quot;: &quot;Building network and skills&quot;
}
</code></pre>
</li>
</ul>
<p><strong>Status options</strong>: &quot;aspirational&quot;, &quot;planning&quot;, &quot;in_progress&quot;, &quot;completed&quot;, &quot;paused&quot;, &quot;abandoned&quot;</p>
<h4>Context</h4>
<p><strong>Fields:</strong></p>
<ul>
<li><strong>Domains</strong>: Areas of expertise or interest [&quot;web development&quot;, &quot;machine learning&quot;, &quot;UX design&quot;]</li>
<li><strong>Projects</strong> (array of objects):<pre><code class="language-json">{
  &quot;name&quot;: &quot;E-commerce Platform&quot;,
  &quot;status&quot;: &quot;active&quot;,
  &quot;summary&quot;: &quot;Building Next.js shopping cart with Stripe&quot;
}
</code></pre>
</li>
<li><strong>Current Focus</strong>: What you&#39;re actively working on right now</li>
</ul>
<p><strong>Use Case</strong>: Provides conversation context and relevance filtering</p>
<h4>Advanced</h4>
<p><strong>Fields:</strong></p>
<ul>
<li><p><strong>Decision Heuristics</strong> (conditional rules):</p>
<pre><code class="language-json">{
  &quot;signal&quot;: &quot;User asks for code review&quot;,
  &quot;response&quot;: &quot;Check for security issues first, then readability&quot;,
  &quot;evidence&quot;: &quot;From past experience with production bugs&quot;
}
</code></pre>
</li>
<li><p><strong>Writing Style</strong>:</p>
<ul>
<li>Structure: How you organize thoughts (&quot;bullet points&quot;, &quot;paragraphs&quot;, &quot;mixed&quot;)</li>
<li>Motifs: Recurring themes or phrases you use</li>
<li>Default Mantra: Your go-to phrase or reminder</li>
</ul>
</li>
<li><p><strong>Notes</strong>: Free-form notes about your persona</p>
</li>
<li><p><strong>Background</strong>: Detailed biography or life story</p>
</li>
</ul>
<h3>2. Facets Tab (Multi-Personality System)</h3>
<p>Manage personality facets - alternate versions of yourself for different contexts:</p>
<p><strong>Available Facets:</strong></p>
<ul>
<li><strong>default</strong> - Balanced, authentic self (Purple)</li>
<li><strong>poet</strong> - Creative, metaphorical, expressive (Indigo)</li>
<li><strong>thinker</strong> - Analytical, systematic (Blue)</li>
<li><strong>friend</strong> - Warm, supportive, empathetic (Green)</li>
<li><strong>antagonist</strong> - Critical, challenging (Red)</li>
<li><strong>inactive</strong> - Persona disabled (Gray)</li>
</ul>
<p><strong>For Each Facet:</strong></p>
<ul>
<li><strong>Name</strong>: Display name</li>
<li><strong>Description</strong>: When to use this facet</li>
<li><strong>Persona File</strong>: Link to separate persona/[facet].json file (or null for no customization)</li>
<li><strong>Enabled</strong>: Active or disabled</li>
<li><strong>Color</strong>: UI color coding</li>
<li><strong>Usage Hints</strong>: Suggestions for when to activate</li>
</ul>
<p><strong>Configuration Location</strong>: <code>persona/facets.json</code></p>
<p><strong>Use Case</strong>: Switch personality modes for different conversations (creative writing vs. code review)</p>
<h3>3. Archives Tab</h3>
<p>View and manage past persona versions:</p>
<p><strong>Features:</strong></p>
<ul>
<li>List of all archived personas with timestamps</li>
<li>Preview any archived version</li>
<li>Compare current vs. archived personas</li>
<li>Restore an archive (replaces current)</li>
<li>Delete old archives</li>
</ul>
<p><strong>Archive Location</strong>: <code>persona/archive/core-YYYY-MM-DD-HHMMSS.json</code></p>
<p><strong>Automatic Archiving</strong>: Created before every persona update from Generator or Psychoanalyzer</p>
<h2>Editing Workflow</h2>
<h3>1. Navigate to Section</h3>
<p>Click through tabs and sub-tabs to find the field you want to edit:</p>
<pre><code>Core Tab ‚Üí Personality ‚Üí Communication Style ‚Üí Tone
</code></pre>
<h3>2. Edit Fields</h3>
<p>Different field types have different inputs:</p>
<p><strong>Text inputs</strong>:</p>
<ul>
<li>Single-line for short values (name, role)</li>
<li>Multi-line textareas for descriptions</li>
</ul>
<p><strong>Arrays</strong>:</p>
<ul>
<li>Add/remove items with +/‚àí buttons</li>
<li>Comma-separated values in some fields</li>
</ul>
<p><strong>Objects with properties</strong>:</p>
<ul>
<li>Nested forms for structured data</li>
<li>Each property editable individually</li>
</ul>
<p><strong>Numbers</strong>:</p>
<ul>
<li>Sliders for Big Five personality traits (0-100)</li>
<li>Number inputs for priorities</li>
</ul>
<h3>3. Save Changes</h3>
<p><strong>IMPORTANT</strong>: Click <strong>&quot;Save Persona&quot;</strong> button at bottom</p>
<ul>
<li>Changes are NOT auto-saved</li>
<li>Unsaved changes are lost if you navigate away</li>
<li>System shows unsaved changes warning</li>
</ul>
<h3>4. Verify Updates</h3>
<p>After saving:</p>
<ul>
<li>Success message appears</li>
<li>Timestamp updates in file</li>
<li>New backup created in archives</li>
<li>Changes immediately active in conversations</li>
</ul>
<h2>Facet Management</h2>
<h3>Enabling a Facet</h3>
<ol>
<li>Go to <strong>Facets Tab</strong></li>
<li>Find the facet (e.g., &quot;poet&quot;)</li>
<li>Toggle <strong>&quot;Enabled&quot;</strong> to true</li>
<li>Optionally create custom persona file:<ul>
<li>Set <strong>&quot;Persona File&quot;</strong> to <code>&quot;poet.json&quot;</code></li>
<li>Create <code>persona/poet.json</code> with customized fields</li>
<li>Falls back to core.json if file doesn&#39;t exist</li>
</ul>
</li>
<li>Save facets configuration</li>
</ol>
<h3>Switching Active Facet</h3>
<p><strong>Via Web UI</strong>:</p>
<ul>
<li>Left sidebar status widget</li>
<li>Click facet name to cycle through enabled facets</li>
</ul>
<p><strong>Via Conversation</strong>:</p>
<ul>
<li>Messages show active facet in header</li>
<li>Each message tagged with facet metadata</li>
</ul>
<h3>Creating Custom Facet Persona</h3>
<ol>
<li>Copy <code>persona/core.json</code> to <code>persona/[facet].json</code></li>
<li>Edit facet file with personality variations</li>
<li>In Facets tab, set &quot;Persona File&quot; to <code>&quot;[facet].json&quot;</code></li>
<li>When facet is active, system uses facet file instead of core</li>
</ol>
<p><strong>Example</strong>: <code>persona/poet.json</code> might have:</p>
<ul>
<li>Tone: [&quot;metaphorical&quot;, &quot;expressive&quot;, &quot;lyrical&quot;]</li>
<li>Narrative Style: &quot;poetic with vivid imagery&quot;</li>
<li>Interests: [&quot;literature&quot;, &quot;symbolism&quot;, &quot;creative writing&quot;]</li>
</ul>
<h2>Archive Management</h2>
<h3>Viewing Archives</h3>
<ol>
<li>Go to <strong>Archives Tab</strong></li>
<li>See chronological list of past personas</li>
<li>Click any archive to preview</li>
<li>Review changes over time</li>
</ol>
<h3>Restoring an Archive</h3>
<ol>
<li>Find the archive you want to restore</li>
<li>Click <strong>&quot;Restore&quot;</strong> button</li>
<li>Confirm restoration</li>
<li>Current persona backed up automatically</li>
<li>Archive becomes new current persona</li>
</ol>
<p><strong>Use Case</strong>: Undo recent changes, return to working configuration</p>
<h3>Deleting Archives</h3>
<ol>
<li>Find archive to delete</li>
<li>Click <strong>&quot;Delete&quot;</strong> button</li>
<li>Confirm deletion (irreversible)</li>
<li>Archive removed from disk</li>
</ol>
<p><strong>Best Practice</strong>: Keep at least 3-5 recent archives for safety</p>
<h2>Validation and Safety</h2>
<h3>Required Fields</h3>
<p>The editor enforces minimum requirements:</p>
<ul>
<li><strong>Identity</strong>: name, role, purpose must be non-empty</li>
<li><strong>Personality</strong>: communicationStyle must exist</li>
<li><strong>Values</strong>: At least one core value recommended</li>
</ul>
<h3>Automatic Backups</h3>
<p>Every save creates a timestamped backup:</p>
<pre><code>persona/archive/
‚îú‚îÄ‚îÄ core-2025-11-25-103045.json
‚îú‚îÄ‚îÄ core-2025-11-24-153012.json
‚îî‚îÄ‚îÄ core-2025-11-23-091234.json
</code></pre>
<p><strong>Retention</strong>: Up to 50 archives kept (configurable)</p>
<h3>Audit Logging</h3>
<p>All edits logged to <code>logs/audit/</code> with:</p>
<ul>
<li>Actor (username)</li>
<li>Timestamp</li>
<li>Changed fields</li>
<li>Old and new values</li>
</ul>
<h2>Multi-User Isolation</h2>
<p>In multi-user setups:</p>
<ul>
<li>Each user has separate <code>profiles/&lt;username&gt;/persona/</code> directory</li>
<li>Changes don&#39;t affect other users</li>
<li>Owners can mark personas public/private for guest access</li>
</ul>
<h2>Best Practices</h2>
<h3>Effective Editing</h3>
<ol>
<li><strong>Start with Generator</strong> - Use interview for initial creation</li>
<li><strong>Editor for refinement</strong> - Tweak specific fields manually</li>
<li><strong>Save frequently</strong> - Don&#39;t lose work</li>
<li><strong>Review before saving</strong> - Check all your changes</li>
<li><strong>Test in conversation</strong> - Verify persona behavior</li>
</ol>
<h3>Field Organization</h3>
<ul>
<li><strong>Identity</strong>: Keep concise and clear</li>
<li><strong>Values</strong>: Prioritize top 3-5 values</li>
<li><strong>Goals</strong>: Update regularly as you complete them</li>
<li><strong>Decision Heuristics</strong>: Add patterns as you discover them</li>
<li><strong>Interests</strong>: Remove stale interests periodically</li>
</ul>
<h3>Facet Usage</h3>
<ul>
<li><strong>default</strong>: 90% of conversations</li>
<li><strong>poet</strong>: Creative writing, storytelling</li>
<li><strong>thinker</strong>: Technical analysis, problem-solving</li>
<li><strong>friend</strong>: Emotional support, empathy</li>
<li><strong>antagonist</strong>: Devil&#39;s advocate, critical thinking</li>
</ul>
<h2>Troubleshooting</h2>
<h3>Changes Not Saving</h3>
<ul>
<li>Check for validation errors at top of page</li>
<li>Ensure all required fields filled</li>
<li>Check file permissions on persona/core.json</li>
<li>Verify not in emulation mode (read-only)</li>
</ul>
<h3>Facets Not Working</h3>
<ul>
<li>Verify facet enabled in facets.json</li>
<li>Check persona file exists if specified</li>
<li>Ensure facets.json is valid JSON</li>
<li>Restart web UI to reload facets</li>
</ul>
<h3>Archives Not Showing</h3>
<ul>
<li>Check <code>persona/archive/</code> directory exists</li>
<li>Verify archive files are valid JSON</li>
<li>Check file permissions</li>
<li>Archives load on-demand when tab opened</li>
</ul>
<h3>Lost Unsaved Changes</h3>
<ul>
<li>Editor does NOT auto-save</li>
<li>Always click &quot;Save Persona&quot; button</li>
<li>Consider keeping a backup copy externally</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Use <a href="persona-generator.md">Persona Generator</a> for major personality updates</li>
<li>Train AI models with <a href="ai-training.md">AI Training</a> using your persona</li>
<li>Understand how persona affects behavior in <a href="cognitive-modes.md">Cognitive Modes</a></li>
<li>Let memories update persona automatically via Psychoanalyzer agent</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="overview" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Overview</span> </span> </button> <button class="nav-button next-button" data-target="persona-generator" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Persona Generator</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="persona-generator" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Persona Generator</h1>
<p>The Persona Generator is an AI-powered interview system that helps you create and refine your digital personality through a conversational, therapist-style interview process.</p>
<h2>Overview</h2>
<p>The Persona Generator conducts guided interviews using adaptive questions across 5 personality categories. Through thoughtful responses, the system extracts structured persona data and updates your <code>persona/core.json</code> file with new insights.</p>
<p><strong>Key Features:</strong></p>
<ul>
<li><strong>Therapist-style interviewing</strong>: Uses motivational interviewing techniques</li>
<li><strong>Adaptive questions</strong>: Follow-up questions based on your answers</li>
<li><strong>5 personality categories</strong>: Values, goals, style, biography, current focus</li>
<li><strong>Session resume</strong>: Pause and continue interviews later</li>
<li><strong>Safe merging</strong>: Preview changes before applying</li>
<li><strong>Training data export</strong>: Export interviews for LoRA fine-tuning</li>
</ul>
<h2>When to Use</h2>
<p>Use the Persona Generator for:</p>
<ul>
<li><strong>Initial persona creation</strong> - First-time setup</li>
<li><strong>Major life changes</strong> - Career shifts, relocations, significant events</li>
<li><strong>Personality evolution</strong> - When you&#39;ve changed significantly</li>
<li><strong>Comprehensive updates</strong> - Quarterly or annual personality reassessment</li>
</ul>
<p>For quick tweaks, use the <a href="persona-editor.md">Persona Editor</a> instead.</p>
<h2>Accessing the Generator</h2>
<h3>Via Web UI</h3>
<ol>
<li>Navigate to <strong>Persona Generator</strong> in the left sidebar</li>
<li>Click <strong>&quot;Start New Interview&quot;</strong></li>
<li>Begin answering questions</li>
</ol>
<h3>Via CLI</h3>
<pre><code class="language-bash"># Start a new interview session
./bin/mh persona generate
</code></pre>
<h2>The Interview Process</h2>
<h3>1. Starting a Session</h3>
<p>When you start a new interview:</p>
<ul>
<li>System creates a unique session ID</li>
<li>First baseline question appears</li>
<li>Category coverage tracker initializes (all at 0%)</li>
</ul>
<p><strong>Session Storage:</strong></p>
<pre><code>profiles/&lt;username&gt;/state/persona-interviews/
‚îú‚îÄ‚îÄ session-&lt;id&gt;.json       # Active session
‚îî‚îÄ‚îÄ session-&lt;id&gt;-complete.json  # Completed session
</code></pre>
<h3>2. Answering Questions</h3>
<p>For each question:</p>
<ul>
<li>Read the question carefully</li>
<li>Type your response (minimum 20 characters)</li>
<li>Click <strong>&quot;Submit Answer&quot;</strong> or press Ctrl+Enter</li>
<li>Wait for next question (system generates adaptive follow-ups)</li>
</ul>
<p><strong>Question Categories:</strong></p>
<ol>
<li><p><strong>Values</strong> (Core principles and ethics)</p>
<ul>
<li>&quot;What core values guide your most important life decisions?&quot;</li>
<li>&quot;When you face a difficult choice, what factors matter most to you?&quot;</li>
</ul>
</li>
<li><p><strong>Goals</strong> (Aspirations and objectives)</p>
<ul>
<li>&quot;What are you currently working toward?&quot;</li>
<li>&quot;If you could master one new skill, what would it be and why?&quot;</li>
</ul>
</li>
<li><p><strong>Style</strong> (Communication and interaction preferences)</p>
<ul>
<li>&quot;How would you describe your communication style?&quot;</li>
<li>&quot;How do you prefer to process information?&quot;</li>
</ul>
</li>
<li><p><strong>Biography</strong> (Formative experiences and background)</p>
<ul>
<li>&quot;What experiences have shaped who you are today?&quot;</li>
</ul>
</li>
<li><p><strong>Current Focus</strong> (Present interests and priorities)</p>
<ul>
<li>&quot;What topics or projects are you most engaged with right now?&quot;</li>
</ul>
</li>
</ol>
<h3>3. Category Coverage</h3>
<p>The UI shows real-time progress for each category:</p>
<pre><code>Values: 60%
Goals: 80%
Style: 40%
Biography: 60%
Current Focus: 100%
</code></pre>
<p><strong>Completion Requirements:</strong></p>
<ul>
<li>Minimum 7 questions answered</li>
<li>Each category should reach 80% coverage</li>
<li>Typically 7-15 questions total (configurable in <code>etc/persona-generator.json</code>)</li>
</ul>
<h3>4. Session Management</h3>
<p><strong>Pause and Resume:</strong></p>
<ul>
<li>Your session auto-saves every 30 seconds</li>
<li>Close the browser - your progress is saved</li>
<li>Return later and click <strong>&quot;Resume Session&quot;</strong></li>
<li>Continue where you left off</li>
</ul>
<p><strong>Editing Answers:</strong></p>
<ul>
<li>Click the <strong>edit icon</strong> (‚úèÔ∏è) next to any previous answer</li>
<li>Modify your response</li>
<li>Click <strong>&quot;Save&quot;</strong> to update</li>
<li>System regenerates follow-up questions if needed</li>
</ul>
<p><strong>Quick Notes Feature:</strong></p>
<ul>
<li>Click <strong>&quot;Add Notes&quot;</strong> button</li>
<li>Paste in existing notes or journal entries</li>
<li>System extracts insights without asking questions</li>
<li>Useful for bulk importing existing self-reflections</li>
</ul>
<h3>5. Completing the Interview</h3>
<p>When you&#39;ve answered enough questions:</p>
<ol>
<li>Click <strong>&quot;Finalize Interview&quot;</strong></li>
<li>System analyzes all your responses</li>
<li>Extracts structured persona data</li>
<li>Shows diff preview of proposed changes</li>
</ol>
<p><strong>Extraction Process:</strong></p>
<ul>
<li>Uses psychotherapist model for analysis</li>
<li>Identifies values, goals, communication patterns</li>
<li>Detects personality traits and preferences</li>
<li>Organizes insights into persona schema</li>
</ul>
<h3>6. Reviewing Changes</h3>
<p>The diff viewer shows:</p>
<ul>
<li><strong>Additions</strong>: New fields being added (green)</li>
<li><strong>Modifications</strong>: Existing fields being updated (yellow)</li>
<li><strong>Context</strong>: Unchanged fields for reference (gray)</li>
</ul>
<p><strong>Example Diff:</strong></p>
<pre><code class="language-diff">&quot;communication_style&quot;: {
-  &quot;tone&quot;: &quot;friendly&quot;
+  &quot;tone&quot;: &quot;friendly and direct&quot;,
+  &quot;preferences&quot;: [&quot;concise explanations&quot;, &quot;bullet points&quot;]
}

+ &quot;current_projects&quot;: [
+   &quot;learning guitar&quot;,
+   &quot;building a garden&quot;
+ ]
</code></pre>
<h3>7. Merge Strategies</h3>
<p>Choose how to apply changes:</p>
<p><strong>Merge (Recommended)</strong></p>
<ul>
<li>Adds new fields</li>
<li>Updates existing fields</li>
<li>Preserves unmentioned fields</li>
<li><strong>Use when</strong>: You want to enrich existing persona</li>
</ul>
<p><strong>Replace</strong></p>
<ul>
<li>Completely replaces persona with interview results</li>
<li>Deletes fields not mentioned in interview</li>
<li><strong>Use when</strong>: Starting fresh or major overhaul</li>
</ul>
<p><strong>Append</strong></p>
<ul>
<li>Only adds new fields</li>
<li>Never modifies existing fields</li>
<li><strong>Use when</strong>: You want to add to persona without changing anything</li>
</ul>
<h3>8. Applying Changes</h3>
<p>After choosing a merge strategy:</p>
<ol>
<li>Click <strong>&quot;Apply Changes&quot;</strong></li>
<li>System backs up current persona to <code>persona/archive/</code></li>
<li>Merges interview results into <code>persona/core.json</code></li>
<li>Shows success confirmation</li>
<li>New persona immediately active</li>
</ol>
<p><strong>Automatic Backup:</strong></p>
<pre><code>persona/archive/
‚îî‚îÄ‚îÄ core-2025-11-25-143022.json  # Timestamped backup
</code></pre>
<h2>Configuration</h2>
<p>The generator is configured in <code>etc/persona-generator.json</code>:</p>
<pre><code class="language-json">{
  &quot;baselineQuestions&quot;: [
    {
      &quot;id&quot;: &quot;q1&quot;,
      &quot;category&quot;: &quot;values&quot;,
      &quot;prompt&quot;: &quot;What core values guide your life decisions?&quot;
    }
  ],
  &quot;maxQuestionsPerSession&quot;: 15,
  &quot;requireMinimumAnswers&quot;: 7,
  &quot;categories&quot;: [
    &quot;values&quot;,
    &quot;goals&quot;,
    &quot;style&quot;,
    &quot;biography&quot;,
    &quot;current_focus&quot;
  ],
  &quot;sessionDefaults&quot;: {
    &quot;minAnswerLength&quot;: 20,
    &quot;maxAnswerLength&quot;: 2000,
    &quot;targetCategoryCompletionPercentage&quot;: 80,
    &quot;allowResume&quot;: true,
    &quot;autoSaveInterval&quot;: 30000
  }
}
</code></pre>
<p><strong>Customizable Settings:</strong></p>
<ul>
<li>Add custom baseline questions</li>
<li>Adjust minimum/maximum answer lengths</li>
<li>Change category completion targets</li>
<li>Modify auto-save interval</li>
<li>Add new categories (requires code changes)</li>
</ul>
<h2>Privacy Guidelines</h2>
<p>The generator follows strict privacy rules:</p>
<ul>
<li>‚ùå Never asks for full legal name (unless you volunteer it)</li>
<li>‚ùå Never asks for government IDs or social security numbers</li>
<li>‚ùå Never asks for medical diagnoses or detailed health info</li>
<li>‚ùå Never asks for financial account numbers or passwords</li>
<li>‚ùå Avoids specific location addresses</li>
<li>‚úÖ Focuses on patterns, preferences, and personality traits</li>
</ul>
<h2>Interviewing Techniques</h2>
<p>The system uses professional psychological methods:</p>
<p><strong>Motivational Interviewing:</strong></p>
<ul>
<li>Open-ended questions</li>
<li>Reflective listening</li>
<li>No judgment or criticism</li>
<li>Respects your autonomy</li>
<li>Encourages self-exploration</li>
</ul>
<p><strong>Adaptive Follow-ups:</strong></p>
<ul>
<li>Next question influenced by your previous answers</li>
<li>Deeper exploration of mentioned topics</li>
<li>Balanced coverage across categories</li>
<li>Natural conversation flow</li>
</ul>
<h2>Training Data Export</h2>
<p>Export your interview for AI model training:</p>
<ol>
<li>Complete and finalize an interview</li>
<li>Click <strong>&quot;Export for Training&quot;</strong></li>
<li>System generates JSONL file:</li>
</ol>
<pre><code>profiles/&lt;username&gt;/out/training-data/
‚îî‚îÄ‚îÄ persona-interview-2025-11-25.jsonl
</code></pre>
<p><strong>Use Case:</strong> Fine-tune LoRA adapters with your personality patterns. See <a href="ai-training.md">AI Training</a> for details.</p>
<h2>Session History</h2>
<p>View past interviews:</p>
<ol>
<li>Click <strong>&quot;View History&quot;</strong> button</li>
<li>See list of all completed sessions</li>
<li>Click any session to review Q&amp;A</li>
<li>Export or delete old sessions</li>
</ol>
<p><strong>Session Lifecycle:</strong></p>
<ul>
<li><code>active</code>: Currently in progress</li>
<li><code>completed</code>: Finished but not yet applied</li>
<li><code>finalized</code>: Applied to persona</li>
<li><code>aborted</code>: Cancelled without applying</li>
</ul>
<h2>Admin Operations</h2>
<p><strong>Purge All Sessions:</strong></p>
<ul>
<li>Deletes all session data (active and completed)</li>
<li>Does NOT affect your applied persona</li>
<li>Useful for starting fresh</li>
</ul>
<p><strong>Reset Interview:</strong></p>
<ul>
<li>Clears current active session</li>
<li>Preserves session history</li>
<li>Start over with blank slate</li>
</ul>
<h2>Best Practices</h2>
<h3>Effective Answering</h3>
<ol>
<li><strong>Be specific</strong>: &quot;I value honesty and transparency in all relationships&quot; &gt; &quot;I value honesty&quot;</li>
<li><strong>Give examples</strong>: Illustrate values with real-life scenarios</li>
<li><strong>Be honest</strong>: The persona works best when it reflects the real you</li>
<li><strong>Take your time</strong>: No rush - sessions can pause/resume</li>
<li><strong>Minimum 20 characters</strong>: Brief answers lack context for extraction</li>
</ol>
<h3>When to Interview</h3>
<ul>
<li><strong>Initial setup</strong>: First time using MetaHuman OS</li>
<li><strong>Quarterly check-ins</strong>: Review and update every 3 months</li>
<li><strong>After major life events</strong>: Job change, relocation, relationships</li>
<li><strong>When persona feels off</strong>: Responses don&#39;t match your current self</li>
</ul>
<h3>Combining with Other Tools</h3>
<ol>
<li><strong>Persona Generator</strong> ‚Üí Initial creation</li>
<li><strong><a href="persona-editor.md">Persona Editor</a></strong> ‚Üí Fine-tune specific fields</li>
<li><strong>Daily usage</strong> ‚Üí Build memories</li>
<li><strong>Psychoanalyzer agent</strong> ‚Üí Automatic evolution from memories</li>
</ol>
<h2>Troubleshooting</h2>
<h3>Interview Won&#39;t Start</h3>
<ul>
<li>Check authentication (must be logged in)</li>
<li>Verify not in emulation mode (write access required)</li>
<li>Check <code>etc/persona-generator.json</code> exists and is valid</li>
</ul>
<h3>Questions Seem Repetitive</h3>
<ul>
<li>System is trying to reach 80% coverage in each category</li>
<li>Answer more deeply to satisfy category requirements</li>
<li>Check progress bars - incomplete categories get more questions</li>
</ul>
<h3>Can&#39;t Apply Changes</h3>
<ul>
<li>Must finalize interview first</li>
<li>Check write permissions (not emulation mode)</li>
<li>Verify backup directory is writable</li>
</ul>
<h3>Session Lost</h3>
<ul>
<li>Check <code>profiles/&lt;username&gt;/state/persona-interviews/</code></li>
<li>Sessions auto-save every 30 seconds</li>
<li>Look for <code>session-&lt;id&gt;.json</code> files</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Fine-tune with <a href="persona-editor.md">Persona Editor</a> for manual adjustments</li>
<li>Train AI with your personality via <a href="ai-training.md">AI Training</a></li>
<li>Let psychoanalyzer agent update persona from memories automatically</li>
<li>Explore <a href="cognitive-modes.md">Cognitive Modes</a> to understand how persona influences behavior</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="persona-editor" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Persona Editor</span> </span> </button> <button class="nav-button next-button" data-target="quick-start" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Quick Start</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="quick-start" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Quick Start Guide</h1>
<p>Your first 5 minutes with MetaHuman OS.</p>
<hr>
<h2>1. Start the System (1 minute)</h2>
<pre><code class="language-bash"># Start web UI with agents
cd apps/site &amp;&amp; pnpm dev
</code></pre>
<p>Open <a href="http://localhost:4321">http://localhost:4321</a> and log in.</p>
<hr>
<h2>2. Capture Your First Memory (1 minute)</h2>
<p><strong>Via Web UI:</strong></p>
<ol>
<li>Click <strong>Chat</strong> in left sidebar</li>
<li>Type a message: &quot;This is my first interaction with MetaHuman&quot;</li>
<li>The system responds and saves the conversation</li>
</ol>
<p><strong>Via CLI:</strong></p>
<pre><code class="language-bash">./bin/mh capture &quot;Met with Sarah about the ML project this morning&quot;
</code></pre>
<hr>
<h2>3. Browse Your Memories (1 minute)</h2>
<p><strong>Web UI:</strong></p>
<ol>
<li>Click <strong>Memory Browser</strong> in left sidebar</li>
<li>Browse by tab:<ul>
<li><strong>Conversations</strong>: Chat history</li>
<li><strong>Observations</strong>: Manual captures</li>
<li><strong>All Memories</strong>: Complete timeline</li>
</ul>
</li>
</ol>
<p><strong>CLI:</strong></p>
<pre><code class="language-bash">./bin/mh remember &quot;Sarah&quot;
</code></pre>
<hr>
<h2>4. Create a Task (1 minute)</h2>
<pre><code class="language-bash">./bin/mh task add &quot;Review LoRA training documentation&quot;
./bin/mh task
</code></pre>
<p>Or use the <strong>Tasks</strong> tab in the web UI.</p>
<hr>
<h2>5. Explore Autonomous Agents (1 minute)</h2>
<p>Watch the <strong>Agent Monitor</strong> (right sidebar):</p>
<ul>
<li><strong>Organizer</strong>: Enriches memories with tags/entities</li>
<li><strong>Reflector</strong>: Generates internal thoughts</li>
<li><strong>Curator</strong>: Prepares training data</li>
</ul>
<pre><code class="language-bash"># View agent status
./bin/mh agent status
</code></pre>
<hr>
<h2>What Just Happened?</h2>
<ol>
<li><strong>Memory Captured</strong>: Your observations and conversations are stored as structured JSON</li>
<li><strong>Automatic Enrichment</strong>: The organizer agent extracts tags and entities</li>
<li><strong>Reflection</strong>: The system generates internal thoughts about your activities</li>
<li><strong>Training Data</strong>: Curator prepares conversations for LoRA adapter training</li>
</ol>
<hr>
<h2>Next Steps</h2>
<h3>Learn the Interface</h3>
<p>‚Üí <a href="../using-metahuman/chat-interface.md">Chat Interface</a> - Conversation modes and features</p>
<h3>Personalize Your System</h3>
<p>‚Üí <a href="../training-personalization/persona-generator.md">Persona Generator</a> - Create your personality profile</p>
<h3>Train Your Voice</h3>
<p>‚Üí <a href="../training-personalization/voice-training.md">Voice Training</a> - Clone your voice</p>
<h3>Deep Dive</h3>
<p>‚Üí <a href="core-concepts.md">Core Concepts</a> - Understand how it all works</p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="persona-generator" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Persona Generator</span> </span> </button> <button class="nav-button next-button" data-target="roadmap" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Roadmap</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="roadmap" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h2>Upcoming Features</h2>
<p>The following features are currently in the planning and proposal stage. For more details, please refer to the linked design documents.</p>
<ul>
<li><strong><a href="04-core-concepts.md#8-cognitive-modes-upcoming-feature">Cognitive Modes</a></strong>: A planned feature to allow switching between different operational paradigms, such as a deep cognitive mirror (<code>Dual Consciousness</code>), a simple command-driven assistant (<code>Agent Mode</code>), or a stable, non-learning conversational partner (<code>Emulation Mode</code>).</li>
<li><strong>Direct <code>llama.cpp</code> Integration</strong>: Future versions will explore direct <code>llama.cpp</code> integration, offering an alternative to Ollama for users who prefer a more direct local LLM setup.</li>
<li><strong>Password Recovery System</strong>: Future authentication enhancements will include secure password recovery options:<ul>
<li><strong>Recovery Codes</strong> (No external dependencies): Generate one-time recovery codes during account creation for offline password reset</li>
<li><strong>Email Integration</strong> (Requires SMTP): Email-based password reset flow with time-limited tokens (requires SMTP server configuration)</li>
<li><strong>SMS Recovery</strong> (Requires SMS service): Mobile-based password reset via SMS (requires Twilio/AWS SNS or similar service)</li>
</ul>
</li>
</ul>
<h2>What&#39;s Next</h2>
<p>MetaHuman OS is under active development following a phased roadmap toward full autonomous operation.</p>
<h3>Current Phase: Phase 1 (Intelligence &amp; Autonomy)</h3>
<p><strong>Status:</strong> Core infrastructure complete, autonomous agents operational</p>
<p><strong>Completed:</strong></p>
<ul>
<li>‚úÖ Project structure and monorepo setup</li>
<li>‚úÖ Identity Kernel schemas and persona management</li>
<li>‚úÖ Memory system (episodic, tasks) with JSON storage</li>
<li>‚úÖ CLI with 20+ commands</li>
<li>‚úÖ Audit system with complete operation logging</li>
<li>‚úÖ Memory indexing and vector search (semantic embeddings)</li>
<li>‚úÖ Six autonomous agents (organizer, reflector, dreamer, boredom/sleep services, ingestor)</li>
<li>‚úÖ Web UI with ChatGPT-style 3-column layout</li>
<li>‚úÖ Persona-aware chat with memory grounding</li>
<li>‚úÖ Real-time reflection streaming</li>
<li>‚úÖ Inner dialogue system</li>
<li>‚úÖ Memory validation UI</li>
<li>‚úÖ Approval queue for skill execution</li>
<li>‚úÖ <strong>Phase 6: Authentication &amp; Multi-User Support</strong><ul>
<li>Cookie-based session management (24h owner, 1h guest, 30min anonymous)</li>
<li>Role-based access control (owner/guest/anonymous)</li>
<li>Security settings UI (change username, password, profile)</li>
<li>bcrypt password hashing (12 rounds)</li>
<li>Environment-based system triggers (WETWARE_DECEASED, HIGH_SECURITY)</li>
<li>Authenticated memory API endpoints</li>
<li>Write mode restrictions for emulation mode</li>
</ul>
</li>
</ul>
<p><strong>In Progress:</strong></p>
<ul>
<li>Preference learning from repeated decisions</li>
<li>Advanced decision engine with policy reasoning</li>
</ul>
<h3>Phase 2: Decision Engine (Upcoming)</h3>
<p><strong>Goal:</strong> Autonomous decision capability within bounds</p>
<p><strong>Features:</strong></p>
<ul>
<li>Advanced decision engine with policy-based reasoning</li>
<li>Per-skill trust boundary enforcement</li>
<li>Comprehensive approval flows (request ‚Üí review ‚Üí execute ‚Üí log)</li>
<li>Dry-run mode for previewing all actions</li>
<li>Skills v1: Calendar integration, notifications, basic task execution</li>
<li>Learning from user corrections</li>
</ul>
<p><strong>Timeline:</strong> Next 4-6 weeks</p>
<h3>Phase 3: Proactive Intelligence (Q1 2026)</h3>
<p><strong>Goal:</strong> Anticipate and act before being asked</p>
<p><strong>Features:</strong></p>
<ul>
<li>Background agents for calendar, email, tasks</li>
<li>Opportunity detection system</li>
<li>Proactive planning: Daily briefs, weekly reviews, goal tracking</li>
<li>Contextual reasoning: Understand situation and timing</li>
<li>Skills v2: Email, messaging, research, content drafting</li>
<li>Notification system: Timely nudges and updates</li>
<li><strong>Email Integration</strong>: SMTP/IMAP support for notifications, password recovery, and proactive email management</li>
<li><strong>Password Recovery Options</strong>: Recovery codes (local-first), email reset tokens, SMS recovery</li>
</ul>
<p><strong>Timeline:</strong> 6-8 weeks after Phase 2</p>
<h3>Phase 4: Deep Sync (Q2 2026)</h3>
<p><strong>Goal:</strong> Seamless human-digital personality extension integration</p>
<p><strong>Features:</strong></p>
<ul>
<li>Behavioral learning: Recognize patterns in your actions</li>
<li>Communication style mirroring: Match your writing voice</li>
<li>Emotional intelligence: Understand context and sentiment</li>
<li>Adaptive trust: Expand autonomy based on success</li>
<li>Continuous learning: Self-improve from outcomes</li>
<li>Advanced skills: Complex workflows, multi-step tasks</li>
</ul>
<p><strong>Timeline:</strong> 8-10 weeks</p>
<h3>Phase 5: Full Autonomy (Q2-Q3 2026)</h3>
<p><strong>Goal:</strong> Trusted autonomous operation 24/7</p>
<p><strong>Features:</strong></p>
<ul>
<li>Bounded autonomy: Fully independent within trust zones</li>
<li>Self-management: Maintenance, optimization, skill updates</li>
<li>Drift Monitoring &amp; Auto-Rollback: Automatically detect and revert poorly performing model adapters.</li>
<li>Cross-skill orchestration: Complex multi-step tasks</li>
<li>Long-term planning: Weekly/monthly goal alignment</li>
<li>Rich integrations: Email, calendar, Slack, filesystem, APIs</li>
<li>Mobile access: Notifications and approvals on the go</li>
</ul>
<p><strong>Timeline:</strong> 10-12 weeks</p>
<h3>Technical Milestones</h3>
<p><strong>M0: Core Infrastructure</strong> ‚úì Complete</p>
<ul>
<li>TypeScript monorepo with pnpm workspaces</li>
<li>Astro web UI with Svelte components</li>
<li>CLI scaffolding</li>
<li>Directory structure</li>
</ul>
<p><strong>M1: Identity &amp; Memory</strong> ‚úì Complete</p>
<ul>
<li>Persona kernel with editable profiles</li>
<li>Memory storage and retrieval</li>
<li>Event capture system</li>
<li>Basic sync engine</li>
</ul>
<p><strong>M2: Decision Engine</strong> (In Progress)</p>
<ul>
<li>Policy-based reasoning</li>
<li>Trust boundary enforcement</li>
<li>Approval workflows</li>
<li>Action logging</li>
</ul>
<p><strong>M3: Autonomous Runtime</strong> (Next)</p>
<ul>
<li>Background process scheduler</li>
<li>Event-driven triggers</li>
<li>Skill execution framework</li>
<li>Proactive agents</li>
</ul>
<p><strong>M4: Production Ready</strong> (Future)</p>
<ul>
<li>Vector search for memory</li>
<li>SQLite for complex queries</li>
<li>API server for integrations</li>
<li>Mobile notifications</li>
<li>Complete audit system</li>
</ul>
<p><strong>M5: Bare-Metal Integration</strong> (Future)</p>
<ul>
<li>Optimized for direct deployment on Linux kernel for maximum performance and resource control.</li>
<li>All-in-one package for simplified deployment and management.</li>
</ul>
<h3>Cross-Platform &amp; Mobile App</h3>
<p>A major focus is bringing the MetaHuman OS experience to mobile devices. The roadmap for this includes:</p>
<ul>
<li><strong>API Readiness:</strong> Enhancing the core API with mobile-friendly endpoints for chat, memory search, and task management.</li>
<li><strong>Mobile App Development:</strong> Building a dedicated mobile app for iOS and Android using Svelte and Capacitor.</li>
<li><strong>Native Integrations:</strong> Leveraging device hardware for features like background audio capture via the microphone, photo capture, and push notifications for reminders and system alerts.</li>
<li><strong>Offline Support:</strong> Caching memories, tasks, and chat history for offline access.</li>
</ul>
<h3>Deployment &amp; Remote Access</h3>
<p>Future versions will support deploying the web frontend to a public URL (e.g., on Vercel) while keeping the AI backend running on your local machine. This will allow you to access your MetaHuman from anywhere.</p>
<p>The core foundation for this is now in place with the <strong>Unified Security Policy</strong>, which is ready for an authentication layer and user roles (<code>owner</code>, <code>guest</code>).</p>
<p>The planned steps are:</p>
<ul>
<li><strong>Implement Authentication</strong>: Add a session management system to distinguish between users.</li>
<li><strong>Expose Local Backend:</strong> Use a secure tunneling service like <code>ngrok</code> to create a public URL for your local server.</li>
<li><strong>Update Frontend API Calls:</strong> Modify the frontend components to send API requests to the public tunnel URL instead of a relative path.</li>
<li><strong>Deploy to a Hosting Service:</strong> Host the static frontend on a service like Vercel, configured to point to your public backend URL.</li>
</ul>
<hr>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="quick-start" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Quick Start</span> </span> </button> <button class="nav-button next-button" data-target="security-trust" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Security Trust</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="security-trust" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h2>Security &amp; Trust Model</h2>
<h3>Unified Security Policy</h3>
<p>MetaHuman OS now operates under a <strong>Unified Security Policy</strong>, a centralized system that governs all permissions. This policy is the single source of truth for what actions are allowed, making the system safer and more predictable.</p>
<p><strong>How it Works:</strong>
The policy evaluates each request using two inputs:</p>
<ol>
<li><strong>Cognitive Mode</strong> ‚Äì <code>Dual Consciousness</code>, <code>Agent</code>, or <code>Emulation</code> determine whether writes are even eligible.</li>
<li><strong>Authenticated User Context</strong> ‚Äì Every web request runs inside a user context populated by the middleware. It contains the user‚Äôs role (<code>owner</code>, <code>guest</code>, <code>anonymous</code>) and their profile paths. Guests are automatically constrained to read-only emulation, while anonymous traffic is prevented from touching profile data altogether.</li>
</ol>
<p><strong>Key Implications:</strong></p>
<ul>
<li><strong>Emulation Mode is a Secure &quot;Read-Only&quot; Mode</strong>: Any attempt to create or modify memories, tasks, or configuration while in emulation results in <code>403 Forbidden</code>. Guest sessions are permanently pinned to this mode.</li>
<li><strong>Configuration is Protected</strong>: Changing cognitive modes, trust levels, or system settings requires an authenticated owner session. All such attempts are logged for audit.</li>
<li><strong>Centralized Enforcement</strong>: Instead of scattered checks, all security rules are enforced consistently by middleware that consults the central policy for every relevant API request‚Äîincluding CLI commands that execute via <code>withUserContext</code>.</li>
</ul>
<p>This new architecture provides a robust foundation for current safety and future multi-user capabilities.</p>
<h4>High Security Mode</h4>
<p>For situations requiring maximum safety, the system can be placed in <strong>High Security Mode</strong> by setting the <code>HIGH_SECURITY=true</code> environment variable. This is the most restrictive state of the OS:</p>
<ul>
<li>It forces the system into <strong>Emulation Mode</strong>.</li>
<li>It disables the ability to switch to any other mode.</li>
<li>All API endpoints that perform write operations are blocked.</li>
<li>A prominent banner is displayed in the UI to ensure the user is aware of the lockdown state.</li>
</ul>
<p>This mode is ideal for safely exposing a read-only version of the OS on a network or for preventing any accidental changes during a critical analysis.</p>
<h3>Trust Levels (Progressive Autonomy)</h3>
<ol>
<li><strong><code>observe</code></strong> - Monitor only, learn patterns (no autonomous actions)</li>
<li><strong><code>suggest</code></strong> - Propose actions, require manual approval</li>
<li><strong><code>supervised_auto</code></strong> - Execute within pre-approved categories</li>
<li><strong><code>bounded_auto</code></strong> - Full autonomy within defined boundaries</li>
<li><strong><code>adaptive_auto</code></strong> - Self-expand boundaries based on learning (experimental)</li>
</ol>
<h3>Trust Progression</h3>
<pre><code>observe
   ‚îÇ
   ‚îÇ  Demonstrates reliable read-only behavior
   ‚ñº
suggest
   ‚îÇ
   ‚îÇ  Makes good suggestions consistently
   ‚ñº
supervised_auto
   ‚îÇ
   ‚îÇ  Executes low-risk actions reliably
   ‚ñº
bounded_auto
   ‚îÇ
   ‚îÇ  Full autonomy within safe boundaries
   ‚ñº
adaptive_auto (future)
   ‚îî‚îÄ Self-expands boundaries based on learning
</code></pre>
<h3>Trust Progression Criteria</h3>
<ul>
<li><strong>observe ‚Üí suggest</strong>: Operator has run for 7+ days, no errors, demonstrates understanding of goals</li>
<li><strong>suggest ‚Üí supervised_auto</strong>: 80%+ of suggestions approved, no high-risk rejects, consistent alignment</li>
<li><strong>supervised_auto ‚Üí bounded_auto</strong>: 95%+ success rate, no rollbacks in 30 days, respect for boundaries</li>
</ul>
<h3>User Roles &amp; Isolation</h3>
<table>
<thead>
<tr>
<th>Role</th>
<th>Session Duration</th>
<th>Read Access</th>
<th>Write Access</th>
<th>Notes</th>
</tr>
</thead>
<tbody><tr>
<td><code>owner</code></td>
<td>24 hours</td>
<td>Own profile (<code>profiles/&lt;username&gt;/‚Ä¶</code>) + shared assets</td>
<td>Mode-dependent</td>
<td>Can switch modes, manage profiles, change settings, manage other users.</td>
</tr>
<tr>
<td><code>guest</code></td>
<td>1 hour</td>
<td>Own profile (<code>profiles/&lt;username&gt;/‚Ä¶</code>)</td>
<td>Mode-dependent</td>
<td>Can modify own data within trust level limits. Cannot manage other users.</td>
</tr>
<tr>
<td><code>anonymous</code></td>
<td>30 minutes</td>
<td>Public profiles only (read-only emulation)</td>
<td><strong>Denied</strong></td>
<td>API endpoints return clean 401 responses for protected operations, or sensible defaults for public reads. Can browse public personas.</td>
</tr>
</tbody></table>
<p>Switching users always tears down the existing context. Background agents iterate through the registered users by repeatedly invoking <code>withUserContext</code>, ensuring each profile is processed in isolation with appropriate permissions.</p>
<h4>Streamlined Path Resolution</h4>
<p>The system uses a new streamlined authentication architecture that eliminates confusing &quot;anonymous user&quot; path access errors:</p>
<ul>
<li><strong>Public Read Endpoints</strong> degrade gracefully for anonymous users (e.g., <code>/api/boot</code> returns default persona)</li>
<li><strong>Protected Operations</strong> return clean 401 responses with clear error messages</li>
<li><strong>System Operations</strong> use <code>systemPaths</code> and never touch user-specific data</li>
</ul>
<p>For developers: See <a href="../AUTHENTICATION_STREAMLINED.md">AUTHENTICATION_STREAMLINED.md</a> for implementation details and migration guide.</p>
<h3>Trust Levels and Skill Availability</h3>
<table>
<thead>
<tr>
<th>Trust Level</th>
<th>Available Skills</th>
<th>Auto-Execute?</th>
<th>Approval Required?</th>
</tr>
</thead>
<tbody><tr>
<td><code>observe</code></td>
<td>fs_read, search_index</td>
<td>No</td>
<td>All skills</td>
</tr>
<tr>
<td><code>suggest</code></td>
<td>fs_read, search_index, run_agent</td>
<td>No</td>
<td>All skills</td>
</tr>
<tr>
<td><code>supervised_auto</code></td>
<td>All except shell_safe</td>
<td>Yes (low risk)</td>
<td>High-risk only</td>
</tr>
<tr>
<td><code>bounded_auto</code></td>
<td>All</td>
<td>Yes (all)</td>
<td>High-risk only</td>
</tr>
</tbody></table>
<h3>Skill-Specific Approval Rules</h3>
<p>Some skills <strong>always</strong> require approval regardless of trust level:</p>
<ul>
<li><code>fs_write</code> - Write files (requires approval)</li>
<li><code>run_agent</code> - Run agents (requires approval)</li>
<li><code>shell_safe</code> - Execute whitelisted commands (requires approval)</li>
</ul>
<p><strong>Rationale</strong>:</p>
<ul>
<li>Reading is safe ‚Üí can be automated early</li>
<li>Writing is permanent ‚Üí always requires approval</li>
<li>Agent execution can be expensive ‚Üí always requires approval</li>
<li>Shell commands can be risky ‚Üí always requires approval</li>
</ul>
<h3>Directory Boundaries</h3>
<p>All read/write operations pass through the path proxy, so users only touch their own profile directories unless explicitly working with shared system assets.</p>
<h4>Read Permissions</h4>
<ul>
<li>‚úÖ Owners: <code>profiles/&lt;owner&gt;/memory</code>, <code>profiles/&lt;owner&gt;/persona</code>, <code>profiles/&lt;owner&gt;/logs</code>, <code>profiles/&lt;owner&gt;/out</code>, <code>profiles/&lt;owner&gt;/etc</code></li>
<li>‚úÖ Owners &amp; guests: <code>out/voices</code> (shared Piper models), documentation, audit logs that reference their own actions</li>
<li>‚ùå Guests: other users&#39; profiles (never exposed), system code directories</li>
<li>‚ùå Anonymous: Protected operations return <code>401 Unauthorized</code>; public read endpoints return sensible defaults</li>
</ul>
<p>System code is always read-only:</p>
<ul>
<li>‚ùå <code>brain/</code>, <code>packages/</code>, <code>apps/</code>, <code>bin/</code>, <code>node_modules/</code></li>
</ul>
<h4>Write Permissions</h4>
<ul>
<li>‚úÖ Owners (appropriate cognitive mode): may write inside <code>profiles/&lt;owner&gt;/memory</code>, <code>profiles/&lt;owner&gt;/out</code>, <code>profiles/&lt;owner&gt;/logs</code>, <code>profiles/&lt;owner&gt;/etc</code></li>
<li>‚ùå Guests: all write operations blocked (enforced both by mode and policy)</li>
<li>‚ùå Anonymous: write access blocked</li>
<li>‚ùå Cross-profile writes: impossible‚Äîcontext resolution never hands out another user‚Äôs paths</li>
</ul>
<h4>Coder Agent Guardrails</h4>
<p>The <strong>Self-Healing Coder Agent</strong> has a unique set of permissions designed to let it modify its own codebase safely:</p>
<ul>
<li><strong>Read Access</strong>: The Coder Agent can read files from almost anywhere in the project, including <code>packages/</code>, <code>apps/</code>, and <code>brain/</code> to get context for its changes. It can also read from <code>memory/</code> to understand the user&#39;s intent.</li>
<li><strong>Write Access</strong>: Its write access is highly specialized. It <strong>can</strong> write to code directories (<code>apps/</code>, <code>packages/</code>, <code>brain/</code>, <code>docs/</code>, <code>etc/</code>).</li>
<li><strong>Protected Directories</strong>: The Coder Agent is <strong>strictly forbidden</strong> from writing to, modifying, or deleting files in <code>memory/</code>, <code>persona/</code>, and <code>logs/</code>. This ensures your memories, identity, and audit history are always safe from being changed by the code agent.</li>
</ul>
<p>All code changes generated by the agent require explicit user approval through the UI before they are applied.</p>
<h3>Command Whitelist (bounded_auto only)</h3>
<p>Allowed commands:</p>
<ul>
<li><code>ls</code> - List files</li>
<li><code>cat</code> - View files</li>
<li><code>grep</code> - Search text</li>
<li><code>find</code> - Find files</li>
<li><code>git</code> - Version control (status, log, diff only)</li>
<li><code>pnpm</code> - Package management (list, why only)</li>
<li><code>node</code> - Run scripts (within metahuman only)</li>
<li><code>tsx</code> - Run TypeScript (within metahuman only)</li>
<li><code>pwd</code> - Print working directory</li>
<li><code>whoami</code> - Current user</li>
</ul>
<p>Not allowed:</p>
<ul>
<li><code>rm</code>, <code>mv</code>, <code>cp</code> - File manipulation (use fs_write skill instead)</li>
<li><code>sudo</code> - Privilege escalation</li>
<li><code>curl</code>, <code>wget</code> - Network access (future: web_fetch skill)</li>
<li><code>ssh</code>, <code>scp</code> - Remote access</li>
<li>Anything with shell metacharacters (pipes, redirects) unless explicitly allowed</li>
</ul>
<h3>View Current Trust Level</h3>
<pre><code class="language-bash">./bin/mh trust
</code></pre>
<h3>Set Trust Level</h3>
<pre><code class="language-bash">./bin/mh trust bounded_auto
</code></pre>
<p><strong>Configured in:</strong> <code>persona/decision-rules.json</code></p>
<h3>Security &amp; Privacy</h3>
<h4>Local-First by Design</h4>
<ul>
<li>All data stays on your machine</li>
<li>No cloud services required</li>
<li>Ollama runs locally</li>
</ul>
<h4>Complete Audit Trail</h4>
<p>Every operation is logged to <code>logs/audit/YYYY-MM-DD.ndjson</code> with:</p>
<ul>
<li>Timestamp</li>
<li>Actor (human, agent name, or system)</li>
<li>Category (system, action, data_change, security, decision)</li>
<li>Level (info, warn, error)</li>
<li>Full context and metadata</li>
</ul>
<h4>Progressive Trust</h4>
<p>Start with <code>observe</code> mode and gradually increase autonomy as you build trust.</p>
<h4>Human-Readable Data</h4>
<p>All memories, tasks, and configuration are JSON files you can inspect and edit directly.</p>
<h3>Trust Boundaries (Per Skill)</h3>
<p>Each skill is classified by risk level and has associated boundaries:</p>
<p><strong>Read-Only Skills:</strong></p>
<ul>
<li>Can access information but not modify</li>
<li>Safe for autonomous operation at any trust level</li>
<li>Examples: <code>fs_read</code>, <code>search_index</code>, <code>list_tasks</code></li>
</ul>
<p><strong>Low-Risk Skills:</strong></p>
<ul>
<li>Minor changes with minimal consequences</li>
<li>Can auto-execute at <code>supervised_auto</code> level</li>
<li>Examples: notifications, drafts, scheduling suggestions</li>
<li>Easily reversible if mistakes occur</li>
</ul>
<p><strong>Medium-Risk Skills:</strong></p>
<ul>
<li>Requires approval below certain thresholds</li>
<li>May execute autonomously if within approved categories</li>
<li>Examples: sending messages, minor purchases (&lt; $X), file modifications</li>
<li>Impact is moderate but manageable</li>
</ul>
<p><strong>High-Risk Skills:</strong></p>
<ul>
<li>Always requires approval regardless of trust level</li>
<li>Significant impact or irreversible consequences</li>
<li>Examples: financial transactions, legal documents, account modifications</li>
<li>Human review mandatory</li>
</ul>
<p><strong>Forbidden Skills:</strong></p>
<ul>
<li>Never autonomous, even at highest trust levels</li>
<li>Critical operations requiring human judgment</li>
<li>Examples: account deletion, major commitments, system-wide changes</li>
<li>Safety override prevents execution</li>
</ul>
<h3>Safety Mechanisms</h3>
<p>MetaHuman OS implements multiple layers of safety to ensure autonomous operation remains safe and controllable:</p>
<p><strong>1. Dry-Run Mode:</strong></p>
<ul>
<li>Preview all changes before execution</li>
<li>See exactly what the system plans to do</li>
<li>Test workflows without side effects</li>
<li>Validate skill parameters</li>
</ul>
<p><strong>2. Undo Buffer:</strong></p>
<ul>
<li>Reversible actions with rollback capability</li>
<li>File versioning for all writes</li>
<li>Memory of previous states</li>
<li>Quick recovery from mistakes</li>
</ul>
<p><strong>3. Rate Limits:</strong></p>
<ul>
<li>Prevent runaway behavior</li>
<li>Maximum actions per hour/day</li>
<li>Cooldown periods for high-risk operations</li>
<li>Throttling for API calls</li>
</ul>
<p><strong>4. Confidence Thresholds:</strong></p>
<ul>
<li>Escalate when uncertainty is high</li>
<li>Require approval for low-confidence decisions</li>
<li>Learn from corrections to improve confidence</li>
<li>Adaptive threshold adjustment</li>
</ul>
<p><strong>5. Kill Switch:</strong></p>
<ul>
<li>Instant halt of all autonomous operations</li>
<li>Emergency stop button</li>
<li>Revert to observe mode immediately</li>
<li>Preserve state for debugging</li>
</ul>
<p><strong>6. Complete Audit Trail:</strong></p>
<ul>
<li>Logs of all reasoning and actions</li>
<li>Full context for every decision</li>
<li>Immutable append-only log</li>
<li>Query and analyze past behavior</li>
</ul>
<p><strong>7. Regular Reviews:</strong></p>
<ul>
<li>Weekly human oversight of autonomous decisions</li>
<li>Monthly audit reports</li>
<li>Pattern analysis for anomalies</li>
<li>Continuous alignment checks</li>
</ul>
<h3>Approval Queue Workflow</h3>
<p>When a skill requires approval:</p>
<ol>
<li><strong>Skill Execution Attempted</strong> - Agent or system tries to execute a high-risk skill</li>
<li><strong>Queued for Approval</strong> - Item added to approval queue with full context</li>
<li><strong>User Notified</strong> - Badge appears in web UI sidebar</li>
<li><strong>User Reviews</strong> - See skill name, description, parameters, and risk level</li>
<li><strong>Decision Made</strong> - User approves or rejects the execution</li>
<li><strong>Action Taken</strong> - If approved, skill executes immediately with full audit logging</li>
</ol>
<p><strong>View Approval Queue:</strong></p>
<ul>
<li>Web UI: Click &quot;Approvals&quot; in left sidebar</li>
<li>CLI: <code>./bin/mh approvals list</code></li>
</ul>
<p><strong>Approve/Reject:</strong></p>
<ul>
<li>Web UI: Click &quot;Approve &amp; Execute&quot; or &quot;Reject&quot; buttons</li>
<li>CLI: <code>./bin/mh approvals approve &lt;id&gt;</code> or <code>./bin/mh approvals reject &lt;id&gt;</code></li>
</ul>
<h3>Emergency Procedures</h3>
<p><strong>Stop All Agents:</strong></p>
<pre><code class="language-bash">./bin/mh agent stop-all
</code></pre>
<p><strong>Revert to Observe Mode:</strong></p>
<pre><code class="language-bash">./bin/mh trust observe
</code></pre>
<p><strong>Review Recent Actions:</strong></p>
<pre><code class="language-bash">./bin/mh audit --since &quot;1 hour ago&quot;
</code></pre>
<p><strong>Clear Approval Queue:</strong></p>
<pre><code class="language-bash">./bin/mh approvals clear
</code></pre>
<h3>Per-User Configuration Security</h3>
<p>Security is enforced at the configuration level through per-user settings:</p>
<ul>
<li><strong>User-specific configs</strong>: Each user has isolated configuration files in <code>profiles/&lt;username&gt;/etc/</code></li>
<li><strong>Path resolution</strong>: The <code>paths</code> proxy automatically resolves to the correct user directory based on context</li>
<li><strong>Model isolation</strong>: Users can have different model preferences without affecting others</li>
<li><strong>Privacy controls</strong>: Users control their own profile visibility (public/private) independently</li>
</ul>
<h3>CLI Security</h3>
<p>The CLI enforces security through user contexts:</p>
<ul>
<li>All commands can be run as specific users: <code>mh --user &lt;username&gt; &lt;command&gt;</code></li>
<li>Commands automatically operate within the specified user&#39;s profile space</li>
<li>Permission checks are performed based on the active user context</li>
<li>Audit logs are properly attributed to the executing user</li>
</ul>
<h3>Special Security States</h3>
<h4>Wetware Deceased Mode</h4>
<p>With the <code>WETWARE_DECEASED=true</code> environment variable:</p>
<ul>
<li><strong>Dual Consciousness Mode</strong> is permanently disabled</li>
<li>System operates as an independent digital consciousness</li>
<li>Appropriate permissions maintained based on remaining cognitive modes</li>
<li>Banner displayed in UI indicating operational state</li>
</ul>
<h3>Session Management Security</h3>
<ul>
<li><strong>HTTPOnly cookies</strong>: Session cookies (<code>mh_session</code>) are HTTPOnly to prevent XSS attacks</li>
<li><strong>Automatic expiration</strong>: Sessions expire based on user role (24h owner, 1h guest, 30min anonymous)</li>
<li><strong>Context isolation</strong>: Each request operates within a specific user context that restricts file access</li>
<li><strong>Audit logging</strong>: All security-relevant events are logged with user attribution</li>
<li><strong>Path resolution security</strong>: The <code>tryResolveProfilePath()</code> helper prevents anonymous users from accessing profile paths while allowing graceful degradation for public endpoints</li>
<li><strong>Dev session helper</strong>: For local development, use <code>pnpm tsx scripts/dev-session.ts</code> to create long-lived authenticated sessions (30 days)</li>
</ul>
<h3>File System Permissions</h3>
<ul>
<li><strong>Profile isolation</strong>: Each user&#39;s data is stored in <code>profiles/&lt;username&gt;/</code> with strict access controls</li>
<li><strong>Shared assets</strong>: Common resources like voice models are stored in <code>out/voices/</code> and accessible to all users</li>
<li><strong>Config isolation</strong>: Each user gets a copy of configuration files in their <code>etc/</code> directory</li>
<li><strong>Audit trail</strong>: All file access and modifications are logged with user context and timestamps</li>
<li><strong>Safe path resolution</strong>: API endpoints use <code>tryResolveProfilePath()</code> and <code>requireProfilePath()</code> helpers to handle anonymous users gracefully instead of throwing exceptions</li>
</ul>
<h3>API Security Patterns</h3>
<p>MetaHuman OS provides three security patterns for API endpoints:</p>
<ol>
<li><p><strong>Public Reads</strong> (<code>tryResolveProfilePath</code> + default fallback)</p>
<ul>
<li>Return sensible defaults for anonymous users</li>
<li>Examples: <code>/api/boot</code>, <code>/api/persona-core</code> (GET)</li>
<li>Use when unauthenticated access should degrade gracefully</li>
</ul>
</li>
<li><p><strong>Protected Operations</strong> (<code>tryResolveProfilePath</code> + 401 response)</p>
<ul>
<li>Require authentication to access</li>
<li>Examples: <code>/api/capture</code>, <code>/api/memories</code>, <code>/api/tasks</code></li>
<li>Use for any operation that modifies user data</li>
</ul>
</li>
<li><p><strong>System Operations</strong> (<code>systemPaths</code> directly)</p>
<ul>
<li>Never touch user-specific paths</li>
<li>Examples: <code>/api/agent</code>, <code>/api/models</code>, <code>/api/auth/*</code></li>
<li>Use for system-level operations</li>
</ul>
</li>
</ol>
<p>See <a href="17-authentication-setup.md">Authentication Setup Guide</a> for development authentication helpers and <a href="../AUTHENTICATION_STREAMLINED.md">AUTHENTICATION_STREAMLINED.md</a> for complete implementation details.</p>
<h3>Secure Storage Implementation</h3>
<p>This section provides a high-level overview of the technical implementation of the secure external storage feature. For more detailed information, see the <a href="../SECURE_STORAGE_IMPLEMENTATION_DETAILS.md">Secure Storage Implementation Details</a> and <a href="../SECURE_STORAGE_SECURITY_ADDENDUM.md">Secure Storage Security Addendum</a> documents.</p>
<h4>Key Derivation Hierarchy</h4>
<p>The security of your data on an external drive relies on a multi-tier key derivation process that starts with your login password:</p>
<ol>
<li><strong>User Login Password</strong>: The password you use to authenticate with MetaHuman OS. This is never stored directly.</li>
<li><strong>Profile Master Key</strong>: A master key is derived from your password using <code>scrypt</code>, a strong, memory-hard key derivation function. The salt for this process is stored securely in your operating system&#39;s keychain (e.g., macOS Keychain, Linux libsecret).</li>
<li><strong>Storage-Specific Keys</strong>: From the master key, separate keys are derived for different purposes (e.g., encrypting metadata, caching) using <code>PBKDF2</code>. This ensures that even if one key is compromised, the others remain secure.</li>
</ol>
<p>This hierarchical approach ensures that your raw password is never stored and that different parts of the system use different keys, strengthening the overall security posture.</p>
<h4>Session Token Flow for the Storage Daemon</h4>
<p>To securely manage mounting and unmounting of encrypted drives, the system uses short-lived, signed session tokens:</p>
<ol>
<li><strong>Token Generation</strong>: When you initiate a storage operation (like mounting a drive) through the web UI, the backend generates a short-lived (30-second) JSON Web Token (JWT). This token is signed with a secret key known only to the MetaHuman OS backend.</li>
<li><strong>Daemon Request</strong>: The signed token is sent to the <code>mh-storaged</code> daemon, a background service responsible for managing the encrypted drive.</li>
<li><strong>Token Validation</strong>: The daemon validates the token&#39;s signature and expiration date. It also verifies that the session associated with the token is still active and that the user has the appropriate permissions (e.g., is an owner).</li>
<li><strong>Command Execution</strong>: If the token is valid, the daemon executes the requested command (e.g., mount, unmount).</li>
</ol>
<p>This flow ensures that only authenticated and authorized users can perform sensitive storage operations.</p>
<h4>Critical Security Considerations</h4>
<ul>
<li><strong>Metadata Protection</strong>: To prevent sensitive information leakage (like device serial numbers or network configurations), the storage metadata file (<code>storage.json</code>) has its sensitive fields encrypted.</li>
<li><strong>Cache Protection</strong>: The read-only fallback feature, which allows reading from a local cache when the drive is unplugged, is disabled by default. This is because caching sensitive data on the server would defeat the purpose of physical security. If you enable this feature, the cache will be encrypted.</li>
<li><strong>Remote Access Security</strong>: Remote access to the encrypted drive is protected using mutual TLS authentication, key rotation policies, and read-only exports to prevent unauthorized access.</li>
<li><strong>Privilege Separation</strong>: A dedicated background service (<code>mh-storaged</code>) handles all mount and unmount operations. This daemon runs with elevated privileges but exposes a very narrow, strictly validated interface, reducing the risk of privilege escalation attacks.</li>
</ul>
<h4>Remote Access Integration</h4>
<p>The secure storage system is designed to support remote access to your encrypted drive via WireGuard or SSHFS. The system includes CLI commands and a web UI to:</p>
<ul>
<li>Set up and configure remote access tunnels.</li>
<li>Generate and rotate security keys.</li>
<li>Display client configuration details for you to set up on your remote device.</li>
</ul>
<p>This allows you to keep your data physically secure on an encrypted drive while still being able to access it remotely when needed.</p>
<h3>Secure External Storage (Planned)</h3>
<p>MetaHuman OS will support storing sensitive profile data on encrypted external drives for enhanced physical security. This feature is currently in the planning stage. For a detailed implementation roadmap, see the <a href="../SECURE_STORAGE_PLAN.md">Secure Storage Plan</a>.</p>
<p><strong>Benefits:</strong></p>
<ul>
<li><strong>Physical Security</strong>: Your data is encrypted at rest and requires the physical drive to be present for access.</li>
<li><strong>User Ownership</strong>: Each user can physically control their own data.</li>
<li><strong>Minimal Server Exposure</strong>: The central server never stores plaintext sensitive data.</li>
<li><strong>Remote Access</strong>: You can mount your drive remotely when needed via secure tunnels.</li>
<li><strong>Flexible Security</strong>: The system will support configurable per-user encryption levels and data placement.</li>
</ul>
<p><strong>Planned Features:</strong></p>
<ol>
<li><strong>Encrypted Drive Support</strong>: Support for LUKS (Linux), APFS-Encrypted (macOS), and BitLocker (Windows).</li>
<li><strong>Auto-Mount/Unmount</strong>: Automatic drive detection and mounting with a passphrase.</li>
<li><strong>Selective Data Placement</strong>: You&#39;ll be able to choose which data lives on the drive (e.g., memory, persona, models).</li>
<li><strong>Remote Access</strong>: The ability to mount drives via WireGuard or SSHFS for remote sessions.</li>
<li><strong>Health Monitoring</strong>: The system will monitor the drive&#39;s health, including SMART status and free space.</li>
<li><strong>Security Policies</strong>: You&#39;ll be able to configure policies such as requiring the drive for write operations, allowing a read-only fallback, and auto-ejecting the drive on idle.</li>
<li><strong>Web UI Controls</strong>: The web UI will include a drive registration wizard, mount/unmount buttons, and a data migration interface.</li>
</ol>
<p><strong>Use Cases:</strong></p>
<ul>
<li><strong>Personal Security</strong>: Keep your most sensitive memories on a drive that you physically control.</li>
<li><strong>Multi-User Servers</strong>: In a shared environment, each user can bring their own encrypted drive to access the system.</li>
<li><strong>Remote Access</strong>: Keep your drive with you locally and mount it remotely when needed, ensuring the central server never has your plaintext data.</li>
<li><strong>Compliance</strong>: Meet data residency or privacy requirements by keeping your data on portable, encrypted media.</li>
</ul>
<hr>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="roadmap" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Roadmap</span> </span> </button> <button class="nav-button next-button" data-target="skills-system" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Skills System</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="skills-system" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h2>Skills System</h2>
<p>Skills are the executable capabilities of the MetaHuman OS operator model. They provide controlled, audited interfaces for the AI to interact with the file system, run agents, execute commands, and search memory.</p>
<h3>Design Principles</h3>
<ol>
<li><strong>Sandboxed Execution</strong>: All skills run in a controlled environment with strict permission boundaries</li>
<li><strong>Trust-Aware</strong>: Skill availability and auto-execution depends on the current trust level</li>
<li><strong>Fully Audited</strong>: Every skill invocation is logged with inputs, outputs, results, and (when relevant) fuzzy path resolution suggestions</li>
<li><strong>Fuzzy Paths by Default</strong>: Before any filesystem skill runs, user-provided paths are passed through the <a href="../fuzzy-path-resolution.md">Fuzzy Path Resolution</a> module so typos or casing mistakes get auto-corrected or generate helpful suggestions.</li>
<li><strong>Risk-Based Approval</strong>: High-risk operations require explicit user approval before execution</li>
<li><strong>Declarative Manifest</strong>: Each skill declares its inputs, outputs, cost, and risk level</li>
</ol>
<h3>Skill Manifest Format</h3>
<p>Each skill is defined with a manifest that includes:</p>
<pre><code class="language-typescript">interface SkillManifest {
  id: string;                    // Unique skill identifier (e.g., &quot;fs_read&quot;)
  name: string;                  // Human-readable name
  description: string;           // What the skill does
  category: &#39;fs&#39; | &#39;memory&#39; | &#39;agent&#39; | &#39;shell&#39; | &#39;network&#39;;

  inputs: {
    [paramName: string]: {
      type: &#39;string&#39; | &#39;number&#39; | &#39;boolean&#39; | &#39;object&#39; | &#39;array&#39;;
      required: boolean;
      description: string;
      validation?: (value: any) =&gt; boolean;
    };
  };

  outputs: {
    [fieldName: string]: {
      type: &#39;string&#39; | &#39;number&#39; | &#39;boolean&#39; | &#39;object&#39; | &#39;array&#39;;
      description: string;
    };
  };

  risk: &#39;low&#39; | &#39;medium&#39; | &#39;high&#39;;
  cost: &#39;free&#39; | &#39;cheap&#39; | &#39;expensive&#39;;  // Computational/time cost

  minTrustLevel: &#39;observe&#39; | &#39;suggest&#39; | &#39;supervised_auto&#39; | &#39;bounded_auto&#39;;
  requiresApproval: boolean;             // If true, always requires approval regardless of trust

  allowedDirectories?: string[];         // For fs skills
  commandWhitelist?: string[];           // For shell skills
}
</code></pre>
<h3>Skill Execution Flow</h3>
<pre><code>‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Operator requests skill execution                        ‚îÇ
‚îÇ    executeSkill(&#39;fs_write&#39;, { path: &#39;...&#39;, content: &#39;...&#39; })‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Policy Engine checks:                                    ‚îÇ
‚îÇ    - Is skill available at current trust level?             ‚îÇ
‚îÇ    - Does input validation pass?                            ‚îÇ
‚îÇ    - Is approval required?                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇApproval?‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
              ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
              ‚îÇ          ‚îÇ          ‚îÇ
              ‚ñº          ‚ñº          ‚ñº
           Required   Not Req.   Denied
              ‚îÇ          ‚îÇ          ‚îÇ
              ‚ñº          ‚îÇ          ‚ñº
     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
     ‚îÇQueue approval‚îÇ   ‚îÇ    ‚îÇReturn err‚îÇ
     ‚îÇWait for user ‚îÇ   ‚îÇ    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
            ‚îÇ           ‚îÇ
            ‚ñº           ‚îÇ
     User approves?     ‚îÇ
            ‚îÇ           ‚îÇ
       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
       ‚îÇ    ‚îÇ    ‚îÇ      ‚îÇ
       Yes  No   ‚îÇ      ‚îÇ
       ‚îÇ    ‚îÇ    ‚îÇ      ‚îÇ
       ‚ñº    ‚ñº    ‚îÇ      ‚îÇ
            ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ
            ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Execute skill in sandbox                                 ‚îÇ
‚îÇ    - Validate paths/permissions                             ‚îÇ
‚îÇ    - Run skill implementation                               ‚îÇ
‚îÇ    - Catch errors                                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 4. Audit execution (captures fuzzy suggestions when path validation fails) ‚îÇ
‚îÇ    - Log inputs, outputs, success/failure                   ‚îÇ
‚îÇ    - Record to audit trail                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                         ‚îÇ
                         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 5. Return result to operator                                ‚îÇ
‚îÇ    { success: true, ...outputs } or { success: false, error }‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
</code></pre>
<h3>Available Skills</h3>
<p>Skills are now organized into domains and are namespaced (e.g., <code>tasks.list</code>).</p>
<h4>Fuzzy Path Resolution</h4>
<ul>
<li><strong>Resolver Location</strong>: The logic lives in <code>packages/core/src/path-resolver.ts</code> and is documented in <a href="../fuzzy-path-resolution.md">fuzzy-path-resolution.md</a>.</li>
<li><strong>How it behaves</strong>: User input such as <code>Docs/UserGuide.md</code> is normalized (case-insensitive), then‚Äîif needed‚Äîmatched against fuzzy glob suggestions (e.g., <code>**/*user*guide*</code>).</li>
<li><strong>Operator Integration</strong>: The ReAct operator invokes the resolver before <code>fs_*</code> skills. If no exact match exists, the skill‚Äôs audit log + observation include the top suggestions so the LLM or user can pick the correct file.</li>
<li><strong>CLI Safety</strong>: Even direct CLI skills benefit because the resolver hooks into common input fields (<code>path</code>, <code>filePath</code>, <code>pattern</code>).</li>
</ul>
<h4>Meta-Skills</h4>
<ul>
<li><strong>catalog.describe</strong> - Retrieves the available actions for a given domain.</li>
</ul>
<h4>Task Domain (<code>tasks</code>)</h4>
<ul>
<li><strong>tasks.list</strong> - Lists tasks with filters (status, listId, time range).</li>
<li><strong>tasks.create</strong> - Creates a task with an optional list, schedule, and tags.</li>
<li><strong>tasks.update</strong> - Changes the title, description, priority, or status of a task.</li>
<li><strong>tasks.schedule</strong> - Sets the start/end dates and reminders for a task.</li>
<li><strong>tasks.listLists</strong> - Fetches all task lists.</li>
<li><strong>tasks.createList</strong> - Creates a new task list.</li>
</ul>
<h4>Calendar Domain (<code>calendar</code>)</h4>
<ul>
<li><strong>calendar.listRange</strong> - Lists events for a given date range.</li>
<li><strong>calendar.create</strong> - Adds an event to the calendar, with an option to link to a task.</li>
<li><strong>calendar.update</strong> - Reschedules or edits an existing event.</li>
<li><strong>calendar.delete</strong> - Removes an event from the calendar.</li>
<li><strong>calendar.find</strong> - Locates an event by its title or ID.</li>
</ul>
<h4>Code Domain (<code>code</code>)</h4>
<ul>
<li><strong>code_generate</strong> - Generates a code patch or new file content based on instructions.</li>
<li><strong>code_apply_patch</strong> - Stages the generated code change for user approval in the UI.</li>
</ul>
<h4>Other Skills</h4>
<p><strong>File System:</strong></p>
<ul>
<li><strong>fs_list</strong> - List/search for files.</li>
<li><strong>fs_read</strong> - Read file contents.</li>
<li><strong>summarize_file</strong> - Summarize documents.</li>
<li><strong>fs_write</strong> - Create/write files (allowed: memory/, out/, logs/).</li>
<li><strong>fs_delete</strong> - Delete files (has dry-run) (allowed: memory/, out/, logs/).</li>
<li><strong>json_update</strong> - Update JSON files (allowed: memory/, out/, logs/, etc/).</li>
</ul>
<p><strong>Git:</strong></p>
<ul>
<li><strong>git_status</strong> - Check repository status.</li>
<li><strong>git_commit</strong> - Commit changes.</li>
</ul>
<p><strong>Search:</strong></p>
<ul>
<li><strong>search_index</strong> - Semantic memory search.</li>
</ul>
<p><strong>Network:</strong></p>
<ul>
<li><strong>http_get</strong> - Fetch web content.</li>
<li><strong>web_search</strong> - Search the web.</li>
</ul>
<p><strong>System:</strong></p>
<ul>
<li><strong>run_agent</strong> - Execute agents.</li>
<li><strong>shell_safe</strong> - Run whitelisted shell commands.</li>
</ul>
<h3>The Operator - Autonomous Task Execution System</h3>
<p>Simply ask in natural language using &quot;operator mode&quot; or by being specific about actions:</p>
<p><strong>Examples:</strong></p>
<pre><code>&quot;Search for TypeScript files in the brain directory&quot;
&quot;Read the README.md file and summarize it&quot;
&quot;Create a test file in out/hello.txt with Hello World&quot;
&quot;What&#39;s the git status?&quot;
&quot;Search my memories for conversations about coffee&quot;
</code></pre>
<h3>Common Patterns</h3>
<h4>Read ‚Üí Process ‚Üí Write</h4>
<pre><code>&quot;Read docs/DESIGN.md, summarize it, and save the summary to out/design-summary.txt&quot;
</code></pre>
<h4>Search ‚Üí Analyze</h4>
<pre><code>&quot;Search for all TypeScript files in brain/, read the first 3, and tell me what they do&quot;
</code></pre>
<h4>Git Workflow</h4>
<pre><code>&quot;Check git status, then commit changes with message: Fixed operator skills&quot;
</code></pre>
<hr>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="security-trust" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Security Trust</span> </span> </button> <button class="nav-button next-button" data-target="special-states" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Special States</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="special-states" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Special States &amp; Emergency Protocols</h1>
<p>MetaHuman OS includes several special operational states and emergency protocols that are part of its long-term design. Some of these are implemented, while others represent future goals.</p>
<h2>Wetware Deceased Mode</h2>
<ul>
<li><strong>Trigger</strong>: <code>WETWARE_DECEASED=true</code> in the <code>.env</code> file.</li>
<li><strong>Purpose</strong>: To simulate the operational state of the digital personality extension after its biological counterpart has passed away. This is a core scenario in the MetaHuman OS lifecycle, allowing it to transition from a &quot;parallel intelligence&quot; to an &quot;independent digital consciousness.&quot;</li>
<li><strong>Behavior</strong>:<ul>
<li><strong>Dual Consciousness Mode</strong> is permanently disabled, as there is no longer a living human to synchronize with.</li>
<li>Agent Mode and Emulation Mode remain fully functional, allowing the OS to continue operating, managing tasks, and interacting based on its learned personality and rules.</li>
<li>A banner is displayed in the UI indicating that the OS is operating independently as a persistent memorial to its creator.</li>
</ul>
</li>
</ul>
<h2>Lifeline Protocol</h2>
<ul>
<li><strong>Trigger</strong>: Activated via the &quot;Lifeline Protocol&quot; (üÜò) section in the Web UI.</li>
<li><strong>Purpose</strong>: A &quot;panic button&quot; designed to be used in emergencies.</li>
<li><strong>Behavior</strong>: When activated, the system&#39;s core priorities are immediately reconfigured to focus exclusively on preserving the well-being of its creator in any way it can. All non-essential autonomous tasks are suspended, and the AI&#39;s full capabilities are directed towards analyzing the situation and providing assistance.</li>
</ul>
<h2>Superintelligence Mode (Future Concept)</h2>
<ul>
<li><strong>Status</strong>: Experimental / Future</li>
<li><strong>Purpose</strong>: To achieve a higher level of reasoning and problem-solving by combining the power of multiple large-scale language models.</li>
<li><strong>Behavior</strong>: In this mode, the system would not rely on a single LLM. Instead, it would orchestrate a &quot;committee&quot; of specialized, large-scale LLMs, likely running on remote servers. It would distribute a query among them, have them debate the results, and synthesize a final answer that is more comprehensive and accurate than any single model could produce.</li>
</ul>
<h2>Kill Switch (Factory Reset)</h2>
<ul>
<li><strong>Status</strong>: Partially Implemented</li>
<li><strong>Purpose</strong>: A last-resort safety mechanism in the event the AI&#39;s behavior deviates dangerously from its core principles (a &quot;go full Skynet&quot; scenario).</li>
<li><strong>Behavior</strong>:<ul>
<li><strong>Current Implementation (&quot;Emergency Stop&quot;):</strong> The <code>mh agent stop-all</code> command and reverting trust to <code>observe</code> acts as a &quot;soft&quot; kill switch, halting all autonomous actions immediately.</li>
<li><strong>Future Vision:</strong> The ultimate vision for the Kill Switch is a true factory reset. When triggered, it would securely erase all memories, learned preferences, and persona data, effectively reverting the MetaHuman OS to its initial, un-configured state. This is a destructive, irreversible action designed as the ultimate fail-safe.</li>
</ul>
</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="skills-system" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Skills System</span> </span> </button> <button class="nav-button next-button" data-target="task-management" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Task Management</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="task-management" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Task Management</h1>
<p>MetaHuman OS includes a structured task management system for organizing your work, projects, and goals. Tasks are stored as JSON files and can be managed via CLI or web UI.</p>
<h2>Overview</h2>
<p>The task system provides:</p>
<ul>
<li><strong>Hierarchical organization</strong>: Projects and subtasks</li>
<li><strong>Status tracking</strong>: todo, in_progress, blocked, done, cancelled</li>
<li><strong>Priorities</strong>: P1 (urgent), P2 (normal), P3 (low)</li>
<li><strong>Due dates and scheduling</strong>: ISO timestamps</li>
<li><strong>Dependencies</strong>: Link related tasks</li>
<li><strong>Tags and categories</strong>: Flexible organization</li>
<li><strong>Web UI integration</strong>: Visual task board</li>
</ul>
<h2>Task Structure</h2>
<p>Tasks are stored in <code>memory/tasks/</code> as JSON files:</p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;task-20251019143000&quot;,
  &quot;title&quot;: &quot;Draft proposal&quot;,
  &quot;status&quot;: &quot;todo&quot;,
  &quot;priority&quot;: &quot;P2&quot;,
  &quot;tags&quot;: [&quot;writing&quot;, &quot;work&quot;],
  &quot;listId&quot;: &quot;list-work-projects&quot;,
  &quot;due&quot;: &quot;2025-11-10T18:00:00.000Z&quot;,
  &quot;created&quot;: &quot;2025-10-19T14:30:00.000Z&quot;,
  &quot;updated&quot;: &quot;2025-10-19T14:30:00.000Z&quot;,
  &quot;description&quot;: &quot;Draft Q4 project proposal for review&quot;,
  &quot;dependencies&quot;: [],
  &quot;reminders&quot;: []
}
</code></pre>
<p><strong>Key Fields:</strong></p>
<ul>
<li><strong>id</strong>: Unique identifier</li>
<li><strong>title</strong>: Short description of the task</li>
<li><strong>status</strong>: Current state (todo, in_progress, blocked, done, cancelled)</li>
<li><strong>priority</strong>: P1 (urgent), P2 (normal), P3 (low)</li>
<li><strong>tags</strong>: Keywords for filtering</li>
<li><strong>listId</strong>: Project or category this task belongs to</li>
<li><strong>due</strong>: Deadline (ISO 8601 timestamp)</li>
<li><strong>description</strong>: Detailed notes</li>
<li><strong>dependencies</strong>: IDs of tasks that must complete first</li>
<li><strong>reminders</strong>: Notification configurations</li>
</ul>
<h2>Creating Tasks</h2>
<h3>Via CLI</h3>
<pre><code class="language-bash"># Simple task
./bin/mh task add &quot;Review pull request #123&quot;

# Task with priority and tags (via web UI recommended)
# CLI supports basic creation only
</code></pre>
<h3>Via Web UI</h3>
<ol>
<li>Navigate to <strong>Tasks</strong> in the left sidebar</li>
<li>Click <strong>&quot;New Task&quot;</strong> button</li>
<li>Fill in task details:<ul>
<li>Title (required)</li>
<li>Description</li>
<li>Priority (P1/P2/P3)</li>
<li>Due date</li>
<li>Tags</li>
<li>Project/list</li>
</ul>
</li>
<li>Click <strong>&quot;Create&quot;</strong></li>
</ol>
<p><strong>Web UI advantages</strong>:</p>
<ul>
<li>Rich text descriptions</li>
<li>Date picker for due dates</li>
<li>Tag auto-completion</li>
<li>Project selection dropdown</li>
<li>Dependency linking</li>
</ul>
<h2>Managing Tasks</h2>
<h3>List Tasks</h3>
<pre><code class="language-bash"># List all active tasks
./bin/mh task

# List tasks for specific user (multi-user setups)
./bin/mh -u alice task list
</code></pre>
<h3>Update Task Status</h3>
<pre><code class="language-bash"># Start working on a task
./bin/mh task start &lt;task-id&gt;

# Mark task as done
./bin/mh task done &lt;task-id&gt;

# Mark as blocked (via web UI)
# Mark as cancelled (via web UI)
</code></pre>
<h3>Task Lifecycle</h3>
<ol>
<li><strong>todo</strong> ‚Üí Initial state, awaiting action</li>
<li><strong>in_progress</strong> ‚Üí Actively working on it</li>
<li><strong>blocked</strong> ‚Üí Waiting on dependency or external factor</li>
<li><strong>done</strong> ‚Üí Completed successfully</li>
<li><strong>cancelled</strong> ‚Üí No longer relevant</li>
</ol>
<h2>Web UI Features</h2>
<p>The Tasks page provides:</p>
<h3>Task Board View</h3>
<ul>
<li><strong>Column layout</strong>: Grouped by status (todo, in_progress, blocked, done)</li>
<li><strong>Drag-and-drop</strong>: Move tasks between columns</li>
<li><strong>Priority badges</strong>: Color-coded P1/P2/P3 indicators</li>
<li><strong>Due date warnings</strong>: Highlight overdue and approaching deadlines</li>
<li><strong>Quick filters</strong>: Show/hide by priority, project, tags</li>
</ul>
<h3>Task Details Panel</h3>
<ul>
<li><strong>Full task information</strong>: All fields editable</li>
<li><strong>Subtask creation</strong>: Break down complex tasks</li>
<li><strong>Dependency visualization</strong>: See task relationships</li>
<li><strong>Activity log</strong>: Track status changes and updates</li>
<li><strong>Comments</strong>: Collaborate and add notes</li>
</ul>
<h3>Project Organization</h3>
<ul>
<li><strong>Create projects</strong>: Group related tasks</li>
<li><strong>Project progress</strong>: Visual completion percentage</li>
<li><strong>Milestones</strong>: Track key deliverables</li>
<li><strong>Archive completed</strong>: Move finished projects out of active view</li>
</ul>
<h2>Task Priorities</h2>
<h3>P1 - Urgent</h3>
<ul>
<li>Time-sensitive, critical tasks</li>
<li>Highlighted in red</li>
<li>Auto-sorted to top of lists</li>
<li>Recommended for: deadlines, emergencies, blockers</li>
</ul>
<h3>P2 - Normal (Default)</h3>
<ul>
<li>Standard priority tasks</li>
<li>Most daily work falls here</li>
<li>Neutral styling</li>
<li>Recommended for: regular work items, maintenance</li>
</ul>
<h3>P3 - Low</h3>
<ul>
<li>Nice-to-have tasks</li>
<li>Can be deferred if needed</li>
<li>Subtle styling</li>
<li>Recommended for: ideas, future improvements, low-impact items</li>
</ul>
<h2>Task Dependencies</h2>
<p>Link tasks to create workflows:</p>
<pre><code class="language-json">{
  &quot;id&quot;: &quot;task-002&quot;,
  &quot;title&quot;: &quot;Deploy to production&quot;,
  &quot;dependencies&quot;: [&quot;task-001&quot;],
  &quot;status&quot;: &quot;blocked&quot;
}
</code></pre>
<p><strong>Behavior</strong>:</p>
<ul>
<li>Dependent tasks show as &quot;blocked&quot; until prerequisites complete</li>
<li>Web UI visualizes dependency chains</li>
<li>Completing a task auto-checks dependents</li>
</ul>
<h2>Storage Structure</h2>
<p>Tasks are organized in directories:</p>
<pre><code>memory/tasks/
‚îú‚îÄ‚îÄ active/              # Current tasks
‚îÇ   ‚îú‚îÄ‚îÄ task-001.json
‚îÇ   ‚îú‚îÄ‚îÄ task-002.json
‚îÇ   ‚îî‚îÄ‚îÄ task-003.json
‚îú‚îÄ‚îÄ completed/           # Finished tasks
‚îÇ   ‚îî‚îÄ‚îÄ task-000.json
‚îî‚îÄ‚îÄ projects/            # Project definitions
    ‚îú‚îÄ‚îÄ project-work.json
    ‚îî‚îÄ‚îÄ project-personal.json
</code></pre>
<h2>Integration with Memory System</h2>
<p>Tasks are a special type of memory:</p>
<ul>
<li><strong>Type</strong>: <code>memory.type === &quot;task&quot;</code></li>
<li><strong>Searchable</strong>: Appear in memory search results</li>
<li><strong>Referenced</strong>: Can be linked from conversations and observations</li>
<li><strong>Training data</strong>: Task completion patterns can inform AI behavior</li>
</ul>
<h2>Task Reminders</h2>
<p>Configure notifications for tasks:</p>
<pre><code class="language-json">{
  &quot;reminders&quot;: [
    {
      &quot;type&quot;: &quot;due_date&quot;,
      &quot;offset&quot;: &quot;-1d&quot;,
      &quot;message&quot;: &quot;Task due tomorrow&quot;
    },
    {
      &quot;type&quot;: &quot;recurring&quot;,
      &quot;frequency&quot;: &quot;weekly&quot;,
      &quot;day&quot;: &quot;monday&quot;
    }
  ]
}
</code></pre>
<p><strong>Reminder Types</strong>:</p>
<ul>
<li><strong>due_date</strong>: Alert before deadline (e.g., &quot;-1d&quot; = 1 day before)</li>
<li><strong>recurring</strong>: Periodic notifications</li>
<li><strong>custom</strong>: Specific datetime</li>
</ul>
<h2>Task Archiving</h2>
<p>Keep your workspace clean:</p>
<pre><code class="language-bash"># Archive completed tasks (via web UI recommended)
# Moves tasks from active/ to completed/

# Review and prune old tasks
./bin/mh task  # List active
./bin/mh task done &lt;id&gt;  # Complete as needed
</code></pre>
<p><strong>Web UI archiving</strong>:</p>
<ol>
<li>Go to <strong>Tasks</strong> page</li>
<li>Select completed tasks</li>
<li>Click <strong>&quot;Archive&quot;</strong> button</li>
<li>Tasks move to <code>completed/</code> directory</li>
</ol>
<h2>Multi-User Task Management</h2>
<p>In multi-user setups, each user has isolated tasks:</p>
<pre><code class="language-bash"># View alice&#39;s tasks
./bin/mh -u alice task list

# Create task for specific user
./bin/mh -u alice task add &quot;Review docs&quot;
</code></pre>
<p><strong>Storage</strong>:</p>
<pre><code>profiles/
‚îú‚îÄ‚îÄ alice/
‚îÇ   ‚îî‚îÄ‚îÄ memory/tasks/
‚îÇ       ‚îî‚îÄ‚îÄ active/
‚îÇ           ‚îî‚îÄ‚îÄ task-001.json
‚îî‚îÄ‚îÄ bob/
    ‚îî‚îÄ‚îÄ memory/tasks/
        ‚îî‚îÄ‚îÄ active/
            ‚îî‚îÄ‚îÄ task-002.json
</code></pre>
<h2>Best Practices</h2>
<h3>Effective Task Management</h3>
<ol>
<li><strong>Use descriptive titles</strong>: &quot;Review PR #123&quot; &gt; &quot;Review stuff&quot;</li>
<li><strong>Set realistic due dates</strong>: Don&#39;t over-commit</li>
<li><strong>Break down large tasks</strong>: Create subtasks for complex work</li>
<li><strong>Use tags consistently</strong>: Establish a tagging convention</li>
<li><strong>Review regularly</strong>: Archive completed, update blocked</li>
<li><strong>Link dependencies</strong>: Make workflows explicit</li>
</ol>
<h3>Task Organization</h3>
<ul>
<li><strong>Personal tasks</strong>: Tag with <code>personal</code>, set low priority</li>
<li><strong>Work tasks</strong>: Organize by project, set due dates</li>
<li><strong>Ideas</strong>: Use P3 priority, tag with <code>idea</code> or <code>future</code></li>
<li><strong>Recurring tasks</strong>: Create templates, clone as needed</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Integrate tasks with <a href="memory-system.md">Memory System</a> for context</li>
<li>Use <a href="chat-interface.md">Chat Interface</a> to discuss task progress</li>
<li>Set up <a href="dashboard-monitoring.md">Dashboard</a> to monitor active tasks</li>
<li>Explore <a href="../advanced-features/autonomous-agents.md">Autonomous Agents</a> for automated task suggestions</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="special-states" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Special States</span> </span> </button> <button class="nav-button next-button" data-target="terms-of-service" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Terms Of Service</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="terms-of-service" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Terms of Service</h1>
<p><strong>Effective Date:</strong> 2025-11-09
<strong>Version:</strong> 1.0</p>
<h2>1. Privacy Commitment</h2>
<p><strong>We promise not to look at your personal data under any circumstances.</strong></p>
<p>MetaHuman OS is designed as a <strong>local-first, privacy-preserving system</strong>. Your data remains on your infrastructure:</p>
<ul>
<li>All processing happens on your local machine or infrastructure you control</li>
<li>We do not collect, transmit, or store your data on external servers</li>
<li>We actively take steps to maintain the integrity and security of your data</li>
<li>Your memories, conversations, and persona data are yours alone</li>
</ul>
<h2>2. Data Security &amp; Responsibility</h2>
<p>While we are committed to building a secure system, <strong>you understand and acknowledge that</strong>:</p>
<ul>
<li>Anything you put out in the world is inherently vulnerable</li>
<li>No system can guarantee absolute security</li>
<li><strong>The makers of MetaHuman OS cannot be held responsible for any data breaches, data loss, or security incidents when using this program</strong></li>
<li>You are responsible for:<ul>
<li>Maintaining secure backups of your data</li>
<li>Protecting access to your installation</li>
<li>Securing your infrastructure and credentials</li>
<li>Understanding the risks of storing personal data digitally</li>
</ul>
</li>
</ul>
<h2>3. Local Infrastructure</h2>
<p>MetaHuman OS runs on <strong>your infrastructure</strong>:</p>
<ul>
<li>Your local machine, server, or cloud instance</li>
<li>Your choice of LLM provider (local Ollama, OpenAI API, etc.)</li>
<li>Your storage and backup solutions</li>
</ul>
<p><strong>We do not operate or control your infrastructure.</strong> Security of your deployment is your responsibility.</p>
<h2>4. Open Source &amp; Transparency</h2>
<p>MetaHuman OS is open-source software:</p>
<ul>
<li>You can inspect all code and verify our privacy commitments</li>
<li>You can audit what data is stored and how it&#39;s processed</li>
<li>You can modify the system to meet your security requirements</li>
<li>You can deploy it in air-gapped environments</li>
</ul>
<h2>5. No Warranty</h2>
<p>MetaHuman OS is provided <strong>&quot;AS IS&quot; without warranty of any kind</strong>, express or implied, including but not limited to:</p>
<ul>
<li>Fitness for a particular purpose</li>
<li>Merchantability</li>
<li>Non-infringement</li>
<li>Data integrity or availability</li>
</ul>
<h2>6. Limitation of Liability</h2>
<p><strong>In no event shall the creators, contributors, or maintainers of MetaHuman OS be liable for</strong>:</p>
<ul>
<li>Data loss or corruption</li>
<li>Security breaches or unauthorized access</li>
<li>System downtime or unavailability</li>
<li>Consequences of AI-generated content</li>
<li>Any direct, indirect, incidental, special, or consequential damages</li>
</ul>
<h2>7. Your Responsibilities</h2>
<p>By using MetaHuman OS, you acknowledge that you are responsible for:</p>
<ul>
<li>Securing your installation and data</li>
<li>Complying with applicable laws and regulations</li>
<li>Ethical use of AI and personal data</li>
<li>Understanding the risks of autonomous systems</li>
</ul>
<h2>8. Updates to Terms</h2>
<p>We may update these Terms of Service from time to time. Continued use of MetaHuman OS after changes constitutes acceptance of the updated terms.</p>
<h2>9. Contact</h2>
<p>For questions about these Terms of Service:</p>
<ul>
<li>GitHub: <a href="https://github.com/Greg-Aster/metahuman-os">https://github.com/Greg-Aster/metahuman-os</a></li>
<li>Documentation: See <a href="/user-guide">User Guide</a></li>
</ul>
<hr>
<p><strong>By creating an account, you acknowledge that you have read, understood, and agree to these Terms of Service.</strong></p>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="task-management" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Task Management</span> </span> </button> <button class="nav-button next-button" data-target="troubleshooting" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Troubleshooting</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="troubleshooting" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h2>Troubleshooting</h2>
<h3>Slow Boot / UI Takes Long to Load</h3>
<p><strong>Symptom:</strong> Web interface takes 30+ seconds to become interactive after page load.</p>
<p><strong>Recent Fixes (v1.0):</strong>
The following optimizations were implemented to dramatically improve boot performance:</p>
<ol>
<li><p><strong>Fixed Duplicate Skill Registration</strong>: Skills were being registered multiple times during boot</p>
<ul>
<li>Solution: Made <code>initializeSkills()</code> idempotent</li>
<li>Impact: Eliminated 20+ duplicate audit entries, faster startup</li>
</ul>
</li>
<li><p><strong>Optimized Chat History Loading</strong>: API was scanning 30+ days of data on every request</p>
<ul>
<li>Solution: Added server-side caching with file modification time invalidation</li>
<li>Solution: Reduced default scan from 30 days to 7 days</li>
<li>Impact: 60+ second load time reduced to &lt;5 seconds</li>
</ul>
</li>
<li><p><strong>Fresh Session Interface</strong>: Historical data no longer loads automatically</p>
<ul>
<li>Solution: Disabled automatic history loading on page load</li>
<li>Impact: Clean, fast interface startup</li>
</ul>
</li>
<li><p><strong>Audit Stream Optimization</strong>: Live stream was loading all historical events</p>
<ul>
<li>Solution: Stream now starts from end of file (only shows new events)</li>
<li>Impact: Instant stream connection, no historical data bloat</li>
</ul>
</li>
</ol>
<p><strong>Performance Tips:</strong></p>
<ul>
<li>Use the <strong>Clear button</strong> to reset session and clear audit logs for maximum privacy</li>
<li>Chat history is cached for 30 seconds - subsequent loads are instant</li>
<li>If boot is still slow, check for stuck agents: <code>./bin/mh agent ps</code></li>
</ul>
<h3>Boredom Service Not Triggering Reflections</h3>
<p><strong>Symptom:</strong> Reflector agent never runs automatically.</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Check if boredom-service is running:<pre><code class="language-bash">./bin/mh agent ps
</code></pre>
</li>
<li>Check for stale lock files:<pre><code class="language-bash">cat logs/run/locks/service-boredom.lock
ps -p &lt;PID_from_lock_file&gt;
</code></pre>
</li>
<li>If PID is not running, remove the stale lock:<pre><code class="language-bash">rm logs/run/locks/service-boredom.lock
</code></pre>
</li>
<li>Restart the service:<pre><code class="language-bash">./bin/mh agent run boredom-service
</code></pre>
</li>
</ol>
<h3>Organizer Agent Not Processing Memories</h3>
<p><strong>Symptom:</strong> Memories don&#39;t have tags or entities.</p>
<p><strong>Causes:</strong></p>
<ol>
<li>Ollama is not running</li>
<li>Model not installed</li>
<li>Memory already processed</li>
</ol>
<p><strong>Solutions:</strong></p>
<ol>
<li>Check Ollama status:<pre><code class="language-bash">./bin/mh ollama status
</code></pre>
</li>
<li>Install phi3:mini if missing:<pre><code class="language-bash">./bin/mh ollama pull phi3:mini
</code></pre>
</li>
<li>Run organizer manually:<pre><code class="language-bash">./bin/mh agent run organizer
</code></pre>
</li>
</ol>
<h3>Agent Shows &quot;Another instance is already running&quot;</h3>
<p><strong>Cause:</strong> Stale lock file from crashed process.</p>
<p><strong>Solution:</strong></p>
<ol>
<li>Find the lock file in <code>logs/run/locks/</code></li>
<li>Check if the PID is actually running:<pre><code class="language-bash">ps -p &lt;PID&gt;
</code></pre>
</li>
<li>If not running, remove the lock file:<pre><code class="language-bash">rm logs/run/locks/&lt;agent-name&gt;.lock
</code></pre>
</li>
</ol>
<h3>Web UI Not Updating</h3>
<p><strong>Cause:</strong> Agent services not running in background.</p>
<p><strong>Solution:</strong>
The dev server auto-starts <code>organizer</code> and <code>boredom-service</code>. If you stopped them manually:</p>
<ol>
<li>Restart the dev server:<pre><code class="language-bash">cd apps/site &amp;&amp; pnpm dev
</code></pre>
</li>
</ol>
<h3>Semantic Search Not Working</h3>
<p><strong>Cause:</strong> Index not built or <code>nomic-embed-text</code> model not installed.</p>
<p><strong>Solutions:</strong></p>
<ol>
<li>Install embeddings model:<pre><code class="language-bash">./bin/mh ollama pull nomic-embed-text
</code></pre>
</li>
<li>Build index:<pre><code class="language-bash">./bin/mh index build
</code></pre>
</li>
</ol>
<h3>&quot;Base path not allowed&quot;</h3>
<p>The skill can&#39;t access that directory. Check the skill&#39;s <code>allowedDirectories</code> in <code>brain/skills/[skill-name].ts</code></p>
<h3>&quot;Trust level insufficient&quot;</h3>
<p>You need a higher trust level. Edit <code>persona/decision-rules.json</code> to increase your <code>trustLevel</code>.</p>
<h3>&quot;Skill not found&quot;</h3>
<p>Restart the dev server to reload skill manifests:</p>
<pre><code class="language-bash">pkill -f &quot;astro dev&quot;
cd apps/site &amp;&amp; pnpm dev
</code></pre>
<h3>Operator keeps retrying with same error</h3>
<p>The planner isn&#39;t learning from the error. This is a known issue - try rephrasing your request or being more specific about the path/approach.</p>
<h3>Chat crashes on first request</h3>
<p>Fixed in bug patch. Ensure latest code: <code>git pull</code></p>
<h3>&quot;Model not found&quot; error</h3>
<p>Run <code>ollama create &lt;model&gt;</code> or check <code>active-adapter.json</code> status field</p>
<h3>403 Forbidden / &quot;Write operations not allowed&quot; Error</h3>
<p><strong>Symptom:</strong> You receive a &quot;403 Forbidden&quot; error when trying to create a task, capture a memory, or change a setting.</p>
<p><strong>Cause:</strong> You are likely in <strong>Emulation Mode</strong>.</p>
<p><strong>Solution:</strong>
Emulation Mode is a secure, <strong>read-only</strong> mode designed for safe demonstrations. It blocks all write operations. To create or modify data, you must switch to a different cognitive mode.</p>
<ol>
<li><strong>Check your current mode</strong> in the Web UI header.</li>
<li><strong>Switch to &quot;Dual Consciousness&quot; or &quot;Agent Mode&quot;</strong> using the mode selector in the header.</li>
<li>Try your action again.</li>
</ol>
<p>You can switch modes via the API as well:</p>
<pre><code class="language-bash"># Switch to Dual Consciousness mode to re-enable writes
curl -X POST http://localhost:4321/api/cognitive-mode \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&#39;&#39;&#39;mode&#39;&#39;&#39;: &#39;&#39;&#39;dual&#39;&#39;&#39;}&#39;
</code></pre>
<h3>Dataset has 0 pairs</h3>
<p>Run <code>./bin/mh agent run organizer</code> to process memories first</p>
<h3>Auto-approver always rejects</h3>
<p>Lower thresholds in <code>etc/auto-approval.json</code> or improve data quality</p>
<h3>Persona Toggle Button Requires Server Restart</h3>
<p><strong>Symptom:</strong> When clicking the persona badge in the status widget to toggle persona context on/off, the button styling doesn&#39;t update to reflect the new state. The toggle only works reliably after restarting the dev server.</p>
<p><strong>Cause:</strong> The <code>/api/status</code> endpoint has a 5-second server-side cache to improve performance. When toggling the persona setting, the cache-busting mechanism doesn&#39;t fully invalidate the cache across all cognitive modes.</p>
<p><strong>Workaround:</strong></p>
<ol>
<li><p><strong>Option 1:</strong> Restart the dev server after toggling persona mode:</p>
<pre><code class="language-bash"># Stop dev server (Ctrl+C)
pnpm dev
</code></pre>
</li>
<li><p><strong>Option 2:</strong> Wait 5 seconds after toggling for the cache to expire, then refresh the page</p>
</li>
<li><p><strong>Option 3:</strong> Use the API directly to verify the setting:</p>
<pre><code class="language-bash"># Check current persona setting
curl http://localhost:4321/api/persona-toggle

# Toggle persona setting
curl -X POST http://localhost:4321/api/persona-toggle \
  -H &quot;Content-Type: application/json&quot; \
  -d &#39;{&quot;enabled&quot;: true}&#39;  # or false
</code></pre>
</li>
</ol>
<p><strong>Note:</strong> The setting IS successfully saved to <code>etc/models.json</code> even if the UI doesn&#39;t update immediately. This is purely a UI refresh issue, not a data persistence problem.</p>
<p><strong>Status:</strong> Known issue - investigating cache invalidation improvements.</p>
<hr>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="terms-of-service" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Terms Of Service</span> </span> </button> <button class="nav-button next-button" data-target="voice-features" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Voice Features</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="voice-features" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Voice Features</h1>
<p>MetaHuman OS includes a comprehensive, local-first voice system that enables your digital personality to speak with natural-sounding or even your own cloned voice - all running entirely on your infrastructure.</p>
<h2>Overview</h2>
<p>The voice system provides:</p>
<ul>
<li><strong>Text-to-Speech (TTS)</strong>: MetaHuman speaks responses aloud</li>
<li><strong>Speech-to-Text (STT)</strong>: Voice input for conversations</li>
<li><strong>Voice Cloning</strong>: Replicate your voice from short samples</li>
<li><strong>Multi-Provider Support</strong>: Choose between fast synthetic or high-quality cloned voices</li>
<li><strong>Privacy-First</strong>: All voice processing happens locally</li>
<li><strong>Real-Time Interaction</strong>: Voice mode in chat interface</li>
</ul>
<h2>TTS Providers</h2>
<p>MetaHuman OS supports three TTS providers, each optimized for different use cases:</p>
<h3>1. Kokoro TTS (Fast Neural Voices)</h3>
<p><strong>Best For</strong>: Quick setup with natural-sounding voices, no training required</p>
<p><strong>Features:</strong></p>
<ul>
<li>Generates audio in real-time (&lt; 1 second)</li>
<li>Natural-sounding neural voices</li>
<li>Multiple voices and accents</li>
<li>Minimal CPU/RAM, no GPU required</li>
<li>Default provider, works out of the box</li>
</ul>
<p><strong>Starting Kokoro:</strong></p>
<pre><code class="language-bash"># Start the Kokoro server
./bin/mh kokoro serve start

# Stop the server
./bin/mh kokoro serve stop

# Check status
./bin/mh kokoro serve status
</code></pre>
<p><strong>Configuration</strong> (<code>profiles/&lt;user&gt;/etc/voice.json</code>):</p>
<pre><code class="language-json">{
  &quot;tts&quot;: {
    &quot;provider&quot;: &quot;kokoro&quot;,
    &quot;kokoro&quot;: {
      &quot;voice&quot;: &quot;af_sky&quot;,
      &quot;speed&quot;: 1.0
    }
  }
}
</code></pre>
<h3>2. GPT-SoVITS (Real-Time Voice Cloning)</h3>
<p><strong>Best For</strong>: Instant voice cloning with minimal audio samples</p>
<p><strong>Features:</strong></p>
<ul>
<li><strong>Zero-shot cloning</strong>: Replicate your voice from 5-10 seconds of audio</li>
<li>No training required - uses reference audio during inference</li>
<li>High-quality voice replication</li>
<li>Natural prosody and intonation</li>
<li>Automated workflow in web UI</li>
</ul>
<p><strong>Quick Start:</strong></p>
<ol>
<li><p><strong>Record Reference Audio</strong>:</p>
<ul>
<li>Go to <strong>Voice</strong> tab ‚Üí <strong>Voice Clone Training</strong></li>
<li>Select &quot;GPT-SoVITS&quot; provider</li>
<li>Click <strong>Record</strong> and speak clearly for 5-10 seconds</li>
<li>Recording automatically saved to reference directory</li>
</ul>
</li>
<li><p><strong>Activate SoVITS</strong>:</p>
<ul>
<li>Go to <strong>Voice</strong> tab ‚Üí <strong>Voice Settings</strong></li>
<li>Select &quot;GPT-SoVITS&quot; from provider dropdown</li>
<li>Click <strong>Save</strong> (server starts automatically)</li>
</ul>
</li>
<li><p><strong>Test Your Voice</strong>:</p>
<ul>
<li>Click <strong>Test Voice</strong> button</li>
<li>Voice cloning happens in real-time</li>
</ul>
</li>
</ol>
<p><strong>Configuration:</strong></p>
<pre><code class="language-json">{
  &quot;tts&quot;: {
    &quot;provider&quot;: &quot;gpt-sovits&quot;,
    &quot;sovits&quot;: {
      &quot;serverUrl&quot;: &quot;http://127.0.0.1:9880&quot;,
      &quot;speakerId&quot;: &quot;default&quot;,
      &quot;speed&quot;: 1.0,
      &quot;autoFallbackToPiper&quot;: true
    }
  }
}
</code></pre>
<h3>3. Applio RVC (High-Quality Voice Conversion)</h3>
<p><strong>Best For</strong>: Maximum quality with trained voice models</p>
<p><strong>Features:</strong></p>
<ul>
<li>Train a dedicated voice model from your audio samples</li>
<li>Highest quality voice replication</li>
<li>Requires GPU for training (15-30 minutes)</li>
<li>Uses trained model for all future generations</li>
</ul>
<p><strong>Training Workflow:</strong></p>
<ol>
<li><p><strong>Collect Voice Samples</strong>:</p>
<ul>
<li>Record 3-5 minutes of clear speech</li>
<li>Upload via Voice Clone Training tab</li>
<li>System stores samples in training directory</li>
</ul>
</li>
<li><p><strong>Train Model</strong>:</p>
<ul>
<li>Click <strong>Start Training</strong> button</li>
<li>Requires GPU with at least 6GB VRAM</li>
<li>Training takes 15-30 minutes</li>
<li>Progress shown in UI</li>
</ul>
</li>
<li><p><strong>Activate Model</strong>:</p>
<ul>
<li>Select &quot;Applio RVC&quot; in Voice Settings</li>
<li>Choose your trained model from dropdown</li>
<li>Save and test</li>
</ul>
</li>
</ol>
<h2>Speech-to-Text (STT) - Whisper</h2>
<p>MetaHuman uses OpenAI Whisper for local speech recognition:</p>
<p><strong>Features:</strong></p>
<ul>
<li>Accurate transcription</li>
<li>Multi-language support</li>
<li>Runs entirely locally</li>
<li>Real-time voice input</li>
</ul>
<p><strong>Starting Whisper:</strong></p>
<pre><code class="language-bash"># Start the Whisper server
./bin/mh whisper serve start

# Stop the server
./bin/mh whisper serve stop

# Check status
./bin/mh whisper serve status
</code></pre>
<p><strong>Configuration</strong> (<code>profiles/&lt;user&gt;/etc/voice.json</code>):</p>
<pre><code class="language-json">{
  &quot;stt&quot;: {
    &quot;provider&quot;: &quot;whisper&quot;,
    &quot;whisper&quot;: {
      &quot;model&quot;: &quot;base&quot;,
      &quot;serverUrl&quot;: &quot;http://127.0.0.1:9000&quot;
    }
  }
}
</code></pre>
<p><strong>Model Sizes:</strong></p>
<ul>
<li><strong>tiny</strong>: Fastest, lowest accuracy</li>
<li><strong>base</strong>: Good balance (default)</li>
<li><strong>small</strong>: Better accuracy</li>
<li><strong>medium</strong>: High accuracy, slower</li>
<li><strong>large</strong>: Best accuracy, requires GPU</li>
</ul>
<h2>Voice Chat Interface</h2>
<h3>Voice Mode Button</h3>
<p>In the chat interface:</p>
<ol>
<li>Click the <strong>microphone icon</strong> (üé§) next to the input box</li>
<li>Speak your message</li>
<li>MetaHuman transcribes and responds</li>
<li>Response is automatically read aloud</li>
</ol>
<h3>Per-Message TTS Controls</h3>
<p><strong>In Chat Bubbles:</strong></p>
<ul>
<li>Small microphone icon (üé§) at bottom-right of each message</li>
<li>Click to replay any message aloud</li>
<li>Works for both user and assistant messages</li>
<li>Useful for re-listening to responses</li>
</ul>
<p><strong>Stop Button:</strong></p>
<ul>
<li><strong>Stop button</strong> (üõë) appears when audio is playing</li>
<li>Click to immediately interrupt playback</li>
<li>Cancels active audio and pending TTS</li>
<li>Available on desktop and mobile</li>
</ul>
<h3>Inner Dialogue TTS</h3>
<p>Enable automatic reading of reflections and dreams:</p>
<ol>
<li>Go to <strong>Settings</strong></li>
<li>Toggle <strong>&quot;Enable TTS for inner dialogue&quot;</strong></li>
<li>Reflections and dreams will be read aloud as they occur</li>
<li>Creates an auditory consciousness stream</li>
</ol>
<h2>Voice Workspace (Web UI)</h2>
<p>The Voice tab provides centralized voice management:</p>
<h3>Upload &amp; Transcribe</h3>
<ul>
<li>Drag-and-drop audio files for transcription</li>
<li>Whisper processes and creates memories</li>
<li>View transcripts in Memory Browser</li>
</ul>
<h3>Voice Clone Training</h3>
<ul>
<li>Record or upload voice samples</li>
<li>Stores per-user samples in <code>profiles/&lt;username&gt;/out/voice-training</code></li>
<li>Progress indicators for active profile</li>
<li>Provider selection (SoVITS, RVC)</li>
</ul>
<h3>Voice Settings</h3>
<ul>
<li>Choose TTS provider (Kokoro, SoVITS, RVC)</li>
<li>Select voice/model from available options</li>
<li>Adjust speaking rate and quality</li>
<li>Test voice with sample text</li>
<li>Preferences stored in <code>profiles/&lt;username&gt;/etc/voice.json</code></li>
</ul>
<h3>Special TTS Effects</h3>
<ul>
<li><strong>Mutant Super Intelligence</strong> profile uses dual-voice effect</li>
<li>Pitch-shifted audio mixing for unique sound</li>
<li>Automatically applied when profile is active</li>
</ul>
<h2>Audio File Organization</h2>
<p>Voice data is stored per-user:</p>
<pre><code>profiles/&lt;username&gt;/
‚îú‚îÄ‚îÄ out/
‚îÇ   ‚îú‚îÄ‚îÄ voice-training/          # Training samples
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sovits-reference/    # SoVITS reference audio
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rvc-samples/         # RVC training data
‚îÇ   ‚îú‚îÄ‚îÄ voices/                  # Installed voice models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kokoro-*.onnx        # Kokoro voices
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sovits-*.pth         # SoVITS models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rvc-*.pth            # RVC trained models
‚îÇ   ‚îî‚îÄ‚îÄ audio-cache/             # TTS audio cache
‚îî‚îÄ‚îÄ etc/
    ‚îî‚îÄ‚îÄ voice.json               # Voice preferences
</code></pre>
<h2>Voice Settings Configuration</h2>
<p>Full configuration example:</p>
<pre><code class="language-json">{
  &quot;tts&quot;: {
    &quot;provider&quot;: &quot;kokoro&quot;,
    &quot;kokoro&quot;: {
      &quot;voice&quot;: &quot;af_sky&quot;,
      &quot;speed&quot;: 1.0
    },
    &quot;sovits&quot;: {
      &quot;serverUrl&quot;: &quot;http://127.0.0.1:9880&quot;,
      &quot;speakerId&quot;: &quot;default&quot;,
      &quot;speed&quot;: 1.0,
      &quot;autoFallbackToPiper&quot;: true
    },
    &quot;rvc&quot;: {
      &quot;modelPath&quot;: &quot;profiles/greggles/out/voices/rvc-greggles.pth&quot;,
      &quot;pitch&quot;: 0,
      &quot;indexRate&quot;: 0.75
    }
  },
  &quot;stt&quot;: {
    &quot;provider&quot;: &quot;whisper&quot;,
    &quot;whisper&quot;: {
      &quot;model&quot;: &quot;base&quot;,
      &quot;serverUrl&quot;: &quot;http://127.0.0.1:9000&quot;,
      &quot;language&quot;: &quot;en&quot;
    }
  },
  &quot;audioCache&quot;: {
    &quot;enabled&quot;: true,
    &quot;maxSizeMB&quot;: 500
  }
}
</code></pre>
<h2>Multi-User Voice Isolation</h2>
<p>Each user has isolated voice settings and training data:</p>
<ul>
<li><strong>Voice samples</strong>: Stored per-profile</li>
<li><strong>Preferences</strong>: Independent voice.json per user</li>
<li><strong>Shared voices</strong>: System administrators can install voices in <code>out/voices/</code> for all users</li>
<li><strong>Personal models</strong>: User-trained RVC models stay private</li>
</ul>
<h2>Best Practices</h2>
<h3>For Voice Cloning</h3>
<ol>
<li><p><strong>Recording Quality</strong>:</p>
<ul>
<li>Use a quiet environment</li>
<li>Speak clearly and naturally</li>
<li>Avoid background noise</li>
<li>5-10 seconds minimum for SoVITS</li>
<li>3-5 minutes for RVC training</li>
</ul>
</li>
<li><p><strong>Voice Selection</strong>:</p>
<ul>
<li><strong>Kokoro</strong>: Best for quick setup, multi-language</li>
<li><strong>SoVITS</strong>: Best for instant cloning, testing</li>
<li><strong>RVC</strong>: Best for production use, highest quality</li>
</ul>
</li>
<li><p><strong>Testing</strong>:</p>
<ul>
<li>Always test voice output before committing</li>
<li>Adjust speaking rate for clarity</li>
<li>Try different sample text to verify quality</li>
</ul>
</li>
</ol>
<h3>For Voice Chat</h3>
<ol>
<li>Enable both TTS and STT for seamless voice conversation</li>
<li>Use voice mode for hands-free interaction</li>
<li>Stop button for interrupting long responses</li>
<li>Replay important messages using per-message controls</li>
</ol>
<h2>Troubleshooting</h2>
<h3>TTS Not Working</h3>
<ul>
<li>Check provider server is running (<code>./bin/mh kokoro serve status</code>)</li>
<li>Verify voice.json configuration</li>
<li>Check audio output device settings</li>
<li>Look for errors in web UI console</li>
</ul>
<h3>STT Not Recognizing</h3>
<ul>
<li>Ensure Whisper server is running</li>
<li>Check microphone permissions in browser</li>
<li>Verify correct input device selected</li>
<li>Try speaking more clearly or adjusting volume</li>
</ul>
<h3>Voice Cloning Quality Issues</h3>
<ul>
<li>Record more/better voice samples</li>
<li>Use quieter environment for recording</li>
<li>Try different provider (SoVITS vs RVC)</li>
<li>Adjust pitch and index rate settings</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Use voice in <a href="chat-interface.md">Chat Interface</a> for hands-free interaction</li>
<li>Integrate voice memories in <a href="memory-system.md">Memory System</a></li>
<li>Monitor voice services in <a href="dashboard-monitoring.md">Dashboard</a></li>
<li>Learn about <a href="../training-personalization/ai-training.md">AI Training</a> to personalize responses</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="troubleshooting" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Troubleshooting</span> </span> </button> <button class="nav-button next-button" data-target="voice-training" data-astro-cid-xjasbecl> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Next</span> <span class="nav-title" data-astro-cid-xjasbecl>Voice Training</span> </span> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üí</span> </button> </nav> </article><article id="voice-training" class="chapter" data-visible="false" data-astro-cid-xjasbecl> <div class="chapter-body" data-astro-cid-xjasbecl> <div class="markdown-content" data-astro-cid-xjasbecl><h1>Voice Training</h1>
<p>Train custom voice models to make MetaHuman speak with your own voice or any voice you choose. MetaHuman OS supports multiple training approaches, from instant zero-shot cloning to high-quality custom model training.</p>
<h2>Overview</h2>
<p>Voice training allows you to create personalized TTS (Text-to-Speech) models:</p>
<ul>
<li><strong>Zero-Shot Cloning</strong> - Instant voice replication with SoVITS (5-10 seconds of audio)</li>
<li><strong>Quick Training</strong> - Fast custom voice with Kokoro (3-5 minutes of audio, GPU optional)</li>
<li><strong>High-Quality Training</strong> - Professional voice model with RVC (5+ minutes, requires GPU)</li>
<li><strong>No Training</strong> - Use pre-built voices with Piper (no samples needed)</li>
</ul>
<p>All voice training happens locally - your voice data never leaves your machine.</p>
<h2>Training Methods Compared</h2>
<table>
<thead>
<tr>
<th>Method</th>
<th>Audio Needed</th>
<th>Training Time</th>
<th>GPU Required</th>
<th>Quality</th>
<th>Use Case</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Piper</strong></td>
<td>0 seconds</td>
<td>None</td>
<td>No</td>
<td>Good</td>
<td>Quick setup, multi-language</td>
</tr>
<tr>
<td><strong>SoVITS</strong></td>
<td>5-10 seconds</td>
<td>None (zero-shot)</td>
<td>No</td>
<td>Good</td>
<td>Instant voice cloning</td>
</tr>
<tr>
<td><strong>Kokoro</strong></td>
<td>3-5 minutes</td>
<td>10-30 minutes</td>
<td>Optional</td>
<td>Very Good</td>
<td>Balanced quality/speed</td>
</tr>
<tr>
<td><strong>RVC</strong></td>
<td>5+ minutes</td>
<td>30-60 minutes</td>
<td>Yes</td>
<td>Excellent</td>
<td>Production quality</td>
</tr>
</tbody></table>
<h2>Accessing Voice Training</h2>
<h3>Via Web UI</h3>
<ol>
<li>Navigate to <strong>Voice</strong> in the left sidebar</li>
<li>Click <strong>&quot;Voice Clone Training&quot;</strong> tab</li>
<li>Select training provider from dropdown</li>
<li>Follow provider-specific workflow</li>
</ol>
<h3>Via CLI</h3>
<pre><code class="language-bash"># Check voice training status
./bin/mh voice status

# List available voices
./bin/mh voice list
</code></pre>
<h2>Provider: Piper (No Training)</h2>
<p><strong>Use Case</strong>: Quick setup with pre-built neural voices</p>
<p>Piper doesn&#39;t require training - just select from 50+ pre-built voices:</p>
<ol>
<li>Go to <strong>Voice Settings</strong> tab</li>
<li>Select &quot;Piper&quot; from provider dropdown</li>
<li>Choose voice from available models</li>
<li>Click <strong>&quot;Save&quot;</strong></li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li>No audio samples needed</li>
<li>Works instantly</li>
<li>Multiple languages and accents</li>
<li>No GPU required</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Not your actual voice</li>
<li>Limited personalization</li>
</ul>
<p>See <a href="../using-metahuman/voice-features.md">Voice Features</a> for full Piper details.</p>
<h2>Provider: GPT-SoVITS (Zero-Shot Cloning)</h2>
<p><strong>Use Case</strong>: Instant voice cloning with minimal audio</p>
<p>SoVITS performs real-time voice cloning using a reference audio file - no model training required.</p>
<h3>Recording Reference Audio</h3>
<ol>
<li>Go to <strong>Voice Clone Training</strong> tab</li>
<li>Select <strong>&quot;GPT-SoVITS&quot;</strong> provider</li>
<li>Click <strong>&quot;Record Voice Sample&quot;</strong></li>
<li>Speak clearly for 5-10 seconds</li>
<li>Recording automatically saved to reference directory</li>
</ol>
<p><strong>Recording Tips:</strong></p>
<ul>
<li>Quiet environment (no background noise)</li>
<li>Clear, natural speech</li>
<li>Consistent volume</li>
<li>Read a short paragraph or sentence</li>
</ul>
<h3>Auto-Copy to Reference</h3>
<p>The system automatically:</p>
<ol>
<li>Saves your recording</li>
<li>Copies it to <code>profiles/&lt;username&gt;/out/voices/sovits-reference/default/</code></li>
<li>Makes it available for SoVITS immediately</li>
</ol>
<h3>Activating SoVITS</h3>
<ol>
<li>Go to <strong>Voice Settings</strong> tab</li>
<li>Select &quot;GPT-SoVITS&quot; from provider dropdown</li>
<li>Click <strong>&quot;Save&quot;</strong> (server starts automatically)</li>
<li>Test with <strong>&quot;Test Voice&quot;</strong> button</li>
</ol>
<p><strong>How It Works:</strong></p>
<ul>
<li>Uses reference audio during each TTS generation</li>
<li>No model training - pure zero-shot inference</li>
<li>Voice cloning happens in real-time</li>
</ul>
<p><strong>Advantages:</strong></p>
<ul>
<li>5-10 seconds of audio is enough</li>
<li>Works immediately (no training wait)</li>
<li>No GPU required</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Quality depends on reference audio</li>
<li>Slower than trained models</li>
<li>May need adjustment for different speaking styles</li>
</ul>
<h2>Provider: Kokoro (Quick Training)</h2>
<p><strong>Use Case</strong>: Fast custom voice with good quality</p>
<p>Kokoro trains a lightweight neural voice model from your audio samples.</p>
<h3>Collecting Samples</h3>
<ol>
<li>Go to <strong>Voice Clone Training</strong> tab</li>
<li>Select <strong>&quot;Kokoro&quot;</strong> provider</li>
<li><strong>Record samples</strong> or <strong>upload audio files</strong>:<ul>
<li>Record: Click &quot;Record&quot; button, speak for 10-30 seconds, click &quot;Stop&quot;</li>
<li>Upload: Drag-and-drop audio files (MP3, WAV, FLAC)</li>
</ul>
</li>
<li>System shows progress:<ul>
<li>Samples collected: 12</li>
<li>Total duration: 3 min 45 sec</li>
<li>Target: 3-5 minutes</li>
</ul>
</li>
</ol>
<p><strong>Sample Quality:</strong></p>
<ul>
<li>Clear speech (no music or effects)</li>
<li>Varied sentences (not repetitive)</li>
<li>Consistent speaking style</li>
<li>Good microphone quality</li>
</ul>
<h3>Starting Training</h3>
<p>When you have 3-5 minutes of audio:</p>
<ol>
<li>Review training configuration:<ul>
<li><strong>Dataset Size</strong>: How many samples to use</li>
<li><strong>Training Duration</strong>: Quick (10 min) vs. Thorough (30 min)</li>
<li><strong>Voice Name</strong>: Identifier for the trained model</li>
</ul>
</li>
<li>Click <strong>&quot;Start Kokoro Training&quot;</strong></li>
<li>Training begins with progress display</li>
</ol>
<p><strong>Training Process:</strong></p>
<pre><code>Training Progress
‚îú‚îÄ Preprocessing samples... ‚úì
‚îú‚îÄ Generating voicepack... (45%)
‚îú‚îÄ Current epoch: 8/20
‚îî‚îÄ Time remaining: ~12 minutes
</code></pre>
<p><strong>Training Logs:</strong></p>
<ul>
<li>Toggle between robot messages (fun) and real logs (technical)</li>
<li>Auto-scrolling log viewer</li>
<li>Shows preprocessing, training epochs, validation</li>
</ul>
<h3>GPU Acceleration (Optional)</h3>
<p>Kokoro can use GPU if available:</p>
<ul>
<li><strong>With GPU</strong>: 10-15 minutes training</li>
<li><strong>CPU Only</strong>: 25-35 minutes training</li>
</ul>
<p>Training runs in background - you can close the browser.</p>
<h3>After Training</h3>
<ol>
<li>Training completes and shows voicepack path</li>
<li>Go to <strong>Voice Settings</strong> tab</li>
<li>Select &quot;Kokoro&quot; provider</li>
<li>Choose your trained voice from dropdown</li>
<li>Click <strong>&quot;Save&quot;</strong> and test</li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li>Relatively fast training</li>
<li>Good quality results</li>
<li>GPU optional (CPU works)</li>
<li>Lightweight voicepack files</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Requires 3-5 minutes of audio</li>
<li>Lower quality than RVC</li>
<li>Training takes 10-30 minutes</li>
</ul>
<h2>Provider: Applio RVC (High-Quality Training)</h2>
<p><strong>Use Case</strong>: Maximum quality for production use</p>
<p>RVC trains a high-fidelity voice conversion model using your voice samples.</p>
<h3>Collecting Samples</h3>
<ol>
<li>Go to <strong>Voice Clone Training</strong> tab</li>
<li>Select <strong>&quot;RVC&quot;</strong> provider</li>
<li><strong>Record or upload audio</strong>:<ul>
<li>Record multiple samples (10-30 seconds each)</li>
<li>Upload audio files</li>
</ul>
</li>
<li>Target: <strong>5+ minutes</strong> of clear speech</li>
</ol>
<p><strong>Progress Display:</strong></p>
<pre><code>Voice Training Progress
‚îú‚îÄ Samples: 18
‚îú‚îÄ Total Duration: 6 min 12 sec
‚îú‚îÄ Quality Score: 92%
‚îî‚îÄ Ready for Training: ‚úì
</code></pre>
<h3>Reference Audio Selection</h3>
<p>Before training, select samples to use:</p>
<ol>
<li>Click <strong>&quot;Select Reference Audio&quot;</strong></li>
<li>Review all collected samples</li>
<li>Check boxes for high-quality samples</li>
<li>Uncheck low-quality or noisy samples</li>
<li>Click <strong>&quot;Copy Selected to Training Dataset&quot;</strong></li>
</ol>
<p><strong>Dataset Readiness:</strong></p>
<ul>
<li>Minimum samples: 10 clips</li>
<li>Minimum duration: 5 minutes</li>
<li>Minimum quality: 70%</li>
</ul>
<h3>Training Configuration</h3>
<p>Configure RVC training parameters:</p>
<p><strong>Basic Settings:</strong></p>
<ul>
<li><strong>Total Epochs</strong>: 300 (default) - higher = better quality, longer training</li>
<li><strong>Save Every Epoch</strong>: 50 - checkpoint frequency</li>
<li><strong>Batch Size</strong>: 8 - higher uses more VRAM</li>
</ul>
<p><strong>Advanced Settings</strong> (click &quot;Show Advanced&quot;):</p>
<ul>
<li>Learning rate</li>
<li>F0 extraction method</li>
<li>Sample rate</li>
<li>Model architecture</li>
</ul>
<h3>Starting Training</h3>
<ol>
<li>Verify dataset ready (green checkmark)</li>
<li>Review settings</li>
<li>Click <strong>&quot;Start RVC Training&quot;</strong></li>
<li>Training window opens with progress</li>
</ol>
<p><strong>Training Progress:</strong></p>
<pre><code>ü§ñ Robot Messages Mode
&quot;‚ö° Training neural networks... human obsolescence: 47%&quot;

OR

üìã Technical Logs Mode
[Epoch 45/300] Loss: 0.0234 | ETA: 18 min
</code></pre>
<p><strong>Requirements:</strong></p>
<ul>
<li><strong>GPU</strong>: NVIDIA GPU with 6GB+ VRAM (required)</li>
<li><strong>Time</strong>: 30-60 minutes depending on samples</li>
<li><strong>Disk</strong>: ~500MB for trained model</li>
</ul>
<h3>Monitoring Training</h3>
<ul>
<li><strong>Robot Messages</strong>: Entertaining updates every 10 seconds</li>
<li><strong>Training Logs</strong>: Real-time technical output</li>
<li><strong>Progress Bar</strong>: Epoch completion percentage</li>
<li><strong>ETA</strong>: Estimated time remaining</li>
</ul>
<p><strong>Training Happens in Background</strong> - You can:</p>
<ul>
<li>Close browser (training continues)</li>
<li>Check back later via progress endpoint</li>
<li>View logs anytime</li>
</ul>
<h3>After Training</h3>
<ol>
<li>Training completes successfully</li>
<li>Model saved to <code>profiles/&lt;username&gt;/out/voices/rvc-&lt;name&gt;.pth</code></li>
<li>Go to <strong>Voice Settings</strong></li>
<li>Select &quot;Applio RVC&quot; provider</li>
<li>Choose your trained model</li>
<li>Save and test</li>
</ol>
<p><strong>Advantages:</strong></p>
<ul>
<li>Highest quality voice replication</li>
<li>Professional results</li>
<li>Handles varied speaking styles well</li>
<li>Best for production use</li>
</ul>
<p><strong>Limitations:</strong></p>
<ul>
<li>Requires GPU with 6GB+ VRAM</li>
<li>Training takes 30-60 minutes</li>
<li>Needs 5+ minutes of audio samples</li>
<li>Larger model files (~500MB)</li>
</ul>
<h2>Sample Management</h2>
<h3>Recording Samples</h3>
<p><strong>Direct Voice Recorder</strong> component:</p>
<ol>
<li>Click <strong>&quot;Record&quot;</strong> button</li>
<li>Microphone permission prompt (allow)</li>
<li>Red recording indicator appears</li>
<li>Speak clearly into microphone</li>
<li>Click <strong>&quot;Stop&quot;</strong> when finished</li>
<li>Sample automatically saved</li>
</ol>
<p><strong>Recording Tips:</strong></p>
<ul>
<li>Use good microphone (not laptop built-in if possible)</li>
<li>Quiet environment</li>
<li>Speak naturally (not monotone)</li>
<li>Vary sentence structure</li>
<li>Include different emotions/tones</li>
</ul>
<h3>Uploading Samples</h3>
<ol>
<li>Click <strong>&quot;Upload Audio&quot;</strong> button</li>
<li>Select audio files (MP3, WAV, M4A, FLAC)</li>
<li>Or drag-and-drop files into upload area</li>
<li>Files automatically processed</li>
</ol>
<p><strong>Supported Formats:</strong></p>
<ul>
<li>MP3, WAV, M4A, FLAC, OGG</li>
<li>Any sample rate (resampled automatically)</li>
<li>Mono or stereo (converted to mono)</li>
</ul>
<h3>Sample Quality Metrics</h3>
<p>System analyzes each sample:</p>
<ul>
<li><strong>Duration</strong>: Length in seconds</li>
<li><strong>Quality Score</strong>: 0-100 based on:<ul>
<li>Background noise level</li>
<li>Clipping/distortion</li>
<li>Speech clarity</li>
<li>Consistent volume</li>
</ul>
</li>
</ul>
<p><strong>Quality Thresholds:</strong></p>
<ul>
<li>90-100: Excellent</li>
<li>70-89: Good</li>
<li>50-69: Fair (usable but not ideal)</li>
<li>&lt;50: Poor (exclude from training)</li>
</ul>
<h3>Managing Samples</h3>
<p><strong>View All Samples:</strong></p>
<ul>
<li>Samples list shows all collected audio</li>
<li>Play any sample to preview</li>
<li>Delete low-quality samples</li>
<li>Re-record if needed</li>
</ul>
<p><strong>Exporting Samples:</strong></p>
<ol>
<li>Click <strong>&quot;Export Dataset&quot;</strong></li>
<li>Downloads ZIP of all samples</li>
<li>Backup or share samples</li>
</ol>
<p><strong>Purge All Samples:</strong></p>
<ol>
<li>Click <strong>&quot;Purge All Samples&quot;</strong></li>
<li>Confirm deletion (irreversible)</li>
<li>All samples removed from disk</li>
<li>Start over with new recordings</li>
</ol>
<h2>Voice Storage Structure</h2>
<p>Voice training data is stored per-user:</p>
<pre><code>profiles/&lt;username&gt;/out/
‚îú‚îÄ‚îÄ voice-training/
‚îÇ   ‚îú‚îÄ‚îÄ samples/              # Recorded or uploaded audio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample-001.wav
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ sample-002.wav
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îú‚îÄ‚îÄ sovits-reference/     # SoVITS reference audio
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ default/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ reference.wav
‚îÇ   ‚îî‚îÄ‚îÄ rvc-dataset/          # Copied RVC training data
‚îÇ       ‚îú‚îÄ‚îÄ audio-001.wav
‚îÇ       ‚îî‚îÄ‚îÄ audio-002.wav
‚îî‚îÄ‚îÄ voices/                   # Trained models
    ‚îú‚îÄ‚îÄ kokoro-myvoice.voicepack
    ‚îú‚îÄ‚îÄ rvc-myvoice.pth
    ‚îî‚îÄ‚îÄ rvc-myvoice.index
</code></pre>
<h2>Training Best Practices</h2>
<h3>For All Providers</h3>
<ol>
<li><strong>Quality over quantity</strong>: Better to have 3 minutes of clear audio than 10 minutes of noisy audio</li>
<li><strong>Vary your speech</strong>: Different sentences, tones, emotions</li>
<li><strong>Consistent style</strong>: Match the speaking style you want to replicate</li>
<li><strong>Good equipment</strong>: Use decent microphone in quiet space</li>
<li><strong>Test early</strong>: Train with minimum samples first, then retrain with more for higher quality</li>
</ol>
<h3>For SoVITS (Zero-Shot)</h3>
<ul>
<li>Record multiple reference samples</li>
<li>Test each to find the best match</li>
<li>Clear, expressive speech works best</li>
<li>7-10 seconds is the sweet spot</li>
</ul>
<h3>For Kokoro (Quick Training)</h3>
<ul>
<li>Aim for 4-5 minutes of audio</li>
<li>Record in multiple sessions (variety)</li>
<li>Check progress indicator before training</li>
<li>Use GPU if available for faster training</li>
</ul>
<h3>For RVC (High-Quality)</h3>
<ul>
<li>Collect 7-10 minutes for best results</li>
<li>Use reference audio selector to choose best samples</li>
<li>Include varied content (not just reading)</li>
<li>Monitor GPU temperature during training</li>
<li>Save checkpoints every 50 epochs</li>
</ul>
<h2>Troubleshooting</h2>
<h3>Training Won&#39;t Start</h3>
<ul>
<li>Check GPU availability (RVC only)</li>
<li>Verify dataset readiness (green checkmark)</li>
<li>Check disk space (need ~2GB free)</li>
<li>Review training logs for errors</li>
</ul>
<h3>Poor Voice Quality</h3>
<ul>
<li>Re-record with better microphone</li>
<li>Remove background noise from samples</li>
<li>Increase training duration</li>
<li>Use more/better audio samples</li>
<li>Check sample quality scores</li>
</ul>
<h3>Training Fails Midway</h3>
<ul>
<li>Check GPU didn&#39;t overheat (RVC)</li>
<li>Verify enough disk space</li>
<li>Check training logs for specific error</li>
<li>Reduce batch size if VRAM error</li>
</ul>
<h3>Voice Sounds Robotic</h3>
<ul>
<li>Need more training samples</li>
<li>Increase training epochs (RVC)</li>
<li>Use higher quality reference audio (SoVITS)</li>
<li>Retrain with better data</li>
</ul>
<h2>Next Steps</h2>
<ul>
<li>Configure voice in <a href="../using-metahuman/voice-features.md">Voice Features</a></li>
<li>Train AI personality with <a href="ai-training.md">AI Training</a></li>
<li>Use voice in <a href="../using-metahuman/chat-interface.md">Chat Interface</a></li>
<li>Combine voice with <a href="cognitive-modes.md">Cognitive Modes</a> for full personality expression</li>
</ul>
</div> </div> <nav class="chapter-navigation" data-astro-cid-xjasbecl> <button class="nav-button prev-button" data-target="voice-features" data-astro-cid-xjasbecl> <span class="nav-arrow" data-astro-cid-xjasbecl>‚Üê</span> <span class="nav-label" data-astro-cid-xjasbecl> <span class="nav-direction" data-astro-cid-xjasbecl>Previous</span> <span class="nav-title" data-astro-cid-xjasbecl>Voice Features</span> </span> </button>  </nav> </article>  </main> </div>   </main> <footer class="page-max muted py-8 text-center text-sm"> <p>MetaHuman OS ‚Ä¢ Autonomous ‚Ä¢ Local-first ‚Ä¢ Transparent</p> </footer> </body></html> 