class s{async isModelLoaded(){return{loaded:!1}}async loadModel(e){return{success:!1,error:"Native LLM not available on web platform"}}async unloadModel(){return{success:!0}}async generate(e){return{text:"",tokensGenerated:0,tokensPerSecond:0,finishReason:"error"}}async chat(e){return{response:"",tokensGenerated:0,tokensPerSecond:0}}async listModels(){return{models:[]}}async downloadModel(e){return{success:!1,error:"Model download not available on web platform"}}async getStatus(){return{inferring:!1,modelLoaded:!1,memoryUsageMB:0,cpuUsagePercent:0}}async cancelInference(){return{cancelled:!1}}async addListener(e,a){return{remove:()=>{}}}}export{s as NativeLLMWeb};
