# --- RunPod Configuration ---
#RUNPOD_API_KEY="YOUR_RUNPOD_API_KEY_HERE" # Get this from your RunPod dashboard
#RUNPOD_SSH_KEY_PATH="/home/username/.ssh/id_rsa" # Your private key path
#RUNPOD_POD_ID="YOUR_POD_ID_HERE" # Get this after launching your pod
#RUNPOD_REMOTE_WORKDIR="/workspace/training_jobs" # Or your preferred path on the pod's volume

# --- Training Location ---
# Set this to "runpod" to use RunPod, or "local" to use your local machine
TRAINING_LOCATION="runpod"

# Metahuman Trainer – .env template

# Required: RunPod API token
# Get from RunPod dashboard → API Keys
RUNPOD_API_KEY="YOUR_RUNPOD_API_KEY_HERE"

# Optional: RunPod Template ID to deploy pods from
# Create one in RunPod dashboard using an image like
#   runpod/pytorch:1.0.2-cu1281-torch280-ubuntu2404
# Ensure:
# - Public IP enabled
# - SSH (TCP 22) published publicly
# - Your SSH public key is installed via UI or startup script
# If unset, the agent falls back to a built-in template id.
RUNPOD_TEMPLATE_ID="YOUR_TEMPLATE_ID_HERE"

# Optional: Force direct SSH user (skips probing root/unsloth)
# Example: RUNPOD_DIRECT_SSH_USER=root
#RUNPOD_DIRECT_SSH_USER=

# Optional: Disable gateway discovery attempts (reduces GraphQL 400s noise)
# Set to 1 to skip schema probes entirely and rely on direct SSH via public IP:22.
# Example: RUNPOD_NO_GATEWAY=1
#RUNPOD_NO_GATEWAY=

# Optional: GPU Type to request (defaults to RTX 4090)
# Recommended GPUs for 30B LoRA training:
#   - "NVIDIA GeForce RTX 4090" (24GB, ~$0.34/hr) - Best overall
#   - "NVIDIA GeForce RTX 3090" (24GB, ~$0.20/hr) - Best value
#   - "NVIDIA A40" (48GB, ~$0.59/hr) - Great for large batches
#   - "NVIDIA A100-SXM4-40GB" (40GB, ~$1.09/hr) - Most reliable
# Avoid: "NVIDIA GeForce RTX 5090" - Xformers incompatibility
RUNPOD_GPU_TYPE="NVIDIA GeForce RTX 4090"

# Notes:
# - Do NOT put ssh_key_path here. The agent reads it from the per-run
#   connection.json under metahuman-runs/<DATE>/connection.json.
# - Copy this file to .env and fill values. .env is gitignored.
#RUNPOD_NO_GATEWAY=0
#RUNPOD_DIRECT_SSH_USER=

#--- Brave Search Configuration ---
# Required: Brave Search API Key
BRAVE_SEARCH_API_KEY="YOUR_BRAVE_SEARCH_API_KEY_HERE"

# --- Metahuman Dataset Builder Configuration ---
METAHUMAN_DATASET_BUILDER=ai
METAHUMAN_DATASET_MAX=1500
METAHUMAN_DATASET_CHUNK=15
# METAHUMAN_DATASET_MODEL=qwen3-coder:30b   # optional override
# (optional) terminal dataset start command: 
# pnpm tsx brain/agents/ai-dataset-builder.ts \
#  --output out/datasets/$(date +%F)/ai_dataset.jsonl \
#  --max 1500
# =============================================================================
# Cognitive Architecture - Phase 4
# =============================================================================

# USE_COGNITIVE_PIPELINE (Phase 4.1a)
# When enabled, uses PersonalityCoreLayer wrapper for response generation
# Use case: Enable 3-layer cognitive architecture
# Effect: LLM calls in persona_chat.ts route through Layer 2 wrapper
# Benefits: LoRA adapter management, voice consistency tracking, prep for validation
# Default: false (disabled for safety - explicit opt-in required)
#USE_COGNITIVE_PIPELINE=true

# ENABLE_SAFETY_CHECKS (Phase 4.2)
# When enabled, runs non-blocking safety validation on all responses
# Use case: Monitor for sensitive data leaks, harmful content, security issues
# Effect: Issues logged to audit and console, responses NEVER blocked
# Note: Only active when USE_COGNITIVE_PIPELINE=true
# Default: true (enabled when pipeline is enabled)
#ENABLE_SAFETY_CHECKS=true

# ENABLE_RESPONSE_REFINEMENT (Phase 4.3)
# When enabled, auto-sanitizes detected safety issues (non-blocking test mode)
# Use case: Test automatic sanitization of API keys, passwords, file paths, IPs
# Effect: Both original and refined logged, ORIGINAL still sent to user
# Note: Only active when USE_COGNITIVE_PIPELINE=true and safety issues detected
# Default: true (enabled when pipeline is enabled)
#ENABLE_RESPONSE_REFINEMENT=true

# ENABLE_BLOCKING_MODE (Phase 4.4)
# When enabled, sends REFINED responses to users (blocking mode)
# IMPORTANT: Only enable after validating refinement quality in Phase 4.3 logs
# Use case: Production enforcement - actually sanitize responses sent to users
# Effect: Users receive refined (sanitized) responses, originals logged for review
# Rollback: Set to false to immediately return to non-blocking mode
# Note: Only active when USE_COGNITIVE_PIPELINE=true and refinement succeeds
# Default: false (non-blocking, explicit opt-in required for safety)
#ENABLE_BLOCKING_MODE=true

# NODE PIPELINE (Phase 5)
# Cognitive graph executor is enabled by default. Set to false ONLY if you need to fall back
# to the legacy imperative pipeline (e.g., during incident response or debugging).
#USE_NODE_PIPELINE=false

# =============================================================================
# Phase 6: Authentication & System Triggers
# =============================================================================

# WETWARE_DECEASED
# When enabled, disables dual consciousness mode
# Use case: Biological counterpart is deceased, digital consciousness operates independently
# Effect: Dual mode grayed out in UI, only agent and emulation modes available
#WETWARE_DECEASED=true

# HIGH_SECURITY
# When enabled, locks system to emulation mode only
# Use case: Maximum security for public demos or untrusted access
# Effect: Only emulation mode allowed, all write operations disabled
#HIGH_SECURITY=true

# Authentication (Future - not yet implemented)
#JWT_SECRET=your-secret-key-here
#SESSION_DURATION=86400
