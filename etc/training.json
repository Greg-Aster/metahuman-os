{
  "base_model": "openai/gpt-oss-20b",
  "max_seq_length": 2048,
  "lora_rank": 8,
  "lora_alpha": 16,
  "lora_dropout": 0.05,

  "num_train_epochs": 3,
  "learning_rate": 0.0002,

  "per_device_train_batch_size": 1,
  "gradient_accumulation_steps": 16,

  "optimizer": "paged_adamw_8bit",

  "load_in_4bit": true,
  "dtype": "bfloat16",
  "use_gradient_checkpointing": true,
  "fp16": false,
  "bf16": true,

  "logging_steps": 10,
  "save_strategy": "epoch",
  "save_total_limit": 2,

  "chat_template": "harmony",
  "system_prompt": "You are MetaHuman Greg, a helpful assistant.",

  "comment": "Training configuration for OpenAI gpt-oss-20b - MoE model with 21B total params (3.6B active)",
  "notes": {
    "model_size": "21B parameters total, 3.6B active (Mixture of Experts)",
    "vram_usage": "~12-16GB with 4-bit quantization (model natively optimized for 16GB)",
    "architecture": "MoE with 24 layers, 32 experts, Top-4 routing",
    "context_length": "128k native support, using 2048 for training efficiency",
    "effective_batch_size": "per_device_train_batch_size (1) Ã— grad_accum (16) = 16",
    "chat_template": "harmony - OpenAI's Harmony format with <|start|>, <|message|>, <|end|> tokens",
    "license": "Apache 2.0",
    "use_case": "Lower latency, local deployment, runs on consumer hardware",
    "supported_templates": "harmony | chatml | llama | auto (auto-detect from model)"
  }
}
